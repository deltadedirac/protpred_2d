{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SS_Pred_CNN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O4p687bBuzgb","executionInfo":{"status":"ok","timestamp":1611609259742,"user_tz":300,"elapsed":2065,"user":{"displayName":"Sebastian garcia lopez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjC2t48-bE51c9ubEao0oacZ0i3LJmFyS90LgNgfg=s64","userId":"04181361624039461809"}},"outputId":"c96449a1-6a24-4019-f64c-79652ada8c8e"},"source":["from google.colab import drive\r\n","drive.mount('/gdrive',force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u1PAEh8l1B99","executionInfo":{"status":"ok","timestamp":1611071078310,"user_tz":300,"elapsed":3242,"user":{"displayName":"Sebastian garcia lopez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjC2t48-bE51c9ubEao0oacZ0i3LJmFyS90LgNgfg=s64","userId":"04181361624039461809"}},"outputId":"a1727c70-6dec-4c70-a884-69d618773b31"},"source":["%cd /gdrive/Shareddrives/neurolusion/PhD/Secondary_Structure_Pred/protpred_2d\r\n","!ls\r\n","!wget -r http://www.princeton.edu/~jzthree/datasets/ICML2014/\r\n","%cp -r www.princeton.edu/~jzthree/datasets/ICML2014/ .\r\n","!rm -r www.princeton.edu/\r\n","!rm ICML2014/in*"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/gdrive/Shareddrives/neurolusion/PhD/Secondary_Structure_Pred\n"," Dockerfile\t  ICML2014   pytorch_examples.ipynb   src\n"," git_repo.ipynb   models    'Scheme SSpred.gslides'   SS_Pred_CNN.ipynb\n","--2021-01-19 15:44:37--  http://www.princeton.edu/~jzthree/datasets/ICML2014/\n","Resolving www.princeton.edu (www.princeton.edu)... 104.18.5.101, 104.18.4.101, 2606:4700::6812:565, ...\n","Connecting to www.princeton.edu (www.princeton.edu)|104.18.5.101|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2649 (2.6K) [text/html]\n","Saving to: ‘www.princeton.edu/~jzthree/datasets/ICML2014/index.html’\n","\n","www.princeton.edu/~ 100%[===================>]   2.59K  --.-KB/s    in 0s      \n","\n","2021-01-19 15:44:37 (312 MB/s) - ‘www.princeton.edu/~jzthree/datasets/ICML2014/index.html’ saved [2649/2649]\n","\n","Loading robots.txt; please ignore errors.\n","--2021-01-19 15:44:37--  http://www.princeton.edu/robots.txt\n","Reusing existing connection to www.princeton.edu:80.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://www.princeton.edu/robots.txt [following]\n","--2021-01-19 15:44:37--  https://www.princeton.edu/robots.txt\n","Connecting to www.princeton.edu (www.princeton.edu)|104.18.5.101|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1686 (1.6K) [text/plain]\n","Saving to: ‘www.princeton.edu/robots.txt’\n","\n","www.princeton.edu/r 100%[===================>]   1.65K  --.-KB/s    in 0.001s  \n","\n","2021-01-19 15:44:37 (2.24 MB/s) - ‘www.princeton.edu/robots.txt’ saved [1686/1686]\n","\n","--2021-01-19 15:44:37--  http://www.princeton.edu/icons/blank.gif\n","Connecting to www.princeton.edu (www.princeton.edu)|104.18.5.101|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 148 [image/gif]\n","Saving to: ‘www.princeton.edu/icons/blank.gif’\n","\n","www.princeton.edu/i 100%[===================>]     148  --.-KB/s    in 0s      \n","\n","2021-01-19 15:44:37 (25.6 MB/s) - ‘www.princeton.edu/icons/blank.gif’ saved [148/148]\n","\n","--2021-01-19 15:44:37--  http://www.princeton.edu/~jzthree/datasets/ICML2014/?C=N;O=D\n","Reusing existing connection to www.princeton.edu:80.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2649 (2.6K) [text/html]\n","Saving to: ‘www.princeton.edu/~jzthree/datasets/ICML2014/index.html?C=N;O=D’\n","\n","www.princeton.edu/~ 100%[===================>]   2.59K  --.-KB/s    in 0s      \n","\n","2021-01-19 15:44:37 (481 MB/s) - ‘www.princeton.edu/~jzthree/datasets/ICML2014/index.html?C=N;O=D’ saved [2649/2649]\n","\n","--2021-01-19 15:44:37--  http://www.princeton.edu/~jzthree/datasets/ICML2014/?C=M;O=A\n","Reusing existing connection to www.princeton.edu:80.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2649 (2.6K) [text/html]\n","Saving to: ‘www.princeton.edu/~jzthree/datasets/ICML2014/index.html?C=M;O=A’\n","\n","www.princeton.edu/~ 100%[===================>]   2.59K  --.-KB/s    in 0s      \n","\n","2021-01-19 15:44:37 (265 MB/s) - ‘www.princeton.edu/~jzthree/datasets/ICML2014/index.html?C=M;O=A’ saved [2649/2649]\n","\n","--2021-01-19 15:44:37--  http://www.princeton.edu/~jzthree/datasets/ICML2014/?C=S;O=A\n","Reusing existing connection to www.princeton.edu:80.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2649 (2.6K) [text/html]\n","Saving to: ‘www.princeton.edu/~jzthree/datasets/ICML2014/index.html?C=S;O=A’\n","\n","www.princeton.edu/~ 100%[===================>]   2.59K  --.-KB/s    in 0s      \n","\n","2021-01-19 15:44:37 (385 MB/s) - ‘www.princeton.edu/~jzthree/datasets/ICML2014/index.html?C=S;O=A’ saved [2649/2649]\n","\n","--2021-01-19 15:44:37--  http://www.princeton.edu/~jzthree/datasets/ICML2014/?C=D;O=A\n","Reusing existing connection to www.princeton.edu:80.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2649 (2.6K) [text/html]\n","Saving to: ‘www.princeton.edu/~jzthree/datasets/ICML2014/index.html?C=D;O=A’\n","\n","www.princeton.edu/~ 100%[===================>]   2.59K  --.-KB/s    in 0s      \n","\n","2021-01-19 15:44:37 (392 MB/s) - ‘www.princeton.edu/~jzthree/datasets/ICML2014/index.html?C=D;O=A’ saved [2649/2649]\n","\n","--2021-01-19 15:44:37--  http://www.princeton.edu/icons/back.gif\n","Reusing existing connection to www.princeton.edu:80.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 216 [image/gif]\n","Saving to: ‘www.princeton.edu/icons/back.gif’\n","\n","www.princeton.edu/i 100%[===================>]     216  --.-KB/s    in 0s      \n","\n","2021-01-19 15:44:37 (35.5 MB/s) - ‘www.princeton.edu/icons/back.gif’ saved [216/216]\n","\n","--2021-01-19 15:44:37--  http://www.princeton.edu/~jzthree/datasets/\n","Reusing existing connection to www.princeton.edu:80.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1166 (1.1K) [text/html]\n","Saving to: ‘www.princeton.edu/~jzthree/datasets/index.html’\n","\n","www.princeton.edu/~ 100%[===================>]   1.14K  --.-KB/s    in 0s      \n","\n","2021-01-19 15:44:37 (169 MB/s) - ‘www.princeton.edu/~jzthree/datasets/index.html’ saved [1166/1166]\n","\n","--2021-01-19 15:44:37--  http://www.princeton.edu/icons/compressed.gif\n","Reusing existing connection to www.princeton.edu:80.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1038 (1.0K) [image/gif]\n","Saving to: ‘www.princeton.edu/icons/compressed.gif’\n","\n","www.princeton.edu/i 100%[===================>]   1.01K  --.-KB/s    in 0s      \n","\n","2021-01-19 15:44:37 (146 MB/s) - ‘www.princeton.edu/icons/compressed.gif’ saved [1038/1038]\n","\n","--2021-01-19 15:44:37--  http://www.princeton.edu/~jzthree/datasets/ICML2014/cb513+profile_split1.npy.gz\n","Reusing existing connection to www.princeton.edu:80.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [application/x-gzip]\n","Saving to: ‘www.princeton.edu/~jzthree/datasets/ICML2014/cb513+profile_split1.npy.gz’\n","\n","        www.princet     [  <=>               ]  23.63M  59.0MB/s               ^C\n","^C\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"obuk6MMhvGOg","executionInfo":{"status":"ok","timestamp":1611609266096,"user_tz":300,"elapsed":1006,"user":{"displayName":"Sebastian garcia lopez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjC2t48-bE51c9ubEao0oacZ0i3LJmFyS90LgNgfg=s64","userId":"04181361624039461809"}},"outputId":"7b240d7b-08d5-4010-f473-98e4e9202b56"},"source":["# in case of being in my personal account\r\n","%cd /gdrive/Shareddrives/neurolusion/PhD/Secondary_Structure_Pred/protpred_2d/src"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/gdrive/Shareddrives/neurolusion/PhD/Secondary_Structure_Pred/src\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SVxeb24dU6B3","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1611612340903,"user_tz":300,"elapsed":3070235,"user":{"displayName":"Sebastian garcia lopez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjC2t48-bE51c9ubEao0oacZ0i3LJmFyS90LgNgfg=s64","userId":"04181361624039461809"}},"outputId":"2701c669-d518-4913-f735-98bc4113badb"},"source":["import pdb.\r\n","import torch\r\n","import os\r\n","\r\n","import numpy as np\r\n","from convolutional_nn import *\r\n","from one_hot_encoding import *\r\n","from datasetProcessing import *\r\n","from torch.autograd import Variable\r\n","from torch.utils.data import Dataset, DataLoader\r\n","\r\n","\r\n","if __name__ == \"__main__\":\r\n","\r\n","  pdb.set_trace()\r\n","  path_preexist_model = '/gdrive/Shareddrives/neurolusion/PhD/Secondary_Structure_Pred/protpred_2d/models/ss_struct3.pth'\r\n","  dataset_path = \"../ICML2014/cullpdb+profile_5926.npy.gz\"\r\n","  n_prot = 5926; n_aa = 700; n_features = 57\r\n","  \r\n","  data_management = datasetProcessing(dataset_path, n_prot, n_aa, n_features)\r\n","\r\n","  num_epochs = 1000\r\n","  total_samples = len(data_management.tr)\r\n","  batch_size = 256\r\n","  n_iterations = math.ceil(total_samples/batch_size)\r\n","\r\n","  dataloader = DataLoader(dataset = data_management, batch_size = batch_size, num_workers = 4)\r\n","\r\n","\r\n","  # Beginning of training stage\r\n","  ss_struct_pred = convolutional_nn()\r\n","\r\n","  '''\r\n","  # Find whether the GPU is still available\r\n","  CUDA=torch.cuda.is_available()\r\n","  if CUDA:\r\n","    ss_struct_pred = ss_struct_pred.cuda()\r\n","  '''\r\n","\r\n","  # Definition of loss function\r\n","  loss_function = torch.nn.CrossEntropyLoss()\r\n","  \r\n","  # Selection of optimization algorithm to find the optimal weights in network with l2 norm regularization (weight decay)\r\n","  #optimizer=torch.optim.SGD(ss_struct_pred.parameters(), lr=0.01, weight_decay=1e-4)\r\n","  #optimizer=torch.optim.Adam(ss_struct_pred.parameters(), lr=0.01, weight_decay=1e-4)\r\n","  optimizer=torch.optim.SGD(ss_struct_pred.parameters(), lr=0.01, weight_decay=1e-4)\r\n","\r\n","\r\n","  training_iterations = 1000\r\n","\r\n","  if os.path.isfile(path_preexist_model):\r\n","    print (\"Loading Model\")  \r\n","    state_dict = torch.load(path_preexist_model)\r\n","    #print(state_dict)\r\n","    ss_struct_pred.load_state_dict(state_dict)\r\n","\r\n","  else:\r\n","    \r\n","    print (\"Training Model\")\r\n","    \r\n","\r\n","    for epoch in range(num_epochs):\r\n","        for i, (inputs, labels) in enumerate(dataloader):\r\n","\r\n","          optimizer.zero_grad()\r\n","          output = ss_struct_pred(inputs)\r\n","          loss = loss_function(output, torch.argmax(labels,dim=1))\r\n","          loss.backward()\r\n","          #print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\r\n","          #if (i+1) % 2 ==0:\r\n","          print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_iterations}, loss {loss.item()}')\r\n","          optimizer.step()\r\n","          torch.save(ss_struct_pred.state_dict(), path_preexist_model)\r\n","          torch.cuda.empty_cache()\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["> <ipython-input-3-f901e8a027d8>(16)<module>()\n","-> path_preexist_model = '/gdrive/Shareddrives/neurolusion/PhD/Secondary_Structure_Pred/models/ss_struct3.pth'\n","(Pdb) n\n","> <ipython-input-3-f901e8a027d8>(17)<module>()\n","-> dataset_path = \"../ICML2014/cullpdb+profile_5926.npy.gz\"\n","(Pdb) b 59\n","Breakpoint 1 at <ipython-input-3-f901e8a027d8>:59\n","(Pdb) \n","Breakpoint 2 at <ipython-input-3-f901e8a027d8>:59\n","(Pdb) data_management.tt.shape\n","*** NameError: name 'data_management' is not defined\n","(Pdb) data_management.tt\n","*** NameError: name 'data_management' is not defined\n","(Pdb) l\n"," 12  \t\n"," 13  \tif __name__ == \"__main__\":\n"," 14  \t\n"," 15  \t  pdb.set_trace()\n"," 16  \t  path_preexist_model = '/gdrive/Shareddrives/neurolusion/PhD/Secondary_Structure_Pred/models/ss_struct3.pth'\n"," 17  ->\t  dataset_path = \"../ICML2014/cullpdb+profile_5926.npy.gz\"\n"," 18  \t  n_prot = 5926; n_aa = 700; n_features = 57\n"," 19  \t\n"," 20  \t  data_management = datasetProcessing(dataset_path, n_prot, n_aa, n_features)\n"," 21  \t\n"," 22  \t  num_epochs = 1000\n","(Pdb) b 59\n","Breakpoint 3 at <ipython-input-3-f901e8a027d8>:59\n","(Pdb) c\n","> <ipython-input-3-f901e8a027d8>(59)<module>()\n","-> print (\"Training Model\")\n","(Pdb) data_management.tt.shape\n","torch.Size([255, 42, 700])\n","(Pdb) data_management.tt[0,:,0]\n","tensor([0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.6640e-04, 2.5166e-05, 5.4863e-03,\n","        9.9972e-01, 3.7542e-05, 1.2339e-04, 1.0905e-03, 3.8686e-05, 2.8149e-03,\n","        5.7711e-05, 1.5686e-04, 9.0199e-04, 3.4903e-04, 8.0690e-01, 1.1464e-03,\n","        1.0068e-03, 4.7109e-04, 8.6093e-05, 5.9469e-05, 2.6894e-01, 1.3912e-04])\n","(Pdb) data_management.ttl[0,:,0]\n","tensor([1., 0., 0., 0., 0., 0., 0., 0.])\n","(Pdb) c\n","Training Model\n","epoch 1/1000, step 1/22, loss 2.0730159282684326\n","epoch 1/1000, step 2/22, loss 2.0730295181274414\n","epoch 1/1000, step 3/22, loss 2.0728912353515625\n","epoch 1/1000, step 4/22, loss 2.0732433795928955\n","epoch 1/1000, step 5/22, loss 2.0730769634246826\n","epoch 1/1000, step 6/22, loss 2.072866678237915\n","epoch 1/1000, step 7/22, loss 2.072861671447754\n","epoch 1/1000, step 8/22, loss 2.0730764865875244\n","epoch 1/1000, step 9/22, loss 2.0728821754455566\n","epoch 1/1000, step 10/22, loss 2.0731265544891357\n","epoch 1/1000, step 11/22, loss 2.0728464126586914\n","epoch 1/1000, step 12/22, loss 2.0729901790618896\n","epoch 1/1000, step 13/22, loss 2.072941780090332\n","epoch 1/1000, step 14/22, loss 2.068880558013916\n","epoch 1/1000, step 15/22, loss 2.069049119949341\n","epoch 1/1000, step 16/22, loss 2.068941593170166\n","epoch 1/1000, step 17/22, loss 2.0690054893493652\n","epoch 1/1000, step 18/22, loss 2.069277286529541\n","epoch 1/1000, step 19/22, loss 2.0687661170959473\n","epoch 1/1000, step 20/22, loss 2.0691776275634766\n","epoch 1/1000, step 21/22, loss 2.068955898284912\n","epoch 1/1000, step 22/22, loss 2.071946144104004\n","epoch 2/1000, step 1/22, loss 2.0688605308532715\n","epoch 2/1000, step 2/22, loss 2.068895101547241\n","epoch 2/1000, step 3/22, loss 2.0686511993408203\n","epoch 2/1000, step 4/22, loss 2.068781852722168\n","epoch 2/1000, step 5/22, loss 2.0686800479888916\n","epoch 2/1000, step 6/22, loss 2.0683889389038086\n","epoch 2/1000, step 7/22, loss 2.0683131217956543\n","epoch 2/1000, step 8/22, loss 2.068765163421631\n","epoch 2/1000, step 9/22, loss 2.068376064300537\n","epoch 2/1000, step 10/22, loss 2.068704605102539\n","epoch 2/1000, step 11/22, loss 2.0683913230895996\n","epoch 2/1000, step 12/22, loss 2.0684401988983154\n","epoch 2/1000, step 13/22, loss 2.0685315132141113\n","epoch 2/1000, step 14/22, loss 2.0681421756744385\n","epoch 2/1000, step 15/22, loss 2.067457914352417\n","epoch 2/1000, step 16/22, loss 2.0672414302825928\n","epoch 2/1000, step 17/22, loss 2.0673108100891113\n","epoch 2/1000, step 18/22, loss 2.067586898803711\n","epoch 2/1000, step 19/22, loss 2.067070960998535\n","epoch 2/1000, step 20/22, loss 2.0675830841064453\n","epoch 2/1000, step 21/22, loss 2.0672974586486816\n","epoch 2/1000, step 22/22, loss 2.068523406982422\n","epoch 3/1000, step 1/22, loss 2.0672714710235596\n","epoch 3/1000, step 2/22, loss 2.0673320293426514\n","epoch 3/1000, step 3/22, loss 2.0669643878936768\n","epoch 3/1000, step 4/22, loss 2.0673892498016357\n","epoch 3/1000, step 5/22, loss 2.0671334266662598\n","epoch 3/1000, step 6/22, loss 2.0667836666107178\n","epoch 3/1000, step 7/22, loss 2.066739559173584\n","epoch 3/1000, step 8/22, loss 2.067218780517578\n","epoch 3/1000, step 9/22, loss 2.0669047832489014\n","epoch 3/1000, step 10/22, loss 2.0672786235809326\n","epoch 3/1000, step 11/22, loss 2.066856861114502\n","epoch 3/1000, step 12/22, loss 2.0669403076171875\n","epoch 3/1000, step 13/22, loss 2.067063570022583\n","epoch 3/1000, step 14/22, loss 2.0665361881256104\n","epoch 3/1000, step 15/22, loss 2.0669045448303223\n","epoch 3/1000, step 16/22, loss 2.066730499267578\n","epoch 3/1000, step 17/22, loss 2.066781759262085\n","epoch 3/1000, step 18/22, loss 2.0670902729034424\n","epoch 3/1000, step 19/22, loss 2.066572904586792\n","epoch 3/1000, step 20/22, loss 2.0671138763427734\n","epoch 3/1000, step 21/22, loss 2.06687593460083\n","epoch 3/1000, step 22/22, loss 2.0661709308624268\n","epoch 4/1000, step 1/22, loss 2.066765546798706\n","epoch 4/1000, step 2/22, loss 2.0668649673461914\n","epoch 4/1000, step 3/22, loss 2.0664453506469727\n","epoch 4/1000, step 4/22, loss 2.0668907165527344\n","epoch 4/1000, step 5/22, loss 2.0666942596435547\n","epoch 4/1000, step 6/22, loss 2.0663092136383057\n","epoch 4/1000, step 7/22, loss 2.0662436485290527\n","epoch 4/1000, step 8/22, loss 2.066760540008545\n","epoch 4/1000, step 9/22, loss 2.066450357437134\n","epoch 4/1000, step 10/22, loss 2.0668721199035645\n","epoch 4/1000, step 11/22, loss 2.066401243209839\n","epoch 4/1000, step 12/22, loss 2.0664896965026855\n","epoch 4/1000, step 13/22, loss 2.0666563510894775\n","epoch 4/1000, step 14/22, loss 2.06608510017395\n","epoch 4/1000, step 15/22, loss 2.066450834274292\n","epoch 4/1000, step 16/22, loss 2.065304756164551\n","epoch 4/1000, step 17/22, loss 2.06535005569458\n","epoch 4/1000, step 18/22, loss 2.0657131671905518\n","epoch 4/1000, step 19/22, loss 2.0651514530181885\n","epoch 4/1000, step 20/22, loss 2.06575083732605\n","epoch 4/1000, step 21/22, loss 2.065420389175415\n","epoch 4/1000, step 22/22, loss 2.0627362728118896\n","epoch 5/1000, step 1/22, loss 2.0653886795043945\n","epoch 5/1000, step 2/22, loss 2.065513849258423\n","epoch 5/1000, step 3/22, loss 2.064938545227051\n","epoch 5/1000, step 4/22, loss 2.0654027462005615\n","epoch 5/1000, step 5/22, loss 2.0653233528137207\n","epoch 5/1000, step 6/22, loss 2.064838409423828\n","epoch 5/1000, step 7/22, loss 2.064741849899292\n","epoch 5/1000, step 8/22, loss 2.0653388500213623\n","epoch 5/1000, step 9/22, loss 2.065056085586548\n","epoch 5/1000, step 10/22, loss 2.065398931503296\n","epoch 5/1000, step 11/22, loss 2.0649306774139404\n","epoch 5/1000, step 12/22, loss 2.0649988651275635\n","epoch 5/1000, step 13/22, loss 2.0652260780334473\n","epoch 5/1000, step 14/22, loss 2.064491033554077\n","epoch 5/1000, step 15/22, loss 2.0610504150390625\n","epoch 5/1000, step 16/22, loss 2.0608887672424316\n","epoch 5/1000, step 17/22, loss 2.0609312057495117\n","epoch 5/1000, step 18/22, loss 2.0614705085754395\n","epoch 5/1000, step 19/22, loss 2.060781240463257\n","epoch 5/1000, step 20/22, loss 2.0614101886749268\n","epoch 5/1000, step 21/22, loss 2.061103582382202\n","epoch 5/1000, step 22/22, loss 2.060281276702881\n","epoch 6/1000, step 1/22, loss 2.0609524250030518\n","epoch 6/1000, step 2/22, loss 2.061114549636841\n","epoch 6/1000, step 3/22, loss 2.0600788593292236\n","epoch 6/1000, step 4/22, loss 2.060617208480835\n","epoch 6/1000, step 5/22, loss 2.06083607673645\n","epoch 6/1000, step 6/22, loss 2.060260534286499\n","epoch 6/1000, step 7/22, loss 2.06007981300354\n","epoch 6/1000, step 8/22, loss 2.060835123062134\n","epoch 6/1000, step 9/22, loss 2.0603325366973877\n","epoch 6/1000, step 10/22, loss 2.060858964920044\n","epoch 6/1000, step 11/22, loss 2.060377597808838\n","epoch 6/1000, step 12/22, loss 2.060361623764038\n","epoch 6/1000, step 13/22, loss 2.059769868850708\n","epoch 6/1000, step 14/22, loss 2.0588369369506836\n","epoch 6/1000, step 15/22, loss 2.059359312057495\n","epoch 6/1000, step 16/22, loss 2.0591161251068115\n","epoch 6/1000, step 17/22, loss 2.059105634689331\n","epoch 6/1000, step 18/22, loss 2.059656858444214\n","epoch 6/1000, step 19/22, loss 2.0589587688446045\n","epoch 6/1000, step 20/22, loss 2.0596837997436523\n","epoch 6/1000, step 21/22, loss 2.059351921081543\n","epoch 6/1000, step 22/22, loss 2.0556933879852295\n","epoch 7/1000, step 1/22, loss 2.0590288639068604\n","epoch 7/1000, step 2/22, loss 2.0592501163482666\n","epoch 7/1000, step 3/22, loss 2.058316707611084\n","epoch 7/1000, step 4/22, loss 2.058875799179077\n","epoch 7/1000, step 5/22, loss 2.05916166305542\n","epoch 7/1000, step 6/22, loss 2.058515787124634\n","epoch 7/1000, step 7/22, loss 2.0583302974700928\n","epoch 7/1000, step 8/22, loss 2.0591819286346436\n","epoch 7/1000, step 9/22, loss 2.0587525367736816\n","epoch 7/1000, step 10/22, loss 2.05320143699646\n","epoch 7/1000, step 11/22, loss 2.052543878555298\n","epoch 7/1000, step 12/22, loss 2.0526249408721924\n","epoch 7/1000, step 13/22, loss 2.052861452102661\n","epoch 7/1000, step 14/22, loss 2.0516483783721924\n","epoch 7/1000, step 15/22, loss 2.0525145530700684\n","epoch 7/1000, step 16/22, loss 2.0520215034484863\n","epoch 7/1000, step 17/22, loss 2.0520880222320557\n","epoch 7/1000, step 18/22, loss 2.0527029037475586\n","epoch 7/1000, step 19/22, loss 2.0514190196990967\n","epoch 7/1000, step 20/22, loss 2.0527079105377197\n","epoch 7/1000, step 21/22, loss 2.0517916679382324\n","epoch 7/1000, step 22/22, loss 2.053209066390991\n","epoch 8/1000, step 1/22, loss 2.051877737045288\n","epoch 8/1000, step 2/22, loss 2.0518805980682373\n","epoch 8/1000, step 3/22, loss 2.051058292388916\n","epoch 8/1000, step 4/22, loss 2.051995038986206\n","epoch 8/1000, step 5/22, loss 2.050968885421753\n","epoch 8/1000, step 6/22, loss 2.050272226333618\n","epoch 8/1000, step 7/22, loss 2.050210952758789\n","epoch 8/1000, step 8/22, loss 2.051147222518921\n","epoch 8/1000, step 9/22, loss 2.0504562854766846\n","epoch 8/1000, step 10/22, loss 2.0511810779571533\n","epoch 8/1000, step 11/22, loss 2.050562858581543\n","epoch 8/1000, step 12/22, loss 2.050605058670044\n","epoch 8/1000, step 13/22, loss 2.050907611846924\n","epoch 8/1000, step 14/22, loss 2.049408197402954\n","epoch 8/1000, step 15/22, loss 2.0503664016723633\n","epoch 8/1000, step 16/22, loss 2.049893617630005\n","epoch 8/1000, step 17/22, loss 2.0499508380889893\n","epoch 8/1000, step 18/22, loss 2.0505354404449463\n","epoch 8/1000, step 19/22, loss 2.049569606781006\n","epoch 8/1000, step 20/22, loss 2.0508217811584473\n","epoch 8/1000, step 21/22, loss 2.0499911308288574\n","epoch 8/1000, step 22/22, loss 2.049313545227051\n","epoch 9/1000, step 1/22, loss 2.046367883682251\n","epoch 9/1000, step 2/22, loss 2.046445369720459\n","epoch 9/1000, step 3/22, loss 2.0453553199768066\n","epoch 9/1000, step 4/22, loss 2.046379804611206\n","epoch 9/1000, step 5/22, loss 2.046412467956543\n","epoch 9/1000, step 6/22, loss 2.0456409454345703\n","epoch 9/1000, step 7/22, loss 2.04545259475708\n","epoch 9/1000, step 8/22, loss 2.0466902256011963\n","epoch 9/1000, step 9/22, loss 2.045557975769043\n","epoch 9/1000, step 10/22, loss 2.046529769897461\n","epoch 9/1000, step 11/22, loss 2.045867681503296\n","epoch 9/1000, step 12/22, loss 2.045789957046509\n","epoch 9/1000, step 13/22, loss 2.0462186336517334\n","epoch 9/1000, step 14/22, loss 2.044846296310425\n","epoch 9/1000, step 15/22, loss 2.045623540878296\n","epoch 9/1000, step 16/22, loss 2.0452048778533936\n","epoch 9/1000, step 17/22, loss 2.0443003177642822\n","epoch 9/1000, step 18/22, loss 2.0451345443725586\n","epoch 9/1000, step 19/22, loss 2.0439743995666504\n","epoch 9/1000, step 20/22, loss 2.045344829559326\n","epoch 9/1000, step 21/22, loss 2.044408082962036\n","epoch 9/1000, step 22/22, loss 2.0454466342926025\n","epoch 10/1000, step 1/22, loss 2.04457950592041\n","epoch 10/1000, step 2/22, loss 2.044708490371704\n","epoch 10/1000, step 3/22, loss 2.043215274810791\n","epoch 10/1000, step 4/22, loss 2.044257402420044\n","epoch 10/1000, step 5/22, loss 2.044498920440674\n","epoch 10/1000, step 6/22, loss 2.0436289310455322\n","epoch 10/1000, step 7/22, loss 2.043391227722168\n","epoch 10/1000, step 8/22, loss 2.044666290283203\n","epoch 10/1000, step 9/22, loss 2.0438451766967773\n","epoch 10/1000, step 10/22, loss 2.044807195663452\n","epoch 10/1000, step 11/22, loss 2.044110059738159\n","epoch 10/1000, step 12/22, loss 2.044032096862793\n","epoch 10/1000, step 13/22, loss 2.044602394104004\n","epoch 10/1000, step 14/22, loss 2.043003797531128\n","epoch 10/1000, step 15/22, loss 2.044001579284668\n","epoch 10/1000, step 16/22, loss 2.043508768081665\n","epoch 10/1000, step 17/22, loss 2.043583869934082\n","epoch 10/1000, step 18/22, loss 2.0442054271698\n","epoch 10/1000, step 19/22, loss 2.0430924892425537\n","epoch 10/1000, step 20/22, loss 2.0445125102996826\n","epoch 10/1000, step 21/22, loss 2.043621063232422\n","epoch 10/1000, step 22/22, loss 2.0414044857025146\n","epoch 11/1000, step 1/22, loss 2.043691873550415\n","epoch 11/1000, step 2/22, loss 2.0438878536224365\n","epoch 11/1000, step 3/22, loss 2.0415587425231934\n","epoch 11/1000, step 4/22, loss 2.0426218509674072\n","epoch 11/1000, step 5/22, loss 2.0429787635803223\n","epoch 11/1000, step 6/22, loss 2.042020797729492\n","epoch 11/1000, step 7/22, loss 2.041762590408325\n","epoch 11/1000, step 8/22, loss 2.043128490447998\n","epoch 11/1000, step 9/22, loss 2.0423152446746826\n","epoch 11/1000, step 10/22, loss 2.043074607849121\n","epoch 11/1000, step 11/22, loss 2.042283773422241\n","epoch 11/1000, step 12/22, loss 2.0421855449676514\n","epoch 11/1000, step 13/22, loss 2.0428550243377686\n","epoch 11/1000, step 14/22, loss 2.0410306453704834\n","epoch 11/1000, step 15/22, loss 2.042198896408081\n","epoch 11/1000, step 16/22, loss 2.041635036468506\n","epoch 11/1000, step 17/22, loss 2.037788152694702\n","epoch 11/1000, step 18/22, loss 2.0388829708099365\n","epoch 11/1000, step 19/22, loss 2.0375428199768066\n","epoch 11/1000, step 20/22, loss 2.0391318798065186\n","epoch 11/1000, step 21/22, loss 2.0380802154541016\n","epoch 11/1000, step 22/22, loss 2.037328004837036\n","epoch 12/1000, step 1/22, loss 2.038158893585205\n","epoch 12/1000, step 2/22, loss 2.038144588470459\n","epoch 12/1000, step 3/22, loss 2.0364837646484375\n","epoch 12/1000, step 4/22, loss 2.0376245975494385\n","epoch 12/1000, step 5/22, loss 2.0381104946136475\n","epoch 12/1000, step 6/22, loss 2.036961555480957\n","epoch 12/1000, step 7/22, loss 2.036592960357666\n","epoch 12/1000, step 8/22, loss 2.0382392406463623\n","epoch 12/1000, step 9/22, loss 2.0362629890441895\n","epoch 12/1000, step 10/22, loss 2.0373635292053223\n","epoch 12/1000, step 11/22, loss 2.0365331172943115\n","epoch 12/1000, step 12/22, loss 2.0363118648529053\n","epoch 12/1000, step 13/22, loss 2.037095546722412\n","epoch 12/1000, step 14/22, loss 2.0351202487945557\n","epoch 12/1000, step 15/22, loss 2.0360655784606934\n","epoch 12/1000, step 16/22, loss 2.035517692565918\n","epoch 12/1000, step 17/22, loss 2.035550117492676\n","epoch 12/1000, step 18/22, loss 2.0366549491882324\n","epoch 12/1000, step 19/22, loss 2.035299301147461\n","epoch 12/1000, step 20/22, loss 2.0369551181793213\n","epoch 12/1000, step 21/22, loss 2.0358855724334717\n","epoch 12/1000, step 22/22, loss 2.0321545600891113\n","epoch 13/1000, step 1/22, loss 2.0299315452575684\n","epoch 13/1000, step 2/22, loss 2.029996156692505\n","epoch 13/1000, step 3/22, loss 2.0283544063568115\n","epoch 13/1000, step 4/22, loss 2.02999210357666\n","epoch 13/1000, step 5/22, loss 2.0299744606018066\n","epoch 13/1000, step 6/22, loss 2.0285897254943848\n","epoch 13/1000, step 7/22, loss 2.0284829139709473\n","epoch 13/1000, step 8/22, loss 2.0300796031951904\n","epoch 13/1000, step 9/22, loss 2.02878737449646\n","epoch 13/1000, step 10/22, loss 2.030226230621338\n","epoch 13/1000, step 11/22, loss 2.0292153358459473\n","epoch 13/1000, step 12/22, loss 2.028109550476074\n","epoch 13/1000, step 13/22, loss 2.028818368911743\n","epoch 13/1000, step 14/22, loss 2.0264668464660645\n","epoch 13/1000, step 15/22, loss 2.0280795097351074\n","epoch 13/1000, step 16/22, loss 2.0272295475006104\n","epoch 13/1000, step 17/22, loss 2.0273544788360596\n","epoch 13/1000, step 18/22, loss 2.028230905532837\n","epoch 13/1000, step 19/22, loss 2.02669358253479\n","epoch 13/1000, step 20/22, loss 2.0288472175598145\n","epoch 13/1000, step 21/22, loss 2.0272216796875\n","epoch 13/1000, step 22/22, loss 2.0265440940856934\n","epoch 14/1000, step 1/22, loss 2.027698516845703\n","epoch 14/1000, step 2/22, loss 2.023972749710083\n","epoch 14/1000, step 3/22, loss 2.0220372676849365\n","epoch 14/1000, step 4/22, loss 2.023672580718994\n","epoch 14/1000, step 5/22, loss 2.023953914642334\n","epoch 14/1000, step 6/22, loss 2.022676706314087\n","epoch 14/1000, step 7/22, loss 2.022447347640991\n","epoch 14/1000, step 8/22, loss 2.0241734981536865\n","epoch 14/1000, step 9/22, loss 2.0227277278900146\n","epoch 14/1000, step 10/22, loss 2.0242528915405273\n","epoch 14/1000, step 11/22, loss 2.0232958793640137\n","epoch 14/1000, step 12/22, loss 2.0230650901794434\n","epoch 14/1000, step 13/22, loss 2.022942543029785\n","epoch 14/1000, step 14/22, loss 2.0204198360443115\n","epoch 14/1000, step 15/22, loss 2.022071123123169\n","epoch 14/1000, step 16/22, loss 2.0211756229400635\n","epoch 14/1000, step 17/22, loss 2.021329164505005\n","epoch 14/1000, step 18/22, loss 2.022369384765625\n","epoch 14/1000, step 19/22, loss 2.0206942558288574\n","epoch 14/1000, step 20/22, loss 2.0230274200439453\n","epoch 14/1000, step 21/22, loss 2.0212860107421875\n","epoch 14/1000, step 22/22, loss 2.0206689834594727\n","epoch 15/1000, step 1/22, loss 2.0218002796173096\n","epoch 15/1000, step 2/22, loss 2.021934747695923\n","epoch 15/1000, step 3/22, loss 2.0198395252227783\n","epoch 15/1000, step 4/22, loss 2.021498918533325\n","epoch 15/1000, step 5/22, loss 2.0219435691833496\n","epoch 15/1000, step 6/22, loss 2.0205650329589844\n","epoch 15/1000, step 7/22, loss 2.020169973373413\n","epoch 15/1000, step 8/22, loss 2.022085666656494\n","epoch 15/1000, step 9/22, loss 2.020686149597168\n","epoch 15/1000, step 10/22, loss 2.022325277328491\n","epoch 15/1000, step 11/22, loss 2.0212976932525635\n","epoch 15/1000, step 12/22, loss 2.0200629234313965\n","epoch 15/1000, step 13/22, loss 2.0210132598876953\n","epoch 15/1000, step 14/22, loss 2.0182511806488037\n","epoch 15/1000, step 15/22, loss 2.0201165676116943\n","epoch 15/1000, step 16/22, loss 2.019199848175049\n","epoch 15/1000, step 17/22, loss 2.0191025733947754\n","epoch 15/1000, step 18/22, loss 2.0204403400421143\n","epoch 15/1000, step 19/22, loss 2.01865553855896\n","epoch 15/1000, step 20/22, loss 2.021084785461426\n","epoch 15/1000, step 21/22, loss 2.019299030303955\n","epoch 15/1000, step 22/22, loss 2.0148119926452637\n","epoch 16/1000, step 1/22, loss 2.0159013271331787\n","epoch 16/1000, step 2/22, loss 2.0159740447998047\n","epoch 16/1000, step 3/22, loss 2.0135693550109863\n","epoch 16/1000, step 4/22, loss 2.0151312351226807\n","epoch 16/1000, step 5/22, loss 2.0157408714294434\n","epoch 16/1000, step 6/22, loss 2.014155626296997\n","epoch 16/1000, step 7/22, loss 2.0137877464294434\n","epoch 16/1000, step 8/22, loss 2.0160181522369385\n","epoch 16/1000, step 9/22, loss 2.013446807861328\n","epoch 16/1000, step 10/22, loss 2.0151681900024414\n","epoch 16/1000, step 11/22, loss 2.0140130519866943\n","epoch 16/1000, step 12/22, loss 2.013640880584717\n","epoch 16/1000, step 13/22, loss 2.0145604610443115\n","epoch 16/1000, step 14/22, loss 2.011582374572754\n","epoch 16/1000, step 15/22, loss 2.013339042663574\n","epoch 16/1000, step 16/22, loss 2.0124473571777344\n","epoch 16/1000, step 17/22, loss 2.012509346008301\n","epoch 16/1000, step 18/22, loss 2.0081915855407715\n","epoch 16/1000, step 19/22, loss 2.0059285163879395\n","epoch 16/1000, step 20/22, loss 2.008934736251831\n","epoch 16/1000, step 21/22, loss 2.0066027641296387\n","epoch 16/1000, step 22/22, loss 2.0074057579040527\n","epoch 17/1000, step 1/22, loss 2.007077932357788\n","epoch 17/1000, step 2/22, loss 2.0070769786834717\n","epoch 17/1000, step 3/22, loss 2.004551649093628\n","epoch 17/1000, step 4/22, loss 2.0057897567749023\n","epoch 17/1000, step 5/22, loss 2.006014108657837\n","epoch 17/1000, step 6/22, loss 2.004286766052246\n","epoch 17/1000, step 7/22, loss 2.004148006439209\n","epoch 17/1000, step 8/22, loss 2.006190299987793\n","epoch 17/1000, step 9/22, loss 2.004535675048828\n","epoch 17/1000, step 10/22, loss 2.0063984394073486\n","epoch 17/1000, step 11/22, loss 2.0051512718200684\n","epoch 17/1000, step 12/22, loss 2.0008907318115234\n","epoch 17/1000, step 13/22, loss 2.002082347869873\n","epoch 17/1000, step 14/22, loss 1.9986802339553833\n","epoch 17/1000, step 15/22, loss 2.0007853507995605\n","epoch 17/1000, step 16/22, loss 1.999465823173523\n","epoch 17/1000, step 17/22, loss 1.9995884895324707\n","epoch 17/1000, step 18/22, loss 2.0012729167938232\n","epoch 17/1000, step 19/22, loss 1.998956322669983\n","epoch 17/1000, step 20/22, loss 2.0011653900146484\n","epoch 17/1000, step 21/22, loss 1.9987562894821167\n","epoch 17/1000, step 22/22, loss 1.9996535778045654\n","epoch 18/1000, step 1/22, loss 1.999436855316162\n","epoch 18/1000, step 2/22, loss 1.9993324279785156\n","epoch 18/1000, step 3/22, loss 1.9963315725326538\n","epoch 18/1000, step 4/22, loss 1.9986482858657837\n","epoch 18/1000, step 5/22, loss 1.9992603063583374\n","epoch 18/1000, step 6/22, loss 1.9973829984664917\n","epoch 18/1000, step 7/22, loss 1.9970698356628418\n","epoch 18/1000, step 8/22, loss 1.9997141361236572\n","epoch 18/1000, step 9/22, loss 1.9976743459701538\n","epoch 18/1000, step 10/22, loss 1.9998290538787842\n","epoch 18/1000, step 11/22, loss 1.998400330543518\n","epoch 18/1000, step 12/22, loss 1.9970066547393799\n","epoch 18/1000, step 13/22, loss 1.9983822107315063\n","epoch 18/1000, step 14/22, loss 1.9946101903915405\n","epoch 18/1000, step 15/22, loss 1.9971494674682617\n","epoch 18/1000, step 16/22, loss 1.9956673383712769\n","epoch 18/1000, step 17/22, loss 1.9957566261291504\n","epoch 18/1000, step 18/22, loss 1.997623324394226\n","epoch 18/1000, step 19/22, loss 1.9913382530212402\n","epoch 18/1000, step 20/22, loss 1.9947627782821655\n","epoch 18/1000, step 21/22, loss 1.9921542406082153\n","epoch 18/1000, step 22/22, loss 1.9904136657714844\n","epoch 19/1000, step 1/22, loss 1.9925740957260132\n","epoch 19/1000, step 2/22, loss 1.9927425384521484\n","epoch 19/1000, step 3/22, loss 1.9894517660140991\n","epoch 19/1000, step 4/22, loss 1.990860939025879\n","epoch 19/1000, step 5/22, loss 1.9917285442352295\n","epoch 19/1000, step 6/22, loss 1.9895724058151245\n","epoch 19/1000, step 7/22, loss 1.98896062374115\n","epoch 19/1000, step 8/22, loss 1.9919545650482178\n","epoch 19/1000, step 9/22, loss 1.9898858070373535\n","epoch 19/1000, step 10/22, loss 1.9866878986358643\n","epoch 19/1000, step 11/22, loss 1.9850884675979614\n","epoch 19/1000, step 12/22, loss 1.984616994857788\n","epoch 19/1000, step 13/22, loss 1.9858808517456055\n","epoch 19/1000, step 14/22, loss 1.9816159009933472\n","epoch 19/1000, step 15/22, loss 1.9844250679016113\n","epoch 19/1000, step 16/22, loss 1.9819607734680176\n","epoch 19/1000, step 17/22, loss 1.9821369647979736\n","epoch 19/1000, step 18/22, loss 1.9842473268508911\n","epoch 19/1000, step 19/22, loss 1.9812008142471313\n","epoch 19/1000, step 20/22, loss 1.985184669494629\n","epoch 19/1000, step 21/22, loss 1.9821749925613403\n","epoch 19/1000, step 22/22, loss 1.980502963066101\n","epoch 20/1000, step 1/22, loss 1.9791338443756104\n","epoch 20/1000, step 2/22, loss 1.9790363311767578\n","epoch 20/1000, step 3/22, loss 1.9753339290618896\n","epoch 20/1000, step 4/22, loss 1.9782379865646362\n","epoch 20/1000, step 5/22, loss 1.9779685735702515\n","epoch 20/1000, step 6/22, loss 1.9755403995513916\n","epoch 20/1000, step 7/22, loss 1.9751530885696411\n","epoch 20/1000, step 8/22, loss 1.9783846139907837\n","epoch 20/1000, step 9/22, loss 1.9758274555206299\n","epoch 20/1000, step 10/22, loss 1.9786994457244873\n","epoch 20/1000, step 11/22, loss 1.9770197868347168\n","epoch 20/1000, step 12/22, loss 1.9763606786727905\n","epoch 20/1000, step 13/22, loss 1.977980613708496\n","epoch 20/1000, step 14/22, loss 1.9733209609985352\n","epoch 20/1000, step 15/22, loss 1.9762189388275146\n","epoch 20/1000, step 16/22, loss 1.973718285560608\n","epoch 20/1000, step 17/22, loss 1.9738506078720093\n","epoch 20/1000, step 18/22, loss 1.9760794639587402\n","epoch 20/1000, step 19/22, loss 1.973007321357727\n","epoch 20/1000, step 20/22, loss 1.973394751548767\n","epoch 20/1000, step 21/22, loss 1.9701067209243774\n","epoch 20/1000, step 22/22, loss 1.9687196016311646\n","epoch 21/1000, step 1/22, loss 1.970576524734497\n","epoch 21/1000, step 2/22, loss 1.9707969427108765\n","epoch 21/1000, step 3/22, loss 1.9655181169509888\n","epoch 21/1000, step 4/22, loss 1.968552589416504\n","epoch 21/1000, step 5/22, loss 1.9694621562957764\n","epoch 21/1000, step 6/22, loss 1.9666502475738525\n","epoch 21/1000, step 7/22, loss 1.966055154800415\n","epoch 21/1000, step 8/22, loss 1.964463472366333\n","epoch 21/1000, step 9/22, loss 1.9615174531936646\n","epoch 21/1000, step 10/22, loss 1.9644941091537476\n","epoch 21/1000, step 11/22, loss 1.962477684020996\n","epoch 21/1000, step 12/22, loss 1.9607505798339844\n","epoch 21/1000, step 13/22, loss 1.9626625776290894\n","epoch 21/1000, step 14/22, loss 1.9570021629333496\n","epoch 21/1000, step 15/22, loss 1.9605299234390259\n","epoch 21/1000, step 16/22, loss 1.9546159505844116\n","epoch 21/1000, step 17/22, loss 1.9547390937805176\n","epoch 21/1000, step 18/22, loss 1.9573420286178589\n","epoch 21/1000, step 19/22, loss 1.9536471366882324\n","epoch 21/1000, step 20/22, loss 1.9577085971832275\n","epoch 21/1000, step 21/22, loss 1.9537968635559082\n","epoch 21/1000, step 22/22, loss 1.9529212713241577\n","epoch 22/1000, step 1/22, loss 1.9546316862106323\n","epoch 22/1000, step 2/22, loss 1.9549520015716553\n","epoch 22/1000, step 3/22, loss 1.95021653175354\n","epoch 22/1000, step 4/22, loss 1.9536648988723755\n","epoch 22/1000, step 5/22, loss 1.954642653465271\n","epoch 22/1000, step 6/22, loss 1.9506330490112305\n","epoch 22/1000, step 7/22, loss 1.9500987529754639\n","epoch 22/1000, step 8/22, loss 1.9542880058288574\n","epoch 22/1000, step 9/22, loss 1.9512367248535156\n","epoch 22/1000, step 10/22, loss 1.9507185220718384\n","epoch 22/1000, step 11/22, loss 1.9484572410583496\n","epoch 22/1000, step 12/22, loss 1.9474177360534668\n","epoch 22/1000, step 13/22, loss 1.9497590065002441\n","epoch 22/1000, step 14/22, loss 1.942470908164978\n","epoch 22/1000, step 15/22, loss 1.9462672472000122\n","epoch 22/1000, step 16/22, loss 1.944137454032898\n","epoch 22/1000, step 17/22, loss 1.9390218257904053\n","epoch 22/1000, step 18/22, loss 1.9419443607330322\n","epoch 22/1000, step 19/22, loss 1.9374043941497803\n","epoch 22/1000, step 20/22, loss 1.9423729181289673\n","epoch 22/1000, step 21/22, loss 1.9377446174621582\n","epoch 22/1000, step 22/22, loss 1.935750126838684\n","epoch 23/1000, step 1/22, loss 1.9387437105178833\n","epoch 23/1000, step 2/22, loss 1.9348989725112915\n","epoch 23/1000, step 3/22, loss 1.9292196035385132\n","epoch 23/1000, step 4/22, loss 1.9335111379623413\n","epoch 23/1000, step 5/22, loss 1.9333419799804688\n","epoch 23/1000, step 6/22, loss 1.9296501874923706\n","epoch 23/1000, step 7/22, loss 1.9289613962173462\n","epoch 23/1000, step 8/22, loss 1.9340993165969849\n","epoch 23/1000, step 9/22, loss 1.9302771091461182\n","epoch 23/1000, step 10/22, loss 1.9343913793563843\n","epoch 23/1000, step 11/22, loss 1.9307594299316406\n","epoch 23/1000, step 12/22, loss 1.9296011924743652\n","epoch 23/1000, step 13/22, loss 1.9322751760482788\n","epoch 23/1000, step 14/22, loss 1.9211345911026\n","epoch 23/1000, step 15/22, loss 1.9254502058029175\n","epoch 23/1000, step 16/22, loss 1.923105001449585\n","epoch 23/1000, step 17/22, loss 1.9222313165664673\n","epoch 23/1000, step 18/22, loss 1.9257123470306396\n","epoch 23/1000, step 19/22, loss 1.9155505895614624\n","epoch 23/1000, step 20/22, loss 1.922468900680542\n","epoch 23/1000, step 21/22, loss 1.9170057773590088\n","epoch 23/1000, step 22/22, loss 1.9149490594863892\n","epoch 24/1000, step 1/22, loss 1.9171552658081055\n","epoch 24/1000, step 2/22, loss 1.9175060987472534\n","epoch 24/1000, step 3/22, loss 1.9070608615875244\n","epoch 24/1000, step 4/22, loss 1.9117984771728516\n","epoch 24/1000, step 5/22, loss 1.9118257761001587\n","epoch 24/1000, step 6/22, loss 1.9077808856964111\n","epoch 24/1000, step 7/22, loss 1.9067119359970093\n","epoch 24/1000, step 8/22, loss 1.912664771080017\n","epoch 24/1000, step 9/22, loss 1.9081608057022095\n","epoch 24/1000, step 10/22, loss 1.9119086265563965\n","epoch 24/1000, step 11/22, loss 1.908572793006897\n","epoch 24/1000, step 12/22, loss 1.907389760017395\n","epoch 24/1000, step 13/22, loss 1.9064714908599854\n","epoch 24/1000, step 14/22, loss 1.897829532623291\n","epoch 24/1000, step 15/22, loss 1.9021052122116089\n","epoch 24/1000, step 16/22, loss 1.8989853858947754\n","epoch 24/1000, step 17/22, loss 1.8942924737930298\n","epoch 24/1000, step 18/22, loss 1.8980448246002197\n","epoch 24/1000, step 19/22, loss 1.8914414644241333\n","epoch 24/1000, step 20/22, loss 1.899240493774414\n","epoch 24/1000, step 21/22, loss 1.8929123878479004\n","epoch 24/1000, step 22/22, loss 1.8894864320755005\n","epoch 25/1000, step 1/22, loss 1.8902127742767334\n","epoch 25/1000, step 2/22, loss 1.8894810676574707\n","epoch 25/1000, step 3/22, loss 1.881826400756836\n","epoch 25/1000, step 4/22, loss 1.8875727653503418\n","epoch 25/1000, step 5/22, loss 1.8885302543640137\n","epoch 25/1000, step 6/22, loss 1.8828144073486328\n","epoch 25/1000, step 7/22, loss 1.8816150426864624\n","epoch 25/1000, step 8/22, loss 1.8841028213500977\n","epoch 25/1000, step 9/22, loss 1.8787144422531128\n","epoch 25/1000, step 10/22, loss 1.883203148841858\n","epoch 25/1000, step 11/22, loss 1.8794810771942139\n","epoch 25/1000, step 12/22, loss 1.8734703063964844\n","epoch 25/1000, step 13/22, loss 1.8767309188842773\n","epoch 25/1000, step 14/22, loss 1.8657633066177368\n","epoch 25/1000, step 15/22, loss 1.8678343296051025\n","epoch 25/1000, step 16/22, loss 1.8642420768737793\n","epoch 25/1000, step 17/22, loss 1.8635871410369873\n","epoch 25/1000, step 18/22, loss 1.8682327270507812\n","epoch 25/1000, step 19/22, loss 1.861919641494751\n","epoch 25/1000, step 20/22, loss 1.8707897663116455\n","epoch 25/1000, step 21/22, loss 1.8626121282577515\n","epoch 25/1000, step 22/22, loss 1.8563175201416016\n","epoch 26/1000, step 1/22, loss 1.8598955869674683\n","epoch 26/1000, step 2/22, loss 1.8591575622558594\n","epoch 26/1000, step 3/22, loss 1.850023865699768\n","epoch 26/1000, step 4/22, loss 1.8519037961959839\n","epoch 26/1000, step 5/22, loss 1.8521555662155151\n","epoch 26/1000, step 6/22, loss 1.846401572227478\n","epoch 26/1000, step 7/22, loss 1.8409990072250366\n","epoch 26/1000, step 8/22, loss 1.8477580547332764\n","epoch 26/1000, step 9/22, loss 1.8415199518203735\n","epoch 26/1000, step 10/22, loss 1.848122000694275\n","epoch 26/1000, step 11/22, loss 1.8437587022781372\n","epoch 26/1000, step 12/22, loss 1.8407036066055298\n","epoch 26/1000, step 13/22, loss 1.8408609628677368\n","epoch 26/1000, step 14/22, loss 1.8289358615875244\n","epoch 26/1000, step 15/22, loss 1.8350772857666016\n","epoch 26/1000, step 16/22, loss 1.8266866207122803\n","epoch 26/1000, step 17/22, loss 1.8257259130477905\n","epoch 26/1000, step 18/22, loss 1.831007957458496\n","epoch 26/1000, step 19/22, loss 1.8191652297973633\n","epoch 26/1000, step 20/22, loss 1.82899010181427\n","epoch 26/1000, step 21/22, loss 1.8200246095657349\n","epoch 26/1000, step 22/22, loss 1.815259575843811\n","epoch 27/1000, step 1/22, loss 1.821163535118103\n","epoch 27/1000, step 2/22, loss 1.8171334266662598\n","epoch 27/1000, step 3/22, loss 1.8064161539077759\n","epoch 27/1000, step 4/22, loss 1.8128212690353394\n","epoch 27/1000, step 5/22, loss 1.8105472326278687\n","epoch 27/1000, step 6/22, loss 1.8028799295425415\n","epoch 27/1000, step 7/22, loss 1.8012993335723877\n","epoch 27/1000, step 8/22, loss 1.8064388036727905\n","epoch 27/1000, step 9/22, loss 1.7978368997573853\n","epoch 27/1000, step 10/22, loss 1.805830717086792\n","epoch 27/1000, step 11/22, loss 1.7995141744613647\n","epoch 27/1000, step 12/22, loss 1.7970046997070312\n","epoch 27/1000, step 13/22, loss 1.7976608276367188\n","epoch 27/1000, step 14/22, loss 1.7820954322814941\n","epoch 27/1000, step 15/22, loss 1.7872036695480347\n","epoch 27/1000, step 16/22, loss 1.7809334993362427\n","epoch 27/1000, step 17/22, loss 1.7765754461288452\n","epoch 27/1000, step 18/22, loss 1.7832732200622559\n","epoch 27/1000, step 19/22, loss 1.7728132009506226\n","epoch 27/1000, step 20/22, loss 1.7856825590133667\n","epoch 27/1000, step 21/22, loss 1.7741798162460327\n","epoch 27/1000, step 22/22, loss 1.7667185068130493\n","epoch 28/1000, step 1/22, loss 1.7711244821548462\n","epoch 28/1000, step 2/22, loss 1.7714331150054932\n","epoch 28/1000, step 3/22, loss 1.7547050714492798\n","epoch 28/1000, step 4/22, loss 1.7628636360168457\n","epoch 28/1000, step 5/22, loss 1.7603495121002197\n","epoch 28/1000, step 6/22, loss 1.751089334487915\n","epoch 28/1000, step 7/22, loss 1.7495007514953613\n","epoch 28/1000, step 8/22, loss 1.7595125436782837\n","epoch 28/1000, step 9/22, loss 1.7462527751922607\n","epoch 28/1000, step 10/22, loss 1.7544978857040405\n","epoch 28/1000, step 11/22, loss 1.7484327554702759\n","epoch 28/1000, step 12/22, loss 1.742205262184143\n","epoch 28/1000, step 13/22, loss 1.74724280834198\n","epoch 28/1000, step 14/22, loss 1.7259525060653687\n","epoch 28/1000, step 15/22, loss 1.7354539632797241\n","epoch 28/1000, step 16/22, loss 1.72959303855896\n","epoch 28/1000, step 17/22, loss 1.7286875247955322\n","epoch 28/1000, step 18/22, loss 1.7322434186935425\n","epoch 28/1000, step 19/22, loss 1.7201552391052246\n","epoch 28/1000, step 20/22, loss 1.7322059869766235\n","epoch 28/1000, step 21/22, loss 1.7195860147476196\n","epoch 28/1000, step 22/22, loss 1.7120822668075562\n","epoch 29/1000, step 1/22, loss 1.7168760299682617\n","epoch 29/1000, step 2/22, loss 1.7162853479385376\n","epoch 29/1000, step 3/22, loss 1.701022982597351\n","epoch 29/1000, step 4/22, loss 1.7108738422393799\n","epoch 29/1000, step 5/22, loss 1.708970546722412\n","epoch 29/1000, step 6/22, loss 1.6983096599578857\n","epoch 29/1000, step 7/22, loss 1.693270206451416\n","epoch 29/1000, step 8/22, loss 1.7054654359817505\n","epoch 29/1000, step 9/22, loss 1.6952036619186401\n","epoch 29/1000, step 10/22, loss 1.701467752456665\n","epoch 29/1000, step 11/22, loss 1.693344235420227\n","epoch 29/1000, step 12/22, loss 1.6900769472122192\n","epoch 29/1000, step 13/22, loss 1.6964465379714966\n","epoch 29/1000, step 14/22, loss 1.6720038652420044\n","epoch 29/1000, step 15/22, loss 1.6831943988800049\n","epoch 29/1000, step 16/22, loss 1.6735639572143555\n","epoch 29/1000, step 17/22, loss 1.6739280223846436\n","epoch 29/1000, step 18/22, loss 1.6822021007537842\n","epoch 29/1000, step 19/22, loss 1.6648330688476562\n","epoch 29/1000, step 20/22, loss 1.681934118270874\n","epoch 29/1000, step 21/22, loss 1.6674010753631592\n","epoch 29/1000, step 22/22, loss 1.6600559949874878\n","epoch 30/1000, step 1/22, loss 1.6700221300125122\n","epoch 30/1000, step 2/22, loss 1.666029930114746\n","epoch 30/1000, step 3/22, loss 1.647756814956665\n","epoch 30/1000, step 4/22, loss 1.6582343578338623\n","epoch 30/1000, step 5/22, loss 1.6597331762313843\n","epoch 30/1000, step 6/22, loss 1.6490741968154907\n","epoch 30/1000, step 7/22, loss 1.6422122716903687\n","epoch 30/1000, step 8/22, loss 1.6568659543991089\n","epoch 30/1000, step 9/22, loss 1.645255446434021\n","epoch 30/1000, step 10/22, loss 1.6580276489257812\n","epoch 30/1000, step 11/22, loss 1.6489304304122925\n","epoch 30/1000, step 12/22, loss 1.6409369707107544\n","epoch 30/1000, step 13/22, loss 1.6493617296218872\n","epoch 30/1000, step 14/22, loss 1.6258387565612793\n","epoch 30/1000, step 15/22, loss 1.6382123231887817\n","epoch 30/1000, step 16/22, loss 1.629097580909729\n","epoch 30/1000, step 17/22, loss 1.629644751548767\n","epoch 30/1000, step 18/22, loss 1.6357922554016113\n","epoch 30/1000, step 19/22, loss 1.6202566623687744\n","epoch 30/1000, step 20/22, loss 1.6405812501907349\n","epoch 30/1000, step 21/22, loss 1.6244349479675293\n","epoch 30/1000, step 22/22, loss 1.616803765296936\n","epoch 31/1000, step 1/22, loss 1.6276181936264038\n","epoch 31/1000, step 2/22, loss 1.623948335647583\n","epoch 31/1000, step 3/22, loss 1.6045838594436646\n","epoch 31/1000, step 4/22, loss 1.6180849075317383\n","epoch 31/1000, step 5/22, loss 1.6215770244598389\n","epoch 31/1000, step 6/22, loss 1.607825756072998\n","epoch 31/1000, step 7/22, loss 1.6046448945999146\n","epoch 31/1000, step 8/22, loss 1.621675968170166\n","epoch 31/1000, step 9/22, loss 1.6044330596923828\n","epoch 31/1000, step 10/22, loss 1.6186443567276\n","epoch 31/1000, step 11/22, loss 1.6092464923858643\n","epoch 31/1000, step 12/22, loss 1.6054129600524902\n","epoch 31/1000, step 13/22, loss 1.615048885345459\n","epoch 31/1000, step 14/22, loss 1.5909987688064575\n","epoch 31/1000, step 15/22, loss 1.6055622100830078\n","epoch 31/1000, step 16/22, loss 1.5971325635910034\n","epoch 31/1000, step 17/22, loss 1.5931826829910278\n","epoch 31/1000, step 18/22, loss 1.6050114631652832\n","epoch 31/1000, step 19/22, loss 1.588895559310913\n","epoch 31/1000, step 20/22, loss 1.6108040809631348\n","epoch 31/1000, step 21/22, loss 1.5917870998382568\n","epoch 31/1000, step 22/22, loss 1.5851008892059326\n","epoch 32/1000, step 1/22, loss 1.5958377122879028\n","epoch 32/1000, step 2/22, loss 1.5970985889434814\n","epoch 32/1000, step 3/22, loss 1.5766360759735107\n","epoch 32/1000, step 4/22, loss 1.5877090692520142\n","epoch 32/1000, step 5/22, loss 1.5910260677337646\n","epoch 32/1000, step 6/22, loss 1.577731966972351\n","epoch 32/1000, step 7/22, loss 1.5756323337554932\n","epoch 32/1000, step 8/22, loss 1.5940288305282593\n","epoch 32/1000, step 9/22, loss 1.5809457302093506\n","epoch 32/1000, step 10/22, loss 1.5960497856140137\n","epoch 32/1000, step 11/22, loss 1.5872743129730225\n","epoch 32/1000, step 12/22, loss 1.5824960470199585\n","epoch 32/1000, step 13/22, loss 1.5925005674362183\n","epoch 32/1000, step 14/22, loss 1.562180519104004\n","epoch 32/1000, step 15/22, loss 1.5791155099868774\n","epoch 32/1000, step 16/22, loss 1.5703822374343872\n","epoch 32/1000, step 17/22, loss 1.5701686143875122\n","epoch 32/1000, step 18/22, loss 1.5832335948944092\n","epoch 32/1000, step 19/22, loss 1.5671217441558838\n","epoch 32/1000, step 20/22, loss 1.5907090902328491\n","epoch 32/1000, step 21/22, loss 1.5710444450378418\n","epoch 32/1000, step 22/22, loss 1.5636626482009888\n","epoch 33/1000, step 1/22, loss 1.5767501592636108\n","epoch 33/1000, step 2/22, loss 1.576941967010498\n","epoch 33/1000, step 3/22, loss 1.5559128522872925\n","epoch 33/1000, step 4/22, loss 1.572109580039978\n","epoch 33/1000, step 5/22, loss 1.5761314630508423\n","epoch 33/1000, step 6/22, loss 1.5585780143737793\n","epoch 33/1000, step 7/22, loss 1.556316614151001\n","epoch 33/1000, step 8/22, loss 1.5755009651184082\n","epoch 33/1000, step 9/22, loss 1.5604512691497803\n","epoch 33/1000, step 10/22, loss 1.5764548778533936\n","epoch 33/1000, step 11/22, loss 1.5674514770507812\n","epoch 33/1000, step 12/22, loss 1.5633221864700317\n","epoch 33/1000, step 13/22, loss 1.573917031288147\n","epoch 33/1000, step 14/22, loss 1.5477789640426636\n","epoch 33/1000, step 15/22, loss 1.5652047395706177\n","epoch 33/1000, step 16/22, loss 1.5561199188232422\n","epoch 33/1000, step 17/22, loss 1.5572068691253662\n","epoch 33/1000, step 18/22, loss 1.569236397743225\n","epoch 33/1000, step 19/22, loss 1.552931547164917\n","epoch 33/1000, step 20/22, loss 1.5768930912017822\n","epoch 33/1000, step 21/22, loss 1.5583134889602661\n","epoch 33/1000, step 22/22, loss 1.5496127605438232\n","epoch 34/1000, step 1/22, loss 1.5594052076339722\n","epoch 34/1000, step 2/22, loss 1.5608564615249634\n","epoch 34/1000, step 3/22, loss 1.5390535593032837\n","epoch 34/1000, step 4/22, loss 1.5560917854309082\n","epoch 34/1000, step 5/22, loss 1.560162901878357\n","epoch 34/1000, step 6/22, loss 1.545917272567749\n","epoch 34/1000, step 7/22, loss 1.5437076091766357\n","epoch 34/1000, step 8/22, loss 1.5636905431747437\n","epoch 34/1000, step 9/22, loss 1.5494734048843384\n","epoch 34/1000, step 10/22, loss 1.565942406654358\n","epoch 34/1000, step 11/22, loss 1.5567567348480225\n","epoch 34/1000, step 12/22, loss 1.5516715049743652\n","epoch 34/1000, step 13/22, loss 1.5624973773956299\n","epoch 34/1000, step 14/22, loss 1.5359771251678467\n","epoch 34/1000, step 15/22, loss 1.5535531044006348\n","epoch 34/1000, step 16/22, loss 1.5447100400924683\n","epoch 34/1000, step 17/22, loss 1.5458807945251465\n","epoch 34/1000, step 18/22, loss 1.559404969215393\n","epoch 34/1000, step 19/22, loss 1.5417299270629883\n","epoch 34/1000, step 20/22, loss 1.5664526224136353\n","epoch 34/1000, step 21/22, loss 1.5476858615875244\n","epoch 34/1000, step 22/22, loss 1.5395736694335938\n","epoch 35/1000, step 1/22, loss 1.5538021326065063\n","epoch 35/1000, step 2/22, loss 1.555398941040039\n","epoch 35/1000, step 3/22, loss 1.5334209203720093\n","epoch 35/1000, step 4/22, loss 1.5460759401321411\n","epoch 35/1000, step 5/22, loss 1.5501031875610352\n","epoch 35/1000, step 6/22, loss 1.5370272397994995\n","epoch 35/1000, step 7/22, loss 1.5348092317581177\n","epoch 35/1000, step 8/22, loss 1.555039882659912\n","epoch 35/1000, step 9/22, loss 1.5406190156936646\n","epoch 35/1000, step 10/22, loss 1.5575222969055176\n","epoch 35/1000, step 11/22, loss 1.5482290983200073\n","epoch 35/1000, step 12/22, loss 1.5430634021759033\n","epoch 35/1000, step 13/22, loss 1.5542120933532715\n","epoch 35/1000, step 14/22, loss 1.526994228363037\n","epoch 35/1000, step 15/22, loss 1.5453068017959595\n","epoch 35/1000, step 16/22, loss 1.5362160205841064\n","epoch 35/1000, step 17/22, loss 1.5371770858764648\n","epoch 35/1000, step 18/22, loss 1.5510635375976562\n","epoch 35/1000, step 19/22, loss 1.5341767072677612\n","epoch 35/1000, step 20/22, loss 1.5594860315322876\n","epoch 35/1000, step 21/22, loss 1.5401569604873657\n","epoch 35/1000, step 22/22, loss 1.5335147380828857\n","epoch 36/1000, step 1/22, loss 1.546708583831787\n","epoch 36/1000, step 2/22, loss 1.5482549667358398\n","epoch 36/1000, step 3/22, loss 1.5258527994155884\n","epoch 36/1000, step 4/22, loss 1.543678879737854\n","epoch 36/1000, step 5/22, loss 1.5476465225219727\n","epoch 36/1000, step 6/22, loss 1.5343906879425049\n","epoch 36/1000, step 7/22, loss 1.5321040153503418\n","epoch 36/1000, step 8/22, loss 1.5526920557022095\n","epoch 36/1000, step 9/22, loss 1.538299560546875\n","epoch 36/1000, step 10/22, loss 1.55524480342865\n","epoch 36/1000, step 11/22, loss 1.544540286064148\n","epoch 36/1000, step 12/22, loss 1.5405434370040894\n","epoch 36/1000, step 13/22, loss 1.5518648624420166\n","epoch 36/1000, step 14/22, loss 1.5242886543273926\n","epoch 36/1000, step 15/22, loss 1.5427987575531006\n","epoch 36/1000, step 16/22, loss 1.533545970916748\n","epoch 36/1000, step 17/22, loss 1.5345237255096436\n","epoch 36/1000, step 18/22, loss 1.5485281944274902\n","epoch 36/1000, step 19/22, loss 1.5316095352172852\n","epoch 36/1000, step 20/22, loss 1.5569745302200317\n","epoch 36/1000, step 21/22, loss 1.5375641584396362\n","epoch 36/1000, step 22/22, loss 1.528215765953064\n","epoch 37/1000, step 1/22, loss 1.5440607070922852\n","epoch 37/1000, step 2/22, loss 1.540605068206787\n","epoch 37/1000, step 3/22, loss 1.5178543329238892\n","epoch 37/1000, step 4/22, loss 1.5360891819000244\n","epoch 37/1000, step 5/22, loss 1.5402798652648926\n","epoch 37/1000, step 6/22, loss 1.5268208980560303\n","epoch 37/1000, step 7/22, loss 1.5245417356491089\n","epoch 37/1000, step 8/22, loss 1.5455771684646606\n","epoch 37/1000, step 9/22, loss 1.5307646989822388\n","epoch 37/1000, step 10/22, loss 1.5477919578552246\n","epoch 37/1000, step 11/22, loss 1.538341760635376\n","epoch 37/1000, step 12/22, loss 1.5342389345169067\n","epoch 37/1000, step 13/22, loss 1.5456602573394775\n","epoch 37/1000, step 14/22, loss 1.5178240537643433\n","epoch 37/1000, step 15/22, loss 1.5364755392074585\n","epoch 37/1000, step 16/22, loss 1.5271852016448975\n","epoch 37/1000, step 17/22, loss 1.528450608253479\n","epoch 37/1000, step 18/22, loss 1.5414667129516602\n","epoch 37/1000, step 19/22, loss 1.5242441892623901\n","epoch 37/1000, step 20/22, loss 1.5500760078430176\n","epoch 37/1000, step 21/22, loss 1.5303115844726562\n","epoch 37/1000, step 22/22, loss 1.525349736213684\n","epoch 38/1000, step 1/22, loss 1.5369384288787842\n","epoch 38/1000, step 2/22, loss 1.5386000871658325\n","epoch 38/1000, step 3/22, loss 1.5156607627868652\n","epoch 38/1000, step 4/22, loss 1.5339041948318481\n","epoch 38/1000, step 5/22, loss 1.5383557081222534\n","epoch 38/1000, step 6/22, loss 1.5244789123535156\n","epoch 38/1000, step 7/22, loss 1.522113561630249\n","epoch 38/1000, step 8/22, loss 1.543380856513977\n","epoch 38/1000, step 9/22, loss 1.5286003351211548\n","epoch 38/1000, step 10/22, loss 1.545987844467163\n","epoch 38/1000, step 11/22, loss 1.53630793094635\n","epoch 38/1000, step 12/22, loss 1.5321857929229736\n","epoch 38/1000, step 13/22, loss 1.5437629222869873\n","epoch 38/1000, step 14/22, loss 1.515697717666626\n","epoch 38/1000, step 15/22, loss 1.5345816612243652\n","epoch 38/1000, step 16/22, loss 1.5243394374847412\n","epoch 38/1000, step 17/22, loss 1.5256092548370361\n","epoch 38/1000, step 18/22, loss 1.539777159690857\n","epoch 38/1000, step 19/22, loss 1.5224719047546387\n","epoch 38/1000, step 20/22, loss 1.5484156608581543\n","epoch 38/1000, step 21/22, loss 1.528701901435852\n","epoch 38/1000, step 22/22, loss 1.5222293138504028\n","epoch 39/1000, step 1/22, loss 1.5353528261184692\n","epoch 39/1000, step 2/22, loss 1.5370045900344849\n","epoch 39/1000, step 3/22, loss 1.5141807794570923\n","epoch 39/1000, step 4/22, loss 1.532377004623413\n","epoch 39/1000, step 5/22, loss 1.5365890264511108\n","epoch 39/1000, step 6/22, loss 1.5231893062591553\n","epoch 39/1000, step 7/22, loss 1.5205059051513672\n","epoch 39/1000, step 8/22, loss 1.5416327714920044\n","epoch 39/1000, step 9/22, loss 1.5270005464553833\n","epoch 39/1000, step 10/22, loss 1.5442577600479126\n","epoch 39/1000, step 11/22, loss 1.5344574451446533\n","epoch 39/1000, step 12/22, loss 1.5306106805801392\n","epoch 39/1000, step 13/22, loss 1.5421032905578613\n","epoch 39/1000, step 14/22, loss 1.5140912532806396\n","epoch 39/1000, step 15/22, loss 1.5329270362854004\n","epoch 39/1000, step 16/22, loss 1.5236254930496216\n","epoch 39/1000, step 17/22, loss 1.5248348712921143\n","epoch 39/1000, step 18/22, loss 1.5389988422393799\n","epoch 39/1000, step 19/22, loss 1.521715760231018\n","epoch 39/1000, step 20/22, loss 1.547752857208252\n","epoch 39/1000, step 21/22, loss 1.5267066955566406\n","epoch 39/1000, step 22/22, loss 1.5201629400253296\n","epoch 40/1000, step 1/22, loss 1.533450722694397\n","epoch 40/1000, step 2/22, loss 1.535118579864502\n","epoch 40/1000, step 3/22, loss 1.5121909379959106\n","epoch 40/1000, step 4/22, loss 1.530306100845337\n","epoch 40/1000, step 5/22, loss 1.534765601158142\n","epoch 40/1000, step 6/22, loss 1.5211604833602905\n","epoch 40/1000, step 7/22, loss 1.5187678337097168\n","epoch 40/1000, step 8/22, loss 1.5398807525634766\n","epoch 40/1000, step 9/22, loss 1.525373935699463\n","epoch 40/1000, step 10/22, loss 1.5425835847854614\n","epoch 40/1000, step 11/22, loss 1.5327551364898682\n","epoch 40/1000, step 12/22, loss 1.5288951396942139\n","epoch 40/1000, step 13/22, loss 1.5404539108276367\n","epoch 40/1000, step 14/22, loss 1.5123385190963745\n","epoch 40/1000, step 15/22, loss 1.5313265323638916\n","epoch 40/1000, step 16/22, loss 1.5215809345245361\n","epoch 40/1000, step 17/22, loss 1.5228960514068604\n","epoch 40/1000, step 18/22, loss 1.5371350049972534\n","epoch 40/1000, step 19/22, loss 1.51978600025177\n","epoch 40/1000, step 20/22, loss 1.5458450317382812\n","epoch 40/1000, step 21/22, loss 1.5260419845581055\n","epoch 40/1000, step 22/22, loss 1.5180996656417847\n","epoch 41/1000, step 1/22, loss 1.5328055620193481\n","epoch 41/1000, step 2/22, loss 1.5344996452331543\n","epoch 41/1000, step 3/22, loss 1.5115152597427368\n","epoch 41/1000, step 4/22, loss 1.5295443534851074\n","epoch 41/1000, step 5/22, loss 1.5341310501098633\n","epoch 41/1000, step 6/22, loss 1.5204581022262573\n","epoch 41/1000, step 7/22, loss 1.5181115865707397\n","epoch 41/1000, step 8/22, loss 1.539260983467102\n","epoch 41/1000, step 9/22, loss 1.5247998237609863\n","epoch 41/1000, step 10/22, loss 1.5420087575912476\n","epoch 41/1000, step 11/22, loss 1.532151222229004\n","epoch 41/1000, step 12/22, loss 1.528292179107666\n","epoch 41/1000, step 13/22, loss 1.5399119853973389\n","epoch 41/1000, step 14/22, loss 1.5065248012542725\n","epoch 41/1000, step 15/22, loss 1.5255576372146606\n","epoch 41/1000, step 16/22, loss 1.516159176826477\n","epoch 41/1000, step 17/22, loss 1.5174522399902344\n","epoch 41/1000, step 18/22, loss 1.531888484954834\n","epoch 41/1000, step 19/22, loss 1.5143285989761353\n","epoch 41/1000, step 20/22, loss 1.5407037734985352\n","epoch 41/1000, step 21/22, loss 1.5205949544906616\n","epoch 41/1000, step 22/22, loss 1.5173556804656982\n","epoch 42/1000, step 1/22, loss 1.52748703956604\n","epoch 42/1000, step 2/22, loss 1.5291129350662231\n","epoch 42/1000, step 3/22, loss 1.505977988243103\n","epoch 42/1000, step 4/22, loss 1.5244872570037842\n","epoch 42/1000, step 5/22, loss 1.528699278831482\n","epoch 42/1000, step 6/22, loss 1.5151015520095825\n","epoch 42/1000, step 7/22, loss 1.5127604007720947\n","epoch 42/1000, step 8/22, loss 1.5342398881912231\n","epoch 42/1000, step 9/22, loss 1.5191924571990967\n","epoch 42/1000, step 10/22, loss 1.5368342399597168\n","epoch 42/1000, step 11/22, loss 1.527066707611084\n","epoch 42/1000, step 12/22, loss 1.5230233669281006\n","epoch 42/1000, step 13/22, loss 1.5346499681472778\n","epoch 42/1000, step 14/22, loss 1.506374716758728\n","epoch 42/1000, step 15/22, loss 1.5254417657852173\n","epoch 42/1000, step 16/22, loss 1.516038179397583\n","epoch 42/1000, step 17/22, loss 1.5173227787017822\n","epoch 42/1000, step 18/22, loss 1.5314527750015259\n","epoch 42/1000, step 19/22, loss 1.5138756036758423\n","epoch 42/1000, step 20/22, loss 1.5403388738632202\n","epoch 42/1000, step 21/22, loss 1.5201894044876099\n","epoch 42/1000, step 22/22, loss 1.5154343843460083\n","epoch 43/1000, step 1/22, loss 1.5271013975143433\n","epoch 43/1000, step 2/22, loss 1.5287433862686157\n","epoch 43/1000, step 3/22, loss 1.5055056810379028\n","epoch 43/1000, step 4/22, loss 1.5241223573684692\n","epoch 43/1000, step 5/22, loss 1.5283933877944946\n","epoch 43/1000, step 6/22, loss 1.5147584676742554\n","epoch 43/1000, step 7/22, loss 1.5123711824417114\n","epoch 43/1000, step 8/22, loss 1.533905029296875\n","epoch 43/1000, step 9/22, loss 1.518879771232605\n","epoch 43/1000, step 10/22, loss 1.5365194082260132\n","epoch 43/1000, step 11/22, loss 1.5266411304473877\n","epoch 43/1000, step 12/22, loss 1.5225969552993774\n","epoch 43/1000, step 13/22, loss 1.5342869758605957\n","epoch 43/1000, step 14/22, loss 1.5059328079223633\n","epoch 43/1000, step 15/22, loss 1.5250548124313354\n","epoch 43/1000, step 16/22, loss 1.5156254768371582\n","epoch 43/1000, step 17/22, loss 1.5168910026550293\n","epoch 43/1000, step 18/22, loss 1.5313671827316284\n","epoch 43/1000, step 19/22, loss 1.5137770175933838\n","epoch 43/1000, step 20/22, loss 1.5402240753173828\n","epoch 43/1000, step 21/22, loss 1.5200824737548828\n","epoch 43/1000, step 22/22, loss 1.5147210359573364\n","epoch 44/1000, step 1/22, loss 1.5269722938537598\n","epoch 44/1000, step 2/22, loss 1.5286420583724976\n","epoch 44/1000, step 3/22, loss 1.5053918361663818\n","epoch 44/1000, step 4/22, loss 1.5240094661712646\n","epoch 44/1000, step 5/22, loss 1.5283132791519165\n","epoch 44/1000, step 6/22, loss 1.513303279876709\n","epoch 44/1000, step 7/22, loss 1.5109626054763794\n","epoch 44/1000, step 8/22, loss 1.532486081123352\n","epoch 44/1000, step 9/22, loss 1.5175780057907104\n","epoch 44/1000, step 10/22, loss 1.5351653099060059\n","epoch 44/1000, step 11/22, loss 1.5253522396087646\n","epoch 44/1000, step 12/22, loss 1.5212997198104858\n","epoch 44/1000, step 13/22, loss 1.5330156087875366\n","epoch 44/1000, step 14/22, loss 1.5045204162597656\n","epoch 44/1000, step 15/22, loss 1.5237665176391602\n","epoch 44/1000, step 16/22, loss 1.5142114162445068\n","epoch 44/1000, step 17/22, loss 1.515537977218628\n","epoch 44/1000, step 18/22, loss 1.530130386352539\n","epoch 44/1000, step 19/22, loss 1.5124696493148804\n","epoch 44/1000, step 20/22, loss 1.5388914346694946\n","epoch 44/1000, step 21/22, loss 1.5187804698944092\n","epoch 44/1000, step 22/22, loss 1.5145208835601807\n","epoch 45/1000, step 1/22, loss 1.5256423950195312\n","epoch 45/1000, step 2/22, loss 1.527334451675415\n","epoch 45/1000, step 3/22, loss 1.5040709972381592\n","epoch 45/1000, step 4/22, loss 1.5224956274032593\n","epoch 45/1000, step 5/22, loss 1.526984691619873\n","epoch 45/1000, step 6/22, loss 1.513184666633606\n","epoch 45/1000, step 7/22, loss 1.510841965675354\n","epoch 45/1000, step 8/22, loss 1.5323702096939087\n","epoch 45/1000, step 9/22, loss 1.5174691677093506\n","epoch 45/1000, step 10/22, loss 1.5350587368011475\n","epoch 45/1000, step 11/22, loss 1.5252394676208496\n","epoch 45/1000, step 12/22, loss 1.5211793184280396\n","epoch 45/1000, step 13/22, loss 1.5329190492630005\n","epoch 45/1000, step 14/22, loss 1.50440514087677\n","epoch 45/1000, step 15/22, loss 1.5236135721206665\n","epoch 45/1000, step 16/22, loss 1.514090895652771\n","epoch 45/1000, step 17/22, loss 1.5153958797454834\n","epoch 45/1000, step 18/22, loss 1.5300108194351196\n","epoch 45/1000, step 19/22, loss 1.512352466583252\n","epoch 45/1000, step 20/22, loss 1.5387698411941528\n","epoch 45/1000, step 21/22, loss 1.5186617374420166\n","epoch 45/1000, step 22/22, loss 1.513961911201477\n","epoch 46/1000, step 1/22, loss 1.5255182981491089\n","epoch 46/1000, step 2/22, loss 1.5272260904312134\n","epoch 46/1000, step 3/22, loss 1.503928542137146\n","epoch 46/1000, step 4/22, loss 1.5224496126174927\n","epoch 46/1000, step 5/22, loss 1.5269471406936646\n","epoch 46/1000, step 6/22, loss 1.512819766998291\n","epoch 46/1000, step 7/22, loss 1.5104328393936157\n","epoch 46/1000, step 8/22, loss 1.5320132970809937\n","epoch 46/1000, step 9/22, loss 1.5171222686767578\n","epoch 46/1000, step 10/22, loss 1.5347223281860352\n","epoch 46/1000, step 11/22, loss 1.524852991104126\n","epoch 46/1000, step 12/22, loss 1.5208035707473755\n","epoch 46/1000, step 13/22, loss 1.5325881242752075\n","epoch 46/1000, step 14/22, loss 1.504012107849121\n","epoch 46/1000, step 15/22, loss 1.5233019590377808\n","epoch 46/1000, step 16/22, loss 1.513756275177002\n","epoch 46/1000, step 17/22, loss 1.515059232711792\n","epoch 46/1000, step 18/22, loss 1.5296589136123657\n","epoch 46/1000, step 19/22, loss 1.5119553804397583\n","epoch 46/1000, step 20/22, loss 1.5384535789489746\n","epoch 46/1000, step 21/22, loss 1.518302321434021\n","epoch 46/1000, step 22/22, loss 1.5126113891601562\n","epoch 47/1000, step 1/22, loss 1.5251667499542236\n","epoch 47/1000, step 2/22, loss 1.5268781185150146\n","epoch 47/1000, step 3/22, loss 1.5035299062728882\n","epoch 47/1000, step 4/22, loss 1.5220640897750854\n","epoch 47/1000, step 5/22, loss 1.5266202688217163\n","epoch 47/1000, step 6/22, loss 1.512778639793396\n","epoch 47/1000, step 7/22, loss 1.5103871822357178\n","epoch 47/1000, step 8/22, loss 1.5319745540618896\n","epoch 47/1000, step 9/22, loss 1.5170857906341553\n","epoch 47/1000, step 10/22, loss 1.5346872806549072\n","epoch 47/1000, step 11/22, loss 1.5248149633407593\n","epoch 47/1000, step 12/22, loss 1.52068293094635\n","epoch 47/1000, step 13/22, loss 1.5324862003326416\n","epoch 47/1000, step 14/22, loss 1.5038907527923584\n","epoch 47/1000, step 15/22, loss 1.5231815576553345\n","epoch 47/1000, step 16/22, loss 1.513643503189087\n","epoch 47/1000, step 17/22, loss 1.5149396657943726\n","epoch 47/1000, step 18/22, loss 1.5295534133911133\n","epoch 47/1000, step 19/22, loss 1.5118772983551025\n","epoch 47/1000, step 20/22, loss 1.5383726358413696\n","epoch 47/1000, step 21/22, loss 1.5182133913040161\n","epoch 47/1000, step 22/22, loss 1.5122029781341553\n","epoch 48/1000, step 1/22, loss 1.525076150894165\n","epoch 48/1000, step 2/22, loss 1.5268033742904663\n","epoch 48/1000, step 3/22, loss 1.5034153461456299\n","epoch 48/1000, step 4/22, loss 1.5219734907150269\n","epoch 48/1000, step 5/22, loss 1.5265454053878784\n","epoch 48/1000, step 6/22, loss 1.5126655101776123\n","epoch 48/1000, step 7/22, loss 1.510272741317749\n","epoch 48/1000, step 8/22, loss 1.5318653583526611\n","epoch 48/1000, step 9/22, loss 1.5169827938079834\n","epoch 48/1000, step 10/22, loss 1.5345975160598755\n","epoch 48/1000, step 11/22, loss 1.5247063636779785\n","epoch 48/1000, step 12/22, loss 1.5206637382507324\n","epoch 48/1000, step 13/22, loss 1.5324677228927612\n","epoch 48/1000, step 14/22, loss 1.5038645267486572\n","epoch 48/1000, step 15/22, loss 1.523154616355896\n","epoch 48/1000, step 16/22, loss 1.5136165618896484\n","epoch 48/1000, step 17/22, loss 1.51491379737854\n","epoch 48/1000, step 18/22, loss 1.5295213460922241\n","epoch 48/1000, step 19/22, loss 1.5118502378463745\n","epoch 48/1000, step 20/22, loss 1.5383549928665161\n","epoch 48/1000, step 21/22, loss 1.5181885957717896\n","epoch 48/1000, step 22/22, loss 1.5121169090270996\n","epoch 49/1000, step 1/22, loss 1.5250526666641235\n","epoch 49/1000, step 2/22, loss 1.5267834663391113\n","epoch 49/1000, step 3/22, loss 1.5033965110778809\n","epoch 49/1000, step 4/22, loss 1.5219489336013794\n","epoch 49/1000, step 5/22, loss 1.5265244245529175\n","epoch 49/1000, step 6/22, loss 1.512662410736084\n","epoch 49/1000, step 7/22, loss 1.5102657079696655\n","epoch 49/1000, step 8/22, loss 1.531864881515503\n","epoch 49/1000, step 9/22, loss 1.516981601715088\n","epoch 49/1000, step 10/22, loss 1.5345983505249023\n","epoch 49/1000, step 11/22, loss 1.5247057676315308\n","epoch 49/1000, step 12/22, loss 1.5206557512283325\n","epoch 49/1000, step 13/22, loss 1.532466173171997\n","epoch 49/1000, step 14/22, loss 1.503859043121338\n","epoch 49/1000, step 15/22, loss 1.5231560468673706\n","epoch 49/1000, step 16/22, loss 1.5136113166809082\n","epoch 49/1000, step 17/22, loss 1.514909267425537\n","epoch 49/1000, step 18/22, loss 1.5295169353485107\n","epoch 49/1000, step 19/22, loss 1.5118446350097656\n","epoch 49/1000, step 20/22, loss 1.5383538007736206\n","epoch 49/1000, step 21/22, loss 1.5181808471679688\n","epoch 49/1000, step 22/22, loss 1.5120179653167725\n","epoch 50/1000, step 1/22, loss 1.5250235795974731\n","epoch 50/1000, step 2/22, loss 1.5267583131790161\n","epoch 50/1000, step 3/22, loss 1.503371000289917\n","epoch 50/1000, step 4/22, loss 1.52191960811615\n","epoch 50/1000, step 5/22, loss 1.5264955759048462\n","epoch 50/1000, step 6/22, loss 1.5126354694366455\n","epoch 50/1000, step 7/22, loss 1.5102335214614868\n","epoch 50/1000, step 8/22, loss 1.5318392515182495\n","epoch 50/1000, step 9/22, loss 1.5169556140899658\n","epoch 50/1000, step 10/22, loss 1.5345736742019653\n","epoch 50/1000, step 11/22, loss 1.5246732234954834\n","epoch 50/1000, step 12/22, loss 1.5206286907196045\n","epoch 50/1000, step 13/22, loss 1.532441258430481\n","epoch 50/1000, step 14/22, loss 1.5038267374038696\n","epoch 50/1000, step 15/22, loss 1.5231252908706665\n","epoch 50/1000, step 16/22, loss 1.5135807991027832\n","epoch 50/1000, step 17/22, loss 1.5148786306381226\n","epoch 50/1000, step 18/22, loss 1.5294829607009888\n","epoch 50/1000, step 19/22, loss 1.5118143558502197\n","epoch 50/1000, step 20/22, loss 1.5383256673812866\n","epoch 50/1000, step 21/22, loss 1.5181481838226318\n","epoch 50/1000, step 22/22, loss 1.5120021104812622\n","epoch 51/1000, step 1/22, loss 1.524930715560913\n","epoch 51/1000, step 2/22, loss 1.5266790390014648\n","epoch 51/1000, step 3/22, loss 1.5032545328140259\n","epoch 51/1000, step 4/22, loss 1.5218230485916138\n","epoch 51/1000, step 5/22, loss 1.5264101028442383\n","epoch 51/1000, step 6/22, loss 1.512534499168396\n","epoch 51/1000, step 7/22, loss 1.5101345777511597\n","epoch 51/1000, step 8/22, loss 1.5317424535751343\n","epoch 51/1000, step 9/22, loss 1.5168589353561401\n","epoch 51/1000, step 10/22, loss 1.5344890356063843\n","epoch 51/1000, step 11/22, loss 1.5245805978775024\n","epoch 51/1000, step 12/22, loss 1.5205299854278564\n","epoch 51/1000, step 13/22, loss 1.5323644876480103\n","epoch 51/1000, step 14/22, loss 1.5037280321121216\n","epoch 51/1000, step 15/22, loss 1.523024082183838\n","epoch 51/1000, step 16/22, loss 1.5134899616241455\n","epoch 51/1000, step 17/22, loss 1.514768123626709\n","epoch 51/1000, step 18/22, loss 1.5293906927108765\n","epoch 51/1000, step 19/22, loss 1.5117255449295044\n","epoch 51/1000, step 20/22, loss 1.5382341146469116\n","epoch 51/1000, step 21/22, loss 1.5180588960647583\n","epoch 51/1000, step 22/22, loss 1.511569857597351\n","epoch 52/1000, step 1/22, loss 1.524917483329773\n","epoch 52/1000, step 2/22, loss 1.5266674757003784\n","epoch 52/1000, step 3/22, loss 1.5032432079315186\n","epoch 52/1000, step 4/22, loss 1.5218113660812378\n","epoch 52/1000, step 5/22, loss 1.5263757705688477\n","epoch 52/1000, step 6/22, loss 1.5125032663345337\n","epoch 52/1000, step 7/22, loss 1.510097622871399\n","epoch 52/1000, step 8/22, loss 1.531712293624878\n","epoch 52/1000, step 9/22, loss 1.516827940940857\n","epoch 52/1000, step 10/22, loss 1.5344603061676025\n","epoch 52/1000, step 11/22, loss 1.5245368480682373\n","epoch 52/1000, step 12/22, loss 1.5204874277114868\n","epoch 52/1000, step 13/22, loss 1.5323227643966675\n","epoch 52/1000, step 14/22, loss 1.503686547279358\n","epoch 52/1000, step 15/22, loss 1.5229828357696533\n","epoch 52/1000, step 16/22, loss 1.5134491920471191\n","epoch 52/1000, step 17/22, loss 1.5147265195846558\n","epoch 52/1000, step 18/22, loss 1.5293474197387695\n","epoch 52/1000, step 19/22, loss 1.5116838216781616\n","epoch 52/1000, step 20/22, loss 1.5381914377212524\n","epoch 52/1000, step 21/22, loss 1.5180132389068604\n","epoch 52/1000, step 22/22, loss 1.5114457607269287\n","epoch 53/1000, step 1/22, loss 1.524870753288269\n","epoch 53/1000, step 2/22, loss 1.5266247987747192\n","epoch 53/1000, step 3/22, loss 1.5032017230987549\n","epoch 53/1000, step 4/22, loss 1.5217660665512085\n","epoch 53/1000, step 5/22, loss 1.5263527631759644\n","epoch 53/1000, step 6/22, loss 1.5124759674072266\n","epoch 53/1000, step 7/22, loss 1.5100677013397217\n","epoch 53/1000, step 8/22, loss 1.5316871404647827\n","epoch 53/1000, step 9/22, loss 1.516801357269287\n","epoch 53/1000, step 10/22, loss 1.5344336032867432\n","epoch 53/1000, step 11/22, loss 1.5245202779769897\n","epoch 53/1000, step 12/22, loss 1.5204683542251587\n","epoch 53/1000, step 13/22, loss 1.5323091745376587\n","epoch 53/1000, step 14/22, loss 1.5033187866210938\n","epoch 53/1000, step 15/22, loss 1.522660732269287\n","epoch 53/1000, step 16/22, loss 1.5131090879440308\n","epoch 53/1000, step 17/22, loss 1.5143952369689941\n","epoch 53/1000, step 18/22, loss 1.5289957523345947\n","epoch 53/1000, step 19/22, loss 1.5113049745559692\n","epoch 53/1000, step 20/22, loss 1.5378994941711426\n","epoch 53/1000, step 21/22, loss 1.517679214477539\n","epoch 53/1000, step 22/22, loss 1.510184645652771\n","epoch 54/1000, step 1/22, loss 1.5245436429977417\n","epoch 54/1000, step 2/22, loss 1.5262948274612427\n","epoch 54/1000, step 3/22, loss 1.5028212070465088\n","epoch 54/1000, step 4/22, loss 1.5214097499847412\n","epoch 54/1000, step 5/22, loss 1.5260438919067383\n","epoch 54/1000, step 6/22, loss 1.5121418237686157\n","epoch 54/1000, step 7/22, loss 1.5096948146820068\n","epoch 54/1000, step 8/22, loss 1.531362533569336\n","epoch 54/1000, step 9/22, loss 1.5164753198623657\n","epoch 54/1000, step 10/22, loss 1.5341160297393799\n","epoch 54/1000, step 11/22, loss 1.5241552591323853\n","epoch 54/1000, step 12/22, loss 1.5201133489608765\n","epoch 54/1000, step 13/22, loss 1.5319963693618774\n","epoch 54/1000, step 14/22, loss 1.503299355506897\n","epoch 54/1000, step 15/22, loss 1.5226426124572754\n","epoch 54/1000, step 16/22, loss 1.5130897760391235\n","epoch 54/1000, step 17/22, loss 1.514374852180481\n","epoch 54/1000, step 18/22, loss 1.5289736986160278\n","epoch 54/1000, step 19/22, loss 1.5112861394882202\n","epoch 54/1000, step 20/22, loss 1.5378773212432861\n","epoch 54/1000, step 21/22, loss 1.5176564455032349\n","epoch 54/1000, step 22/22, loss 1.510156512260437\n","epoch 55/1000, step 1/22, loss 1.5245226621627808\n","epoch 55/1000, step 2/22, loss 1.5262764692306519\n","epoch 55/1000, step 3/22, loss 1.5028020143508911\n","epoch 55/1000, step 4/22, loss 1.5213900804519653\n","epoch 55/1000, step 5/22, loss 1.5260051488876343\n","epoch 55/1000, step 6/22, loss 1.5121010541915894\n","epoch 55/1000, step 7/22, loss 1.5096495151519775\n","epoch 55/1000, step 8/22, loss 1.5313215255737305\n","epoch 55/1000, step 9/22, loss 1.5164399147033691\n","epoch 55/1000, step 10/22, loss 1.534084439277649\n","epoch 55/1000, step 11/22, loss 1.5241210460662842\n","epoch 55/1000, step 12/22, loss 1.5200812816619873\n","epoch 55/1000, step 13/22, loss 1.5319647789001465\n","epoch 55/1000, step 14/22, loss 1.5032638311386108\n","epoch 55/1000, step 15/22, loss 1.522603988647461\n","epoch 55/1000, step 16/22, loss 1.5130547285079956\n","epoch 55/1000, step 17/22, loss 1.5143382549285889\n","epoch 55/1000, step 18/22, loss 1.5289366245269775\n","epoch 55/1000, step 19/22, loss 1.5112510919570923\n","epoch 55/1000, step 20/22, loss 1.5378444194793701\n","epoch 55/1000, step 21/22, loss 1.5176221132278442\n","epoch 55/1000, step 22/22, loss 1.5100551843643188\n","epoch 56/1000, step 1/22, loss 1.5244879722595215\n","epoch 56/1000, step 2/22, loss 1.5262436866760254\n","epoch 56/1000, step 3/22, loss 1.502766489982605\n","epoch 56/1000, step 4/22, loss 1.5213499069213867\n","epoch 56/1000, step 5/22, loss 1.5259859561920166\n","epoch 56/1000, step 6/22, loss 1.5120805501937866\n","epoch 56/1000, step 7/22, loss 1.509628176689148\n","epoch 56/1000, step 8/22, loss 1.5313024520874023\n","epoch 56/1000, step 9/22, loss 1.5164217948913574\n","epoch 56/1000, step 10/22, loss 1.5340656042099\n","epoch 56/1000, step 11/22, loss 1.5241022109985352\n","epoch 56/1000, step 12/22, loss 1.5200587511062622\n","epoch 56/1000, step 13/22, loss 1.5319466590881348\n","epoch 56/1000, step 14/22, loss 1.5032446384429932\n","epoch 56/1000, step 15/22, loss 1.5225878953933716\n","epoch 56/1000, step 16/22, loss 1.5130369663238525\n","epoch 56/1000, step 17/22, loss 1.5143194198608398\n","epoch 56/1000, step 18/22, loss 1.5289137363433838\n","epoch 56/1000, step 19/22, loss 1.5112322568893433\n","epoch 56/1000, step 20/22, loss 1.5378233194351196\n","epoch 56/1000, step 21/22, loss 1.5175976753234863\n","epoch 56/1000, step 22/22, loss 1.510032296180725\n","epoch 57/1000, step 1/22, loss 1.5244675874710083\n","epoch 57/1000, step 2/22, loss 1.526222825050354\n","epoch 57/1000, step 3/22, loss 1.5026432275772095\n","epoch 57/1000, step 4/22, loss 1.52125084400177\n","epoch 57/1000, step 5/22, loss 1.5258917808532715\n","epoch 57/1000, step 6/22, loss 1.5119763612747192\n","epoch 57/1000, step 7/22, loss 1.5095264911651611\n","epoch 57/1000, step 8/22, loss 1.5311987400054932\n","epoch 57/1000, step 9/22, loss 1.5163198709487915\n","epoch 57/1000, step 10/22, loss 1.5339763164520264\n","epoch 57/1000, step 11/22, loss 1.5240004062652588\n","epoch 57/1000, step 12/22, loss 1.5199495553970337\n","epoch 57/1000, step 13/22, loss 1.5318565368652344\n","epoch 57/1000, step 14/22, loss 1.5031450986862183\n","epoch 57/1000, step 15/22, loss 1.5224806070327759\n","epoch 57/1000, step 16/22, loss 1.5129435062408447\n","epoch 57/1000, step 17/22, loss 1.5142008066177368\n","epoch 57/1000, step 18/22, loss 1.5288169384002686\n","epoch 57/1000, step 19/22, loss 1.511137843132019\n","epoch 57/1000, step 20/22, loss 1.5377277135849\n","epoch 57/1000, step 21/22, loss 1.517507791519165\n","epoch 57/1000, step 22/22, loss 1.509623646736145\n","epoch 58/1000, step 1/22, loss 1.52437424659729\n","epoch 58/1000, step 2/22, loss 1.5261374711990356\n","epoch 58/1000, step 3/22, loss 1.5026283264160156\n","epoch 58/1000, step 4/22, loss 1.521237850189209\n","epoch 58/1000, step 5/22, loss 1.5258769989013672\n","epoch 58/1000, step 6/22, loss 1.5119552612304688\n","epoch 58/1000, step 7/22, loss 1.509505271911621\n","epoch 58/1000, step 8/22, loss 1.531179428100586\n","epoch 58/1000, step 9/22, loss 1.5162990093231201\n","epoch 58/1000, step 10/22, loss 1.5339555740356445\n","epoch 58/1000, step 11/22, loss 1.5239797830581665\n","epoch 58/1000, step 12/22, loss 1.5199261903762817\n","epoch 58/1000, step 13/22, loss 1.5318366289138794\n","epoch 58/1000, step 14/22, loss 1.503126859664917\n","epoch 58/1000, step 15/22, loss 1.5224639177322388\n","epoch 58/1000, step 16/22, loss 1.5129270553588867\n","epoch 58/1000, step 17/22, loss 1.514181137084961\n","epoch 58/1000, step 18/22, loss 1.5287935733795166\n","epoch 58/1000, step 19/22, loss 1.5111192464828491\n","epoch 58/1000, step 20/22, loss 1.537706971168518\n","epoch 58/1000, step 21/22, loss 1.5174853801727295\n","epoch 58/1000, step 22/22, loss 1.5095984935760498\n","epoch 59/1000, step 1/22, loss 1.5243533849716187\n","epoch 59/1000, step 2/22, loss 1.5261189937591553\n","epoch 59/1000, step 3/22, loss 1.502609133720398\n","epoch 59/1000, step 4/22, loss 1.5212194919586182\n","epoch 59/1000, step 5/22, loss 1.525857925415039\n","epoch 59/1000, step 6/22, loss 1.5119414329528809\n","epoch 59/1000, step 7/22, loss 1.5094910860061646\n","epoch 59/1000, step 8/22, loss 1.531166434288025\n","epoch 59/1000, step 9/22, loss 1.516283631324768\n","epoch 59/1000, step 10/22, loss 1.5339409112930298\n","epoch 59/1000, step 11/22, loss 1.5239439010620117\n","epoch 59/1000, step 12/22, loss 1.519896388053894\n","epoch 59/1000, step 13/22, loss 1.5317989587783813\n","epoch 59/1000, step 14/22, loss 1.5030895471572876\n","epoch 59/1000, step 15/22, loss 1.5224244594573975\n","epoch 59/1000, step 16/22, loss 1.5128909349441528\n","epoch 59/1000, step 17/22, loss 1.514144778251648\n","epoch 59/1000, step 18/22, loss 1.5287528038024902\n","epoch 59/1000, step 19/22, loss 1.5110836029052734\n","epoch 59/1000, step 20/22, loss 1.5376731157302856\n","epoch 59/1000, step 21/22, loss 1.5174492597579956\n","epoch 59/1000, step 22/22, loss 1.5094987154006958\n","epoch 60/1000, step 1/22, loss 1.524316430091858\n","epoch 60/1000, step 2/22, loss 1.526084542274475\n","epoch 60/1000, step 3/22, loss 1.502575159072876\n","epoch 60/1000, step 4/22, loss 1.5211825370788574\n","epoch 60/1000, step 5/22, loss 1.525821566581726\n","epoch 60/1000, step 6/22, loss 1.511906385421753\n","epoch 60/1000, step 7/22, loss 1.5094547271728516\n","epoch 60/1000, step 8/22, loss 1.531130313873291\n","epoch 60/1000, step 9/22, loss 1.5162477493286133\n","epoch 60/1000, step 10/22, loss 1.533908724784851\n","epoch 60/1000, step 11/22, loss 1.5239285230636597\n","epoch 60/1000, step 12/22, loss 1.5198808908462524\n","epoch 60/1000, step 13/22, loss 1.5317858457565308\n","epoch 60/1000, step 14/22, loss 1.5030763149261475\n","epoch 60/1000, step 15/22, loss 1.5224124193191528\n","epoch 60/1000, step 16/22, loss 1.5128791332244873\n","epoch 60/1000, step 17/22, loss 1.5141301155090332\n","epoch 60/1000, step 18/22, loss 1.5287396907806396\n","epoch 60/1000, step 19/22, loss 1.5110676288604736\n","epoch 60/1000, step 20/22, loss 1.5376548767089844\n","epoch 60/1000, step 21/22, loss 1.517429232597351\n","epoch 60/1000, step 22/22, loss 1.5094735622406006\n","epoch 61/1000, step 1/22, loss 1.524299144744873\n","epoch 61/1000, step 2/22, loss 1.5260677337646484\n","epoch 61/1000, step 3/22, loss 1.5025604963302612\n","epoch 61/1000, step 4/22, loss 1.5211659669876099\n","epoch 61/1000, step 5/22, loss 1.5258053541183472\n","epoch 61/1000, step 6/22, loss 1.5118882656097412\n","epoch 61/1000, step 7/22, loss 1.5094362497329712\n","epoch 61/1000, step 8/22, loss 1.531115174293518\n","epoch 61/1000, step 9/22, loss 1.5162301063537598\n","epoch 61/1000, step 10/22, loss 1.5338926315307617\n","epoch 61/1000, step 11/22, loss 1.5239126682281494\n","epoch 61/1000, step 12/22, loss 1.5198636054992676\n","epoch 61/1000, step 13/22, loss 1.5317680835723877\n","epoch 61/1000, step 14/22, loss 1.5030603408813477\n","epoch 61/1000, step 15/22, loss 1.5223976373672485\n","epoch 61/1000, step 16/22, loss 1.512863039970398\n","epoch 61/1000, step 17/22, loss 1.5141122341156006\n","epoch 61/1000, step 18/22, loss 1.5287220478057861\n","epoch 61/1000, step 19/22, loss 1.5110536813735962\n","epoch 61/1000, step 20/22, loss 1.537643313407898\n","epoch 61/1000, step 21/22, loss 1.5174157619476318\n","epoch 61/1000, step 22/22, loss 1.5094630718231201\n","epoch 62/1000, step 1/22, loss 1.5242875814437866\n","epoch 62/1000, step 2/22, loss 1.5260566473007202\n","epoch 62/1000, step 3/22, loss 1.5025465488433838\n","epoch 62/1000, step 4/22, loss 1.5211554765701294\n","epoch 62/1000, step 5/22, loss 1.5257915258407593\n","epoch 62/1000, step 6/22, loss 1.5118755102157593\n","epoch 62/1000, step 7/22, loss 1.5094231367111206\n","epoch 62/1000, step 8/22, loss 1.5311042070388794\n","epoch 62/1000, step 9/22, loss 1.5149908065795898\n","epoch 62/1000, step 10/22, loss 1.5325987339019775\n","epoch 62/1000, step 11/22, loss 1.5226150751113892\n","epoch 62/1000, step 12/22, loss 1.5185573101043701\n","epoch 62/1000, step 13/22, loss 1.530508279800415\n","epoch 62/1000, step 14/22, loss 1.5016558170318604\n","epoch 62/1000, step 15/22, loss 1.5210884809494019\n","epoch 62/1000, step 16/22, loss 1.511488676071167\n","epoch 62/1000, step 17/22, loss 1.5127872228622437\n","epoch 62/1000, step 18/22, loss 1.5275102853775024\n","epoch 62/1000, step 19/22, loss 1.5097566843032837\n","epoch 62/1000, step 20/22, loss 1.5363280773162842\n","epoch 62/1000, step 21/22, loss 1.5161287784576416\n","epoch 62/1000, step 22/22, loss 1.509459376335144\n","epoch 63/1000, step 1/22, loss 1.522971510887146\n","epoch 63/1000, step 2/22, loss 1.5247530937194824\n","epoch 63/1000, step 3/22, loss 1.5012210607528687\n","epoch 63/1000, step 4/22, loss 1.5197452306747437\n","epoch 63/1000, step 5/22, loss 1.524549961090088\n","epoch 63/1000, step 6/22, loss 1.5105096101760864\n","epoch 63/1000, step 7/22, loss 1.5080732107162476\n","epoch 63/1000, step 8/22, loss 1.5297648906707764\n","epoch 63/1000, step 9/22, loss 1.5149710178375244\n","epoch 63/1000, step 10/22, loss 1.5325807332992554\n","epoch 63/1000, step 11/22, loss 1.522597074508667\n","epoch 63/1000, step 12/22, loss 1.5185400247573853\n","epoch 63/1000, step 13/22, loss 1.530491590499878\n","epoch 63/1000, step 14/22, loss 1.5016390085220337\n","epoch 63/1000, step 15/22, loss 1.521072268486023\n","epoch 63/1000, step 16/22, loss 1.5114719867706299\n","epoch 63/1000, step 17/22, loss 1.5127698183059692\n","epoch 63/1000, step 18/22, loss 1.5274949073791504\n","epoch 63/1000, step 19/22, loss 1.509741187095642\n","epoch 63/1000, step 20/22, loss 1.5363095998764038\n","epoch 63/1000, step 21/22, loss 1.516121506690979\n","epoch 63/1000, step 22/22, loss 1.5094513893127441\n","epoch 64/1000, step 1/22, loss 1.5229642391204834\n","epoch 64/1000, step 2/22, loss 1.524747371673584\n","epoch 64/1000, step 3/22, loss 1.501212477684021\n","epoch 64/1000, step 4/22, loss 1.5197346210479736\n","epoch 64/1000, step 5/22, loss 1.5245407819747925\n","epoch 64/1000, step 6/22, loss 1.5104976892471313\n","epoch 64/1000, step 7/22, loss 1.5080604553222656\n","epoch 64/1000, step 8/22, loss 1.5297554731369019\n","epoch 64/1000, step 9/22, loss 1.5149602890014648\n","epoch 64/1000, step 10/22, loss 1.5325709581375122\n","epoch 64/1000, step 11/22, loss 1.5225886106491089\n","epoch 64/1000, step 12/22, loss 1.5185294151306152\n","epoch 64/1000, step 13/22, loss 1.5304840803146362\n","epoch 64/1000, step 14/22, loss 1.5016299486160278\n","epoch 64/1000, step 15/22, loss 1.521065354347229\n","epoch 64/1000, step 16/22, loss 1.5114632844924927\n","epoch 64/1000, step 17/22, loss 1.5127601623535156\n","epoch 64/1000, step 18/22, loss 1.527488112449646\n","epoch 64/1000, step 19/22, loss 1.5097339153289795\n","epoch 64/1000, step 20/22, loss 1.5363037586212158\n","epoch 64/1000, step 21/22, loss 1.516114354133606\n","epoch 64/1000, step 22/22, loss 1.5094257593154907\n","epoch 65/1000, step 1/22, loss 1.5229591131210327\n","epoch 65/1000, step 2/22, loss 1.5247429609298706\n","epoch 65/1000, step 3/22, loss 1.5012123584747314\n","epoch 65/1000, step 4/22, loss 1.5197324752807617\n","epoch 65/1000, step 5/22, loss 1.5245411396026611\n","epoch 65/1000, step 6/22, loss 1.5104972124099731\n","epoch 65/1000, step 7/22, loss 1.5080581903457642\n","epoch 65/1000, step 8/22, loss 1.5297538042068481\n","epoch 65/1000, step 9/22, loss 1.5149574279785156\n","epoch 65/1000, step 10/22, loss 1.5325686931610107\n","epoch 65/1000, step 11/22, loss 1.5225887298583984\n","epoch 65/1000, step 12/22, loss 1.5185298919677734\n","epoch 65/1000, step 13/22, loss 1.5304847955703735\n","epoch 65/1000, step 14/22, loss 1.5016306638717651\n","epoch 65/1000, step 15/22, loss 1.52106773853302\n","epoch 65/1000, step 16/22, loss 1.5114655494689941\n","epoch 65/1000, step 17/22, loss 1.5127630233764648\n","epoch 65/1000, step 18/22, loss 1.5274900197982788\n","epoch 65/1000, step 19/22, loss 1.5097370147705078\n","epoch 65/1000, step 20/22, loss 1.5363060235977173\n","epoch 65/1000, step 21/22, loss 1.516117811203003\n","epoch 65/1000, step 22/22, loss 1.509413719177246\n","epoch 66/1000, step 1/22, loss 1.522961974143982\n","epoch 66/1000, step 2/22, loss 1.5247482061386108\n","epoch 66/1000, step 3/22, loss 1.5011972188949585\n","epoch 66/1000, step 4/22, loss 1.5197142362594604\n","epoch 66/1000, step 5/22, loss 1.5245258808135986\n","epoch 66/1000, step 6/22, loss 1.5104825496673584\n","epoch 66/1000, step 7/22, loss 1.508041262626648\n","epoch 66/1000, step 8/22, loss 1.529737949371338\n","epoch 66/1000, step 9/22, loss 1.5149441957473755\n","epoch 66/1000, step 10/22, loss 1.5325572490692139\n","epoch 66/1000, step 11/22, loss 1.5225731134414673\n","epoch 66/1000, step 12/22, loss 1.518521785736084\n","epoch 66/1000, step 13/22, loss 1.5304746627807617\n","epoch 66/1000, step 14/22, loss 1.501617431640625\n","epoch 66/1000, step 15/22, loss 1.521053671836853\n","epoch 66/1000, step 16/22, loss 1.5114527940750122\n","epoch 66/1000, step 17/22, loss 1.5127513408660889\n","epoch 66/1000, step 18/22, loss 1.5274734497070312\n","epoch 66/1000, step 19/22, loss 1.509727120399475\n","epoch 66/1000, step 20/22, loss 1.5362974405288696\n","epoch 66/1000, step 21/22, loss 1.5161091089248657\n","epoch 66/1000, step 22/22, loss 1.5093111991882324\n","epoch 67/1000, step 1/22, loss 1.5229474306106567\n","epoch 67/1000, step 2/22, loss 1.5247379541397095\n","epoch 67/1000, step 3/22, loss 1.5012060403823853\n","epoch 67/1000, step 4/22, loss 1.519723653793335\n","epoch 67/1000, step 5/22, loss 1.524538278579712\n","epoch 67/1000, step 6/22, loss 1.5104897022247314\n","epoch 67/1000, step 7/22, loss 1.508049488067627\n","epoch 67/1000, step 8/22, loss 1.5297447443008423\n","epoch 67/1000, step 9/22, loss 1.5149518251419067\n","epoch 67/1000, step 10/22, loss 1.5325645208358765\n","epoch 67/1000, step 11/22, loss 1.5225807428359985\n","epoch 67/1000, step 12/22, loss 1.5185363292694092\n","epoch 67/1000, step 13/22, loss 1.5304855108261108\n","epoch 67/1000, step 14/22, loss 1.5016286373138428\n","epoch 67/1000, step 15/22, loss 1.52106511592865\n","epoch 67/1000, step 16/22, loss 1.5114644765853882\n","epoch 67/1000, step 17/22, loss 1.5127596855163574\n","epoch 67/1000, step 18/22, loss 1.5274888277053833\n","epoch 67/1000, step 19/22, loss 1.5097399950027466\n","epoch 67/1000, step 20/22, loss 1.5363110303878784\n","epoch 67/1000, step 21/22, loss 1.5161223411560059\n","epoch 67/1000, step 22/22, loss 1.509302020072937\n","epoch 68/1000, step 1/22, loss 1.522965431213379\n","epoch 68/1000, step 2/22, loss 1.5247530937194824\n","epoch 68/1000, step 3/22, loss 1.5012238025665283\n","epoch 68/1000, step 4/22, loss 1.5197408199310303\n","epoch 68/1000, step 5/22, loss 1.5245603322982788\n","epoch 68/1000, step 6/22, loss 1.5105056762695312\n","epoch 68/1000, step 7/22, loss 1.5080674886703491\n","epoch 68/1000, step 8/22, loss 1.5297629833221436\n","epoch 68/1000, step 9/22, loss 1.514968991279602\n","epoch 68/1000, step 10/22, loss 1.5325849056243896\n","epoch 68/1000, step 11/22, loss 1.5225945711135864\n","epoch 68/1000, step 12/22, loss 1.5185539722442627\n","epoch 68/1000, step 13/22, loss 1.5305031538009644\n","epoch 68/1000, step 14/22, loss 1.5016433000564575\n","epoch 68/1000, step 15/22, loss 1.5210820436477661\n","epoch 68/1000, step 16/22, loss 1.511481761932373\n","epoch 68/1000, step 17/22, loss 1.5127767324447632\n","epoch 68/1000, step 18/22, loss 1.5275053977966309\n","epoch 68/1000, step 19/22, loss 1.509763479232788\n","epoch 68/1000, step 20/22, loss 1.536332368850708\n","epoch 68/1000, step 21/22, loss 1.5161410570144653\n","epoch 68/1000, step 22/22, loss 1.50927734375\n","epoch 69/1000, step 1/22, loss 1.5229860544204712\n","epoch 69/1000, step 2/22, loss 1.524770975112915\n","epoch 69/1000, step 3/22, loss 1.5012445449829102\n","epoch 69/1000, step 4/22, loss 1.5197621583938599\n","epoch 69/1000, step 5/22, loss 1.5245816707611084\n","epoch 69/1000, step 6/22, loss 1.5105226039886475\n","epoch 69/1000, step 7/22, loss 1.5080856084823608\n","epoch 69/1000, step 8/22, loss 1.5297811031341553\n","epoch 69/1000, step 9/22, loss 1.5149904489517212\n","epoch 69/1000, step 10/22, loss 1.5326062440872192\n","epoch 69/1000, step 11/22, loss 1.522621512413025\n","epoch 69/1000, step 12/22, loss 1.5185848474502563\n","epoch 69/1000, step 13/22, loss 1.5305352210998535\n","epoch 69/1000, step 14/22, loss 1.5016716718673706\n","epoch 69/1000, step 15/22, loss 1.5211102962493896\n","epoch 69/1000, step 16/22, loss 1.5115121603012085\n","epoch 69/1000, step 17/22, loss 1.5128049850463867\n","epoch 69/1000, step 18/22, loss 1.5275344848632812\n","epoch 69/1000, step 19/22, loss 1.5097953081130981\n","epoch 69/1000, step 20/22, loss 1.5363661050796509\n","epoch 69/1000, step 21/22, loss 1.5161744356155396\n","epoch 69/1000, step 22/22, loss 1.5092655420303345\n","epoch 70/1000, step 1/22, loss 1.523018479347229\n","epoch 70/1000, step 2/22, loss 1.5248037576675415\n","epoch 70/1000, step 3/22, loss 1.501275658607483\n","epoch 70/1000, step 4/22, loss 1.51979660987854\n","epoch 70/1000, step 5/22, loss 1.5246163606643677\n","epoch 70/1000, step 6/22, loss 1.5105550289154053\n","epoch 70/1000, step 7/22, loss 1.5081168413162231\n","epoch 70/1000, step 8/22, loss 1.529814600944519\n","epoch 70/1000, step 9/22, loss 1.5150209665298462\n","epoch 70/1000, step 10/22, loss 1.532644271850586\n","epoch 70/1000, step 11/22, loss 1.522658348083496\n","epoch 70/1000, step 12/22, loss 1.5186265707015991\n","epoch 70/1000, step 13/22, loss 1.530575156211853\n","epoch 70/1000, step 14/22, loss 1.5017094612121582\n","epoch 70/1000, step 15/22, loss 1.5211433172225952\n","epoch 70/1000, step 16/22, loss 1.5115468502044678\n","epoch 70/1000, step 17/22, loss 1.512844204902649\n","epoch 70/1000, step 18/22, loss 1.5275743007659912\n","epoch 70/1000, step 19/22, loss 1.509835958480835\n","epoch 70/1000, step 20/22, loss 1.5364100933074951\n","epoch 70/1000, step 21/22, loss 1.516219139099121\n","epoch 70/1000, step 22/22, loss 1.5092562437057495\n","epoch 71/1000, step 1/22, loss 1.5230671167373657\n","epoch 71/1000, step 2/22, loss 1.5248469114303589\n","epoch 71/1000, step 3/22, loss 1.5013130903244019\n","epoch 71/1000, step 4/22, loss 1.519842505455017\n","epoch 71/1000, step 5/22, loss 1.5245839357376099\n","epoch 71/1000, step 6/22, loss 1.5105115175247192\n","epoch 71/1000, step 7/22, loss 1.5080758333206177\n","epoch 71/1000, step 8/22, loss 1.5297675132751465\n","epoch 71/1000, step 9/22, loss 1.5149773359298706\n","epoch 71/1000, step 10/22, loss 1.5326218605041504\n","epoch 71/1000, step 11/22, loss 1.5226210355758667\n","epoch 71/1000, step 12/22, loss 1.5185883045196533\n","epoch 71/1000, step 13/22, loss 1.5305465459823608\n","epoch 71/1000, step 14/22, loss 1.5016728639602661\n","epoch 71/1000, step 15/22, loss 1.5210986137390137\n","epoch 71/1000, step 16/22, loss 1.511515736579895\n","epoch 71/1000, step 17/22, loss 1.5127899646759033\n","epoch 71/1000, step 18/22, loss 1.5275448560714722\n","epoch 71/1000, step 19/22, loss 1.5098092555999756\n","epoch 71/1000, step 20/22, loss 1.5363852977752686\n","epoch 71/1000, step 21/22, loss 1.5161981582641602\n","epoch 71/1000, step 22/22, loss 1.5088437795639038\n","epoch 72/1000, step 1/22, loss 1.5230393409729004\n","epoch 72/1000, step 2/22, loss 1.5248255729675293\n","epoch 72/1000, step 3/22, loss 1.501255750656128\n","epoch 72/1000, step 4/22, loss 1.5198125839233398\n","epoch 72/1000, step 5/22, loss 1.5246371030807495\n","epoch 72/1000, step 6/22, loss 1.5105634927749634\n","epoch 72/1000, step 7/22, loss 1.5081253051757812\n","epoch 72/1000, step 8/22, loss 1.529823899269104\n","epoch 72/1000, step 9/22, loss 1.5150277614593506\n","epoch 72/1000, step 10/22, loss 1.5326787233352661\n","epoch 72/1000, step 11/22, loss 1.5226775407791138\n","epoch 72/1000, step 12/22, loss 1.5186488628387451\n","epoch 72/1000, step 13/22, loss 1.5306037664413452\n","epoch 72/1000, step 14/22, loss 1.5017309188842773\n","epoch 72/1000, step 15/22, loss 1.5211535692214966\n","epoch 72/1000, step 16/22, loss 1.51157546043396\n","epoch 72/1000, step 17/22, loss 1.5128451585769653\n","epoch 72/1000, step 18/22, loss 1.5276060104370117\n","epoch 72/1000, step 19/22, loss 1.5098681449890137\n","epoch 72/1000, step 20/22, loss 1.5364482402801514\n","epoch 72/1000, step 21/22, loss 1.5162546634674072\n","epoch 72/1000, step 22/22, loss 1.5088356733322144\n","epoch 73/1000, step 1/22, loss 1.5231032371520996\n","epoch 73/1000, step 2/22, loss 1.524886131286621\n","epoch 73/1000, step 3/22, loss 1.501312255859375\n","epoch 73/1000, step 4/22, loss 1.519873023033142\n","epoch 73/1000, step 5/22, loss 1.524703860282898\n","epoch 73/1000, step 6/22, loss 1.5106223821640015\n","epoch 73/1000, step 7/22, loss 1.5081881284713745\n","epoch 73/1000, step 8/22, loss 1.5298880338668823\n","epoch 73/1000, step 9/22, loss 1.5150872468948364\n","epoch 73/1000, step 10/22, loss 1.5327471494674683\n","epoch 73/1000, step 11/22, loss 1.5227428674697876\n","epoch 73/1000, step 12/22, loss 1.5187188386917114\n","epoch 73/1000, step 13/22, loss 1.5306710004806519\n","epoch 73/1000, step 14/22, loss 1.5017964839935303\n","epoch 73/1000, step 15/22, loss 1.5212165117263794\n","epoch 73/1000, step 16/22, loss 1.5116406679153442\n","epoch 73/1000, step 17/22, loss 1.5129133462905884\n","epoch 73/1000, step 18/22, loss 1.527672290802002\n","epoch 73/1000, step 19/22, loss 1.5099358558654785\n","epoch 73/1000, step 20/22, loss 1.5365185737609863\n","epoch 73/1000, step 21/22, loss 1.5163218975067139\n","epoch 73/1000, step 22/22, loss 1.508825659751892\n","epoch 74/1000, step 1/22, loss 1.5231729745864868\n","epoch 74/1000, step 2/22, loss 1.524956226348877\n","epoch 74/1000, step 3/22, loss 1.5013786554336548\n","epoch 74/1000, step 4/22, loss 1.519944429397583\n","epoch 74/1000, step 5/22, loss 1.5247722864151\n","epoch 74/1000, step 6/22, loss 1.5106860399246216\n","epoch 74/1000, step 7/22, loss 1.5082570314407349\n","epoch 74/1000, step 8/22, loss 1.529964566230774\n","epoch 74/1000, step 9/22, loss 1.515153169631958\n","epoch 74/1000, step 10/22, loss 1.5328203439712524\n","epoch 74/1000, step 11/22, loss 1.5228161811828613\n","epoch 74/1000, step 12/22, loss 1.5187904834747314\n","epoch 74/1000, step 13/22, loss 1.5307456254959106\n","epoch 74/1000, step 14/22, loss 1.5018694400787354\n","epoch 74/1000, step 15/22, loss 1.521287441253662\n","epoch 74/1000, step 16/22, loss 1.5117084980010986\n","epoch 74/1000, step 17/22, loss 1.5129808187484741\n","epoch 74/1000, step 18/22, loss 1.5277382135391235\n","epoch 74/1000, step 19/22, loss 1.5100083351135254\n","epoch 74/1000, step 20/22, loss 1.5365923643112183\n","epoch 74/1000, step 21/22, loss 1.5163931846618652\n","epoch 74/1000, step 22/22, loss 1.5087993144989014\n","epoch 75/1000, step 1/22, loss 1.5232439041137695\n","epoch 75/1000, step 2/22, loss 1.5250256061553955\n","epoch 75/1000, step 3/22, loss 1.5014458894729614\n","epoch 75/1000, step 4/22, loss 1.5200207233428955\n","epoch 75/1000, step 5/22, loss 1.5248432159423828\n","epoch 75/1000, step 6/22, loss 1.5107556581497192\n","epoch 75/1000, step 7/22, loss 1.5083304643630981\n","epoch 75/1000, step 8/22, loss 1.5300395488739014\n","epoch 75/1000, step 9/22, loss 1.5152196884155273\n","epoch 75/1000, step 10/22, loss 1.5328900814056396\n","epoch 75/1000, step 11/22, loss 1.5228935480117798\n","epoch 75/1000, step 12/22, loss 1.5188689231872559\n","epoch 75/1000, step 13/22, loss 1.5308283567428589\n","epoch 75/1000, step 14/22, loss 1.5019463300704956\n","epoch 75/1000, step 15/22, loss 1.5213680267333984\n","epoch 75/1000, step 16/22, loss 1.511783480644226\n","epoch 75/1000, step 17/22, loss 1.5130568742752075\n","epoch 75/1000, step 18/22, loss 1.527817964553833\n","epoch 75/1000, step 19/22, loss 1.510087251663208\n","epoch 75/1000, step 20/22, loss 1.536676049232483\n","epoch 75/1000, step 21/22, loss 1.5164744853973389\n","epoch 75/1000, step 22/22, loss 1.5087858438491821\n","epoch 76/1000, step 1/22, loss 1.5233272314071655\n","epoch 76/1000, step 2/22, loss 1.5251140594482422\n","epoch 76/1000, step 3/22, loss 1.5015220642089844\n","epoch 76/1000, step 4/22, loss 1.5201088190078735\n","epoch 76/1000, step 5/22, loss 1.5249286890029907\n","epoch 76/1000, step 6/22, loss 1.510837197303772\n","epoch 76/1000, step 7/22, loss 1.5084120035171509\n","epoch 76/1000, step 8/22, loss 1.5301235914230347\n","epoch 76/1000, step 9/22, loss 1.515290379524231\n","epoch 76/1000, step 10/22, loss 1.5329710245132446\n","epoch 76/1000, step 11/22, loss 1.5229820013046265\n","epoch 76/1000, step 12/22, loss 1.518955945968628\n","epoch 76/1000, step 13/22, loss 1.5309154987335205\n","epoch 76/1000, step 14/22, loss 1.5020270347595215\n","epoch 76/1000, step 15/22, loss 1.5214507579803467\n","epoch 76/1000, step 16/22, loss 1.5118600130081177\n","epoch 76/1000, step 17/22, loss 1.5131406784057617\n","epoch 76/1000, step 18/22, loss 1.52790367603302\n","epoch 76/1000, step 19/22, loss 1.5101677179336548\n","epoch 76/1000, step 20/22, loss 1.5367660522460938\n","epoch 76/1000, step 21/22, loss 1.516556978225708\n","epoch 76/1000, step 22/22, loss 1.5087761878967285\n","epoch 77/1000, step 1/22, loss 1.5234133005142212\n","epoch 77/1000, step 2/22, loss 1.525201439857483\n","epoch 77/1000, step 3/22, loss 1.501603603363037\n","epoch 77/1000, step 4/22, loss 1.520199179649353\n","epoch 77/1000, step 5/22, loss 1.5250118970870972\n","epoch 77/1000, step 6/22, loss 1.510918140411377\n","epoch 77/1000, step 7/22, loss 1.5084974765777588\n","epoch 77/1000, step 8/22, loss 1.5302162170410156\n","epoch 77/1000, step 9/22, loss 1.5153661966323853\n","epoch 77/1000, step 10/22, loss 1.5330599546432495\n","epoch 77/1000, step 11/22, loss 1.5230740308761597\n","epoch 77/1000, step 12/22, loss 1.5190460681915283\n","epoch 77/1000, step 13/22, loss 1.531001329421997\n","epoch 77/1000, step 14/22, loss 1.5021110773086548\n","epoch 77/1000, step 15/22, loss 1.5215351581573486\n","epoch 77/1000, step 16/22, loss 1.511944055557251\n","epoch 77/1000, step 17/22, loss 1.5132288932800293\n","epoch 77/1000, step 18/22, loss 1.5279955863952637\n","epoch 77/1000, step 19/22, loss 1.5102494955062866\n","epoch 77/1000, step 20/22, loss 1.536861538887024\n","epoch 77/1000, step 21/22, loss 1.5166491270065308\n","epoch 77/1000, step 22/22, loss 1.508767008781433\n","epoch 78/1000, step 1/22, loss 1.52349853515625\n","epoch 78/1000, step 2/22, loss 1.5252869129180908\n","epoch 78/1000, step 3/22, loss 1.5016906261444092\n","epoch 78/1000, step 4/22, loss 1.5202938318252563\n","epoch 78/1000, step 5/22, loss 1.5250917673110962\n","epoch 78/1000, step 6/22, loss 1.5110059976577759\n","epoch 78/1000, step 7/22, loss 1.5085822343826294\n","epoch 78/1000, step 8/22, loss 1.530316710472107\n","epoch 78/1000, step 9/22, loss 1.5154472589492798\n","epoch 78/1000, step 10/22, loss 1.5331474542617798\n","epoch 78/1000, step 11/22, loss 1.523170828819275\n","epoch 78/1000, step 12/22, loss 1.5191340446472168\n","epoch 78/1000, step 13/22, loss 1.5310684442520142\n","epoch 78/1000, step 14/22, loss 1.5021697282791138\n","epoch 78/1000, step 15/22, loss 1.5215965509414673\n","epoch 78/1000, step 16/22, loss 1.51200270652771\n","epoch 78/1000, step 17/22, loss 1.5132966041564941\n","epoch 78/1000, step 18/22, loss 1.5280603170394897\n","epoch 78/1000, step 19/22, loss 1.5103093385696411\n","epoch 78/1000, step 20/22, loss 1.5369378328323364\n","epoch 78/1000, step 21/22, loss 1.5167135000228882\n","epoch 78/1000, step 22/22, loss 1.5086629390716553\n","epoch 79/1000, step 1/22, loss 1.5235636234283447\n","epoch 79/1000, step 2/22, loss 1.5253548622131348\n","epoch 79/1000, step 3/22, loss 1.501753330230713\n","epoch 79/1000, step 4/22, loss 1.5203646421432495\n","epoch 79/1000, step 5/22, loss 1.525151014328003\n","epoch 79/1000, step 6/22, loss 1.5110708475112915\n","epoch 79/1000, step 7/22, loss 1.5086493492126465\n","epoch 79/1000, step 8/22, loss 1.5303868055343628\n","epoch 79/1000, step 9/22, loss 1.5155034065246582\n","epoch 79/1000, step 10/22, loss 1.5332200527191162\n","epoch 79/1000, step 11/22, loss 1.5232442617416382\n","epoch 79/1000, step 12/22, loss 1.5192028284072876\n","epoch 79/1000, step 13/22, loss 1.5311557054519653\n","epoch 79/1000, step 14/22, loss 1.5022575855255127\n","epoch 79/1000, step 15/22, loss 1.5216810703277588\n","epoch 79/1000, step 16/22, loss 1.5120865106582642\n","epoch 79/1000, step 17/22, loss 1.5133856534957886\n","epoch 79/1000, step 18/22, loss 1.5281490087509155\n","epoch 79/1000, step 19/22, loss 1.51039719581604\n","epoch 79/1000, step 20/22, loss 1.5370328426361084\n","epoch 79/1000, step 21/22, loss 1.5168001651763916\n","epoch 79/1000, step 22/22, loss 1.508653998374939\n","epoch 80/1000, step 1/22, loss 1.5236499309539795\n","epoch 80/1000, step 2/22, loss 1.5254418849945068\n","epoch 80/1000, step 3/22, loss 1.5018324851989746\n","epoch 80/1000, step 4/22, loss 1.5204553604125977\n","epoch 80/1000, step 5/22, loss 1.5252290964126587\n","epoch 80/1000, step 6/22, loss 1.5111558437347412\n","epoch 80/1000, step 7/22, loss 1.5087348222732544\n","epoch 80/1000, step 8/22, loss 1.530488133430481\n","epoch 80/1000, step 9/22, loss 1.5155770778656006\n","epoch 80/1000, step 10/22, loss 1.5333055257797241\n","epoch 80/1000, step 11/22, loss 1.5233407020568848\n","epoch 80/1000, step 12/22, loss 1.5192934274673462\n","epoch 80/1000, step 13/22, loss 1.5312360525131226\n","epoch 80/1000, step 14/22, loss 1.5023369789123535\n","epoch 80/1000, step 15/22, loss 1.5217666625976562\n","epoch 80/1000, step 16/22, loss 1.5121738910675049\n","epoch 80/1000, step 17/22, loss 1.5134737491607666\n","epoch 80/1000, step 18/22, loss 1.5282315015792847\n","epoch 80/1000, step 19/22, loss 1.5104743242263794\n","epoch 80/1000, step 20/22, loss 1.5371214151382446\n","epoch 80/1000, step 21/22, loss 1.5168819427490234\n","epoch 80/1000, step 22/22, loss 1.5086426734924316\n","epoch 81/1000, step 1/22, loss 1.5237311124801636\n","epoch 81/1000, step 2/22, loss 1.5255266427993774\n","epoch 81/1000, step 3/22, loss 1.5019108057022095\n","epoch 81/1000, step 4/22, loss 1.5205425024032593\n","epoch 81/1000, step 5/22, loss 1.5252991914749146\n","epoch 81/1000, step 6/22, loss 1.5112346410751343\n","epoch 81/1000, step 7/22, loss 1.508818507194519\n","epoch 81/1000, step 8/22, loss 1.5305794477462769\n","epoch 81/1000, step 9/22, loss 1.5156497955322266\n","epoch 81/1000, step 10/22, loss 1.5333904027938843\n","epoch 81/1000, step 11/22, loss 1.5234298706054688\n","epoch 81/1000, step 12/22, loss 1.519374132156372\n","epoch 81/1000, step 13/22, loss 1.5313152074813843\n","epoch 81/1000, step 14/22, loss 1.502414345741272\n","epoch 81/1000, step 15/22, loss 1.5218517780303955\n","epoch 81/1000, step 16/22, loss 1.5122532844543457\n","epoch 81/1000, step 17/22, loss 1.5135486125946045\n","epoch 81/1000, step 18/22, loss 1.5283128023147583\n","epoch 81/1000, step 19/22, loss 1.5105458498001099\n","epoch 81/1000, step 20/22, loss 1.5372061729431152\n","epoch 81/1000, step 21/22, loss 1.5169631242752075\n","epoch 81/1000, step 22/22, loss 1.5086358785629272\n","epoch 82/1000, step 1/22, loss 1.5238112211227417\n","epoch 82/1000, step 2/22, loss 1.5256011486053467\n","epoch 82/1000, step 3/22, loss 1.5019813776016235\n","epoch 82/1000, step 4/22, loss 1.5206347703933716\n","epoch 82/1000, step 5/22, loss 1.5253745317459106\n","epoch 82/1000, step 6/22, loss 1.5113192796707153\n","epoch 82/1000, step 7/22, loss 1.508901596069336\n","epoch 82/1000, step 8/22, loss 1.5306674242019653\n","epoch 82/1000, step 9/22, loss 1.5157173871994019\n","epoch 82/1000, step 10/22, loss 1.5334727764129639\n","epoch 82/1000, step 11/22, loss 1.5235129594802856\n","epoch 82/1000, step 12/22, loss 1.519451379776001\n","epoch 82/1000, step 13/22, loss 1.5313881635665894\n","epoch 82/1000, step 14/22, loss 1.5024880170822144\n","epoch 82/1000, step 15/22, loss 1.521934986114502\n","epoch 82/1000, step 16/22, loss 1.512331485748291\n","epoch 82/1000, step 17/22, loss 1.5136277675628662\n","epoch 82/1000, step 18/22, loss 1.5283931493759155\n","epoch 82/1000, step 19/22, loss 1.510615587234497\n","epoch 82/1000, step 20/22, loss 1.5372910499572754\n","epoch 82/1000, step 21/22, loss 1.5170416831970215\n","epoch 82/1000, step 22/22, loss 1.5086262226104736\n","epoch 83/1000, step 1/22, loss 1.523887276649475\n","epoch 83/1000, step 2/22, loss 1.5256781578063965\n","epoch 83/1000, step 3/22, loss 1.502054214477539\n","epoch 83/1000, step 4/22, loss 1.5207196474075317\n","epoch 83/1000, step 5/22, loss 1.5254476070404053\n","epoch 83/1000, step 6/22, loss 1.5113962888717651\n","epoch 83/1000, step 7/22, loss 1.5089774131774902\n","epoch 83/1000, step 8/22, loss 1.5307531356811523\n","epoch 83/1000, step 9/22, loss 1.515787124633789\n","epoch 83/1000, step 10/22, loss 1.5335466861724854\n","epoch 83/1000, step 11/22, loss 1.5235931873321533\n","epoch 83/1000, step 12/22, loss 1.519521713256836\n","epoch 83/1000, step 13/22, loss 1.5314586162567139\n","epoch 83/1000, step 14/22, loss 1.5025537014007568\n","epoch 83/1000, step 15/22, loss 1.5220032930374146\n","epoch 83/1000, step 16/22, loss 1.5123969316482544\n","epoch 83/1000, step 17/22, loss 1.5136921405792236\n","epoch 83/1000, step 18/22, loss 1.5284658670425415\n","epoch 83/1000, step 19/22, loss 1.5106806755065918\n","epoch 83/1000, step 20/22, loss 1.537367582321167\n","epoch 83/1000, step 21/22, loss 1.5171114206314087\n","epoch 83/1000, step 22/22, loss 1.5086004734039307\n","epoch 84/1000, step 1/22, loss 1.5239543914794922\n","epoch 84/1000, step 2/22, loss 1.5257459878921509\n","epoch 84/1000, step 3/22, loss 1.5021204948425293\n","epoch 84/1000, step 4/22, loss 1.5207955837249756\n","epoch 84/1000, step 5/22, loss 1.525516152381897\n","epoch 84/1000, step 6/22, loss 1.5114554166793823\n","epoch 84/1000, step 7/22, loss 1.5090429782867432\n","epoch 84/1000, step 8/22, loss 1.530828833580017\n","epoch 84/1000, step 9/22, loss 1.5158472061157227\n","epoch 84/1000, step 10/22, loss 1.5336133241653442\n","epoch 84/1000, step 11/22, loss 1.523665189743042\n","epoch 84/1000, step 12/22, loss 1.5195828676223755\n","epoch 84/1000, step 13/22, loss 1.5315215587615967\n","epoch 84/1000, step 14/22, loss 1.502625584602356\n","epoch 84/1000, step 15/22, loss 1.5220774412155151\n","epoch 84/1000, step 16/22, loss 1.512466311454773\n","epoch 84/1000, step 17/22, loss 1.5137596130371094\n","epoch 84/1000, step 18/22, loss 1.5285335779190063\n","epoch 84/1000, step 19/22, loss 1.5107489824295044\n","epoch 84/1000, step 20/22, loss 1.5374383926391602\n","epoch 84/1000, step 21/22, loss 1.5171763896942139\n","epoch 84/1000, step 22/22, loss 1.5085937976837158\n","epoch 85/1000, step 1/22, loss 1.524023413658142\n","epoch 85/1000, step 2/22, loss 1.5258115530014038\n","epoch 85/1000, step 3/22, loss 1.5021815299987793\n","epoch 85/1000, step 4/22, loss 1.5208722352981567\n","epoch 85/1000, step 5/22, loss 1.525580883026123\n","epoch 85/1000, step 6/22, loss 1.511521816253662\n","epoch 85/1000, step 7/22, loss 1.5091118812561035\n","epoch 85/1000, step 8/22, loss 1.530902624130249\n","epoch 85/1000, step 9/22, loss 1.5159012079238892\n","epoch 85/1000, step 10/22, loss 1.5336828231811523\n","epoch 85/1000, step 11/22, loss 1.5237380266189575\n","epoch 85/1000, step 12/22, loss 1.5196470022201538\n","epoch 85/1000, step 13/22, loss 1.5315780639648438\n","epoch 85/1000, step 14/22, loss 1.502690076828003\n","epoch 85/1000, step 15/22, loss 1.5221421718597412\n","epoch 85/1000, step 16/22, loss 1.5125256776809692\n","epoch 85/1000, step 17/22, loss 1.5138237476348877\n","epoch 85/1000, step 18/22, loss 1.5285991430282593\n","epoch 85/1000, step 19/22, loss 1.510804295539856\n","epoch 85/1000, step 20/22, loss 1.5375033617019653\n","epoch 85/1000, step 21/22, loss 1.5172334909439087\n","epoch 85/1000, step 22/22, loss 1.5085859298706055\n","epoch 86/1000, step 1/22, loss 1.5240869522094727\n","epoch 86/1000, step 2/22, loss 1.5258735418319702\n","epoch 86/1000, step 3/22, loss 1.5022428035736084\n","epoch 86/1000, step 4/22, loss 1.520943284034729\n","epoch 86/1000, step 5/22, loss 1.5256333351135254\n","epoch 86/1000, step 6/22, loss 1.511584997177124\n","epoch 86/1000, step 7/22, loss 1.509174108505249\n","epoch 86/1000, step 8/22, loss 1.5309703350067139\n","epoch 86/1000, step 9/22, loss 1.5159587860107422\n","epoch 86/1000, step 10/22, loss 1.533745288848877\n","epoch 86/1000, step 11/22, loss 1.5238006114959717\n","epoch 86/1000, step 12/22, loss 1.5197030305862427\n","epoch 86/1000, step 13/22, loss 1.5316346883773804\n","epoch 86/1000, step 14/22, loss 1.5027512311935425\n","epoch 86/1000, step 15/22, loss 1.5221986770629883\n","epoch 86/1000, step 16/22, loss 1.5125751495361328\n","epoch 86/1000, step 17/22, loss 1.5138825178146362\n","epoch 86/1000, step 18/22, loss 1.5286575555801392\n","epoch 86/1000, step 19/22, loss 1.5108574628829956\n","epoch 86/1000, step 20/22, loss 1.5375659465789795\n","epoch 86/1000, step 21/22, loss 1.517281174659729\n","epoch 86/1000, step 22/22, loss 1.5085771083831787\n","epoch 87/1000, step 1/22, loss 1.5241440534591675\n","epoch 87/1000, step 2/22, loss 1.525917410850525\n","epoch 87/1000, step 3/22, loss 1.5022835731506348\n","epoch 87/1000, step 4/22, loss 1.5209945440292358\n","epoch 87/1000, step 5/22, loss 1.5256723165512085\n","epoch 87/1000, step 6/22, loss 1.511628270149231\n","epoch 87/1000, step 7/22, loss 1.5092216730117798\n","epoch 87/1000, step 8/22, loss 1.5310251712799072\n","epoch 87/1000, step 9/22, loss 1.5159956216812134\n","epoch 87/1000, step 10/22, loss 1.5337902307510376\n","epoch 87/1000, step 11/22, loss 1.5238492488861084\n","epoch 87/1000, step 12/22, loss 1.5197449922561646\n","epoch 87/1000, step 13/22, loss 1.5316776037216187\n","epoch 87/1000, step 14/22, loss 1.5027925968170166\n","epoch 87/1000, step 15/22, loss 1.5222398042678833\n","epoch 87/1000, step 16/22, loss 1.512616515159607\n","epoch 87/1000, step 17/22, loss 1.513925313949585\n","epoch 87/1000, step 18/22, loss 1.5286996364593506\n","epoch 87/1000, step 19/22, loss 1.510898232460022\n","epoch 87/1000, step 20/22, loss 1.5376124382019043\n","epoch 87/1000, step 21/22, loss 1.5173193216323853\n","epoch 87/1000, step 22/22, loss 1.5085722208023071\n","epoch 88/1000, step 1/22, loss 1.5241875648498535\n","epoch 88/1000, step 2/22, loss 1.5259652137756348\n","epoch 88/1000, step 3/22, loss 1.5023280382156372\n","epoch 88/1000, step 4/22, loss 1.521048903465271\n","epoch 88/1000, step 5/22, loss 1.5257198810577393\n","epoch 88/1000, step 6/22, loss 1.5116760730743408\n","epoch 88/1000, step 7/22, loss 1.5092717409133911\n","epoch 88/1000, step 8/22, loss 1.531081199645996\n","epoch 88/1000, step 9/22, loss 1.5160419940948486\n","epoch 88/1000, step 10/22, loss 1.5338410139083862\n","epoch 88/1000, step 11/22, loss 1.5239009857177734\n","epoch 88/1000, step 12/22, loss 1.5197927951812744\n","epoch 88/1000, step 13/22, loss 1.531723976135254\n","epoch 88/1000, step 14/22, loss 1.502841591835022\n","epoch 88/1000, step 15/22, loss 1.5222834348678589\n","epoch 88/1000, step 16/22, loss 1.5126590728759766\n","epoch 88/1000, step 17/22, loss 1.5139713287353516\n","epoch 88/1000, step 18/22, loss 1.5287474393844604\n","epoch 88/1000, step 19/22, loss 1.5109459161758423\n","epoch 88/1000, step 20/22, loss 1.5376588106155396\n","epoch 88/1000, step 21/22, loss 1.5173614025115967\n","epoch 88/1000, step 22/22, loss 1.5085643529891968\n","epoch 89/1000, step 1/22, loss 1.5242302417755127\n","epoch 89/1000, step 2/22, loss 1.5260096788406372\n","epoch 89/1000, step 3/22, loss 1.5023690462112427\n","epoch 89/1000, step 4/22, loss 1.5210965871810913\n","epoch 89/1000, step 5/22, loss 1.525758981704712\n","epoch 89/1000, step 6/22, loss 1.5117202997207642\n","epoch 89/1000, step 7/22, loss 1.509317398071289\n","epoch 89/1000, step 8/22, loss 1.5311321020126343\n","epoch 89/1000, step 9/22, loss 1.5160834789276123\n","epoch 89/1000, step 10/22, loss 1.5338832139968872\n","epoch 89/1000, step 11/22, loss 1.5239442586898804\n","epoch 89/1000, step 12/22, loss 1.519838571548462\n","epoch 89/1000, step 13/22, loss 1.531764268875122\n","epoch 89/1000, step 14/22, loss 1.502880334854126\n","epoch 89/1000, step 15/22, loss 1.522325038909912\n","epoch 89/1000, step 16/22, loss 1.5127006769180298\n","epoch 89/1000, step 17/22, loss 1.5140135288238525\n","epoch 89/1000, step 18/22, loss 1.5287885665893555\n","epoch 89/1000, step 19/22, loss 1.5109856128692627\n","epoch 89/1000, step 20/22, loss 1.5377013683319092\n","epoch 89/1000, step 21/22, loss 1.5173972845077515\n","epoch 89/1000, step 22/22, loss 1.508557677268982\n","epoch 90/1000, step 1/22, loss 1.5242693424224854\n","epoch 90/1000, step 2/22, loss 1.5260473489761353\n","epoch 90/1000, step 3/22, loss 1.5024070739746094\n","epoch 90/1000, step 4/22, loss 1.5211457014083862\n","epoch 90/1000, step 5/22, loss 1.5257933139801025\n","epoch 90/1000, step 6/22, loss 1.511430263519287\n","epoch 90/1000, step 7/22, loss 1.5089997053146362\n","epoch 90/1000, step 8/22, loss 1.5308654308319092\n","epoch 90/1000, step 9/22, loss 1.5158058404922485\n","epoch 90/1000, step 10/22, loss 1.5336191654205322\n","epoch 90/1000, step 11/22, loss 1.5236397981643677\n","epoch 90/1000, step 12/22, loss 1.519545555114746\n","epoch 90/1000, step 13/22, loss 1.5315046310424805\n","epoch 90/1000, step 14/22, loss 1.5025629997253418\n","epoch 90/1000, step 15/22, loss 1.522054672241211\n","epoch 90/1000, step 16/22, loss 1.512407898902893\n","epoch 90/1000, step 17/22, loss 1.5137377977371216\n","epoch 90/1000, step 18/22, loss 1.5284929275512695\n","epoch 90/1000, step 19/22, loss 1.5106595754623413\n","epoch 90/1000, step 20/22, loss 1.537462830543518\n","epoch 90/1000, step 21/22, loss 1.5171034336090088\n","epoch 90/1000, step 22/22, loss 1.5073078870773315\n","epoch 91/1000, step 1/22, loss 1.523991346359253\n","epoch 91/1000, step 2/22, loss 1.5257625579833984\n","epoch 91/1000, step 3/22, loss 1.5020740032196045\n","epoch 91/1000, step 4/22, loss 1.520836353302002\n","epoch 91/1000, step 5/22, loss 1.5255235433578491\n","epoch 91/1000, step 6/22, loss 1.511467456817627\n","epoch 91/1000, step 7/22, loss 1.5090359449386597\n","epoch 91/1000, step 8/22, loss 1.5309070348739624\n","epoch 91/1000, step 9/22, loss 1.5158352851867676\n","epoch 91/1000, step 10/22, loss 1.5336552858352661\n","epoch 91/1000, step 11/22, loss 1.523675560951233\n","epoch 91/1000, step 12/22, loss 1.5195798873901367\n","epoch 91/1000, step 13/22, loss 1.5315347909927368\n","epoch 91/1000, step 14/22, loss 1.502596378326416\n","epoch 91/1000, step 15/22, loss 1.5220893621444702\n","epoch 91/1000, step 16/22, loss 1.51244056224823\n","epoch 91/1000, step 17/22, loss 1.5137739181518555\n","epoch 91/1000, step 18/22, loss 1.5285245180130005\n","epoch 91/1000, step 19/22, loss 1.5106905698776245\n","epoch 91/1000, step 20/22, loss 1.5374962091445923\n","epoch 91/1000, step 21/22, loss 1.5171321630477905\n","epoch 91/1000, step 22/22, loss 1.5073041915893555\n","epoch 92/1000, step 1/22, loss 1.5240256786346436\n","epoch 92/1000, step 2/22, loss 1.525791049003601\n","epoch 92/1000, step 3/22, loss 1.502102017402649\n","epoch 92/1000, step 4/22, loss 1.5208717584609985\n","epoch 92/1000, step 5/22, loss 1.5255485773086548\n","epoch 92/1000, step 6/22, loss 1.511496901512146\n","epoch 92/1000, step 7/22, loss 1.5090694427490234\n","epoch 92/1000, step 8/22, loss 1.5309422016143799\n","epoch 92/1000, step 9/22, loss 1.5158607959747314\n","epoch 92/1000, step 10/22, loss 1.533686876296997\n","epoch 92/1000, step 11/22, loss 1.5237102508544922\n","epoch 92/1000, step 12/22, loss 1.5196107625961304\n","epoch 92/1000, step 13/22, loss 1.5315642356872559\n","epoch 92/1000, step 14/22, loss 1.502626895904541\n","epoch 92/1000, step 15/22, loss 1.5221166610717773\n","epoch 92/1000, step 16/22, loss 1.5124708414077759\n","epoch 92/1000, step 17/22, loss 1.513801097869873\n","epoch 92/1000, step 18/22, loss 1.5285539627075195\n","epoch 92/1000, step 19/22, loss 1.5107166767120361\n","epoch 92/1000, step 20/22, loss 1.5375279188156128\n","epoch 92/1000, step 21/22, loss 1.5171566009521484\n","epoch 92/1000, step 22/22, loss 1.5072970390319824\n","epoch 93/1000, step 1/22, loss 1.5240538120269775\n","epoch 93/1000, step 2/22, loss 1.5258151292800903\n","epoch 93/1000, step 3/22, loss 1.5021283626556396\n","epoch 93/1000, step 4/22, loss 1.5209029912948608\n","epoch 93/1000, step 5/22, loss 1.525572657585144\n","epoch 93/1000, step 6/22, loss 1.5115257501602173\n","epoch 93/1000, step 7/22, loss 1.5090986490249634\n","epoch 93/1000, step 8/22, loss 1.5309722423553467\n","epoch 93/1000, step 9/22, loss 1.5158848762512207\n","epoch 93/1000, step 10/22, loss 1.5337146520614624\n","epoch 93/1000, step 11/22, loss 1.523740291595459\n","epoch 93/1000, step 12/22, loss 1.5196343660354614\n","epoch 93/1000, step 13/22, loss 1.5315903425216675\n","epoch 93/1000, step 14/22, loss 1.5026519298553467\n","epoch 93/1000, step 15/22, loss 1.5221428871154785\n","epoch 93/1000, step 16/22, loss 1.5124945640563965\n","epoch 93/1000, step 17/22, loss 1.5138267278671265\n","epoch 93/1000, step 18/22, loss 1.528580665588379\n","epoch 93/1000, step 19/22, loss 1.5107392072677612\n","epoch 93/1000, step 20/22, loss 1.5375562906265259\n","epoch 93/1000, step 21/22, loss 1.517177939414978\n","epoch 93/1000, step 22/22, loss 1.5072910785675049\n","epoch 94/1000, step 1/22, loss 1.5240761041641235\n","epoch 94/1000, step 2/22, loss 1.525837779045105\n","epoch 94/1000, step 3/22, loss 1.5021486282348633\n","epoch 94/1000, step 4/22, loss 1.520928978919983\n","epoch 94/1000, step 5/22, loss 1.5255917310714722\n","epoch 94/1000, step 6/22, loss 1.5115506649017334\n","epoch 94/1000, step 7/22, loss 1.509122371673584\n","epoch 94/1000, step 8/22, loss 1.5309983491897583\n","epoch 94/1000, step 9/22, loss 1.5159040689468384\n","epoch 94/1000, step 10/22, loss 1.5337350368499756\n","epoch 94/1000, step 11/22, loss 1.5237627029418945\n","epoch 94/1000, step 12/22, loss 1.519657015800476\n","epoch 94/1000, step 13/22, loss 1.5316109657287598\n","epoch 94/1000, step 14/22, loss 1.502671718597412\n","epoch 94/1000, step 15/22, loss 1.5221643447875977\n","epoch 94/1000, step 16/22, loss 1.512515902519226\n","epoch 94/1000, step 17/22, loss 1.5138441324234009\n","epoch 94/1000, step 18/22, loss 1.5286016464233398\n","epoch 94/1000, step 19/22, loss 1.5107587575912476\n","epoch 94/1000, step 20/22, loss 1.5375761985778809\n","epoch 94/1000, step 21/22, loss 1.5171953439712524\n","epoch 94/1000, step 22/22, loss 1.5072863101959229\n","epoch 95/1000, step 1/22, loss 1.524099349975586\n","epoch 95/1000, step 2/22, loss 1.525856375694275\n","epoch 95/1000, step 3/22, loss 1.502169132232666\n","epoch 95/1000, step 4/22, loss 1.520949363708496\n","epoch 95/1000, step 5/22, loss 1.5256084203720093\n","epoch 95/1000, step 6/22, loss 1.5115694999694824\n","epoch 95/1000, step 7/22, loss 1.5091437101364136\n","epoch 95/1000, step 8/22, loss 1.5310198068618774\n","epoch 95/1000, step 9/22, loss 1.5159223079681396\n","epoch 95/1000, step 10/22, loss 1.5337510108947754\n","epoch 95/1000, step 11/22, loss 1.5237821340560913\n","epoch 95/1000, step 12/22, loss 1.5196752548217773\n","epoch 95/1000, step 13/22, loss 1.5316320657730103\n","epoch 95/1000, step 14/22, loss 1.5026904344558716\n","epoch 95/1000, step 15/22, loss 1.522183895111084\n","epoch 95/1000, step 16/22, loss 1.51253342628479\n","epoch 95/1000, step 17/22, loss 1.5138628482818604\n","epoch 95/1000, step 18/22, loss 1.5286216735839844\n","epoch 95/1000, step 19/22, loss 1.5107780694961548\n","epoch 95/1000, step 20/22, loss 1.5375968217849731\n","epoch 95/1000, step 21/22, loss 1.5172107219696045\n","epoch 95/1000, step 22/22, loss 1.5072836875915527\n","epoch 96/1000, step 1/22, loss 1.5241190195083618\n","epoch 96/1000, step 2/22, loss 1.525874376296997\n","epoch 96/1000, step 3/22, loss 1.5021860599517822\n","epoch 96/1000, step 4/22, loss 1.520966649055481\n","epoch 96/1000, step 5/22, loss 1.5256223678588867\n","epoch 96/1000, step 6/22, loss 1.5115869045257568\n","epoch 96/1000, step 7/22, loss 1.509161353111267\n","epoch 96/1000, step 8/22, loss 1.5310410261154175\n","epoch 96/1000, step 9/22, loss 1.515938401222229\n","epoch 96/1000, step 10/22, loss 1.5337674617767334\n","epoch 96/1000, step 11/22, loss 1.5238014459609985\n","epoch 96/1000, step 12/22, loss 1.5196914672851562\n","epoch 96/1000, step 13/22, loss 1.5316474437713623\n","epoch 96/1000, step 14/22, loss 1.5027066469192505\n","epoch 96/1000, step 15/22, loss 1.5222009420394897\n","epoch 96/1000, step 16/22, loss 1.5125513076782227\n","epoch 96/1000, step 17/22, loss 1.5138782262802124\n","epoch 96/1000, step 18/22, loss 1.528641939163208\n","epoch 96/1000, step 19/22, loss 1.5107922554016113\n","epoch 96/1000, step 20/22, loss 1.5376142263412476\n","epoch 96/1000, step 21/22, loss 1.5172253847122192\n","epoch 96/1000, step 22/22, loss 1.5072804689407349\n","epoch 97/1000, step 1/22, loss 1.5241341590881348\n","epoch 97/1000, step 2/22, loss 1.5258909463882446\n","epoch 97/1000, step 3/22, loss 1.5022003650665283\n","epoch 97/1000, step 4/22, loss 1.5209840536117554\n","epoch 97/1000, step 5/22, loss 1.525634765625\n","epoch 97/1000, step 6/22, loss 1.5116008520126343\n","epoch 97/1000, step 7/22, loss 1.5091774463653564\n","epoch 97/1000, step 8/22, loss 1.5310560464859009\n","epoch 97/1000, step 9/22, loss 1.5159528255462646\n","epoch 97/1000, step 10/22, loss 1.5337830781936646\n","epoch 97/1000, step 11/22, loss 1.5238171815872192\n","epoch 97/1000, step 12/22, loss 1.51970636844635\n","epoch 97/1000, step 13/22, loss 1.531661033630371\n","epoch 97/1000, step 14/22, loss 1.502722144126892\n","epoch 97/1000, step 15/22, loss 1.5222160816192627\n","epoch 97/1000, step 16/22, loss 1.5125646591186523\n","epoch 97/1000, step 17/22, loss 1.5138908624649048\n","epoch 97/1000, step 18/22, loss 1.5286556482315063\n","epoch 97/1000, step 19/22, loss 1.5108063220977783\n","epoch 97/1000, step 20/22, loss 1.5376307964324951\n","epoch 97/1000, step 21/22, loss 1.517238974571228\n","epoch 97/1000, step 22/22, loss 1.5072784423828125\n","epoch 98/1000, step 1/22, loss 1.5241484642028809\n","epoch 98/1000, step 2/22, loss 1.5259040594100952\n","epoch 98/1000, step 3/22, loss 1.5022143125534058\n","epoch 98/1000, step 4/22, loss 1.520999550819397\n","epoch 98/1000, step 5/22, loss 1.5256459712982178\n","epoch 98/1000, step 6/22, loss 1.5116143226623535\n","epoch 98/1000, step 7/22, loss 1.5091885328292847\n","epoch 98/1000, step 8/22, loss 1.5310696363449097\n","epoch 98/1000, step 9/22, loss 1.5159679651260376\n","epoch 98/1000, step 10/22, loss 1.5337958335876465\n","epoch 98/1000, step 11/22, loss 1.5238319635391235\n","epoch 98/1000, step 12/22, loss 1.5197206735610962\n","epoch 98/1000, step 13/22, loss 1.5316730737686157\n","epoch 98/1000, step 14/22, loss 1.502732515335083\n","epoch 98/1000, step 15/22, loss 1.522229552268982\n","epoch 98/1000, step 16/22, loss 1.5125739574432373\n","epoch 98/1000, step 17/22, loss 1.513900637626648\n","epoch 98/1000, step 18/22, loss 1.528664231300354\n","epoch 98/1000, step 19/22, loss 1.5108153820037842\n","epoch 98/1000, step 20/22, loss 1.5376394987106323\n","epoch 98/1000, step 21/22, loss 1.517244577407837\n","epoch 98/1000, step 22/22, loss 1.5072755813598633\n","epoch 99/1000, step 1/22, loss 1.5241589546203613\n","epoch 99/1000, step 2/22, loss 1.525913119316101\n","epoch 99/1000, step 3/22, loss 1.5022237300872803\n","epoch 99/1000, step 4/22, loss 1.521010398864746\n","epoch 99/1000, step 5/22, loss 1.5256544351577759\n","epoch 99/1000, step 6/22, loss 1.5116215944290161\n","epoch 99/1000, step 7/22, loss 1.5091973543167114\n","epoch 99/1000, step 8/22, loss 1.5310797691345215\n","epoch 99/1000, step 9/22, loss 1.515971302986145\n","epoch 99/1000, step 10/22, loss 1.5337995290756226\n","epoch 99/1000, step 11/22, loss 1.5238357782363892\n","epoch 99/1000, step 12/22, loss 1.5197227001190186\n","epoch 99/1000, step 13/22, loss 1.5316777229309082\n","epoch 99/1000, step 14/22, loss 1.5027357339859009\n","epoch 99/1000, step 15/22, loss 1.5222347974777222\n","epoch 99/1000, step 16/22, loss 1.5125792026519775\n","epoch 99/1000, step 17/22, loss 1.5139089822769165\n","epoch 99/1000, step 18/22, loss 1.5286692380905151\n","epoch 99/1000, step 19/22, loss 1.5108243227005005\n","epoch 99/1000, step 20/22, loss 1.5376451015472412\n","epoch 99/1000, step 21/22, loss 1.5172455310821533\n","epoch 99/1000, step 22/22, loss 1.507256269454956\n","epoch 100/1000, step 1/22, loss 1.524163842201233\n","epoch 100/1000, step 2/22, loss 1.525915503501892\n","epoch 100/1000, step 3/22, loss 1.5022300481796265\n","epoch 100/1000, step 4/22, loss 1.5210148096084595\n","epoch 100/1000, step 5/22, loss 1.5256589651107788\n","epoch 100/1000, step 6/22, loss 1.5116249322891235\n","epoch 100/1000, step 7/22, loss 1.5092002153396606\n","epoch 100/1000, step 8/22, loss 1.5310875177383423\n","epoch 100/1000, step 9/22, loss 1.5159803628921509\n","epoch 100/1000, step 10/22, loss 1.5338115692138672\n","epoch 100/1000, step 11/22, loss 1.523846983909607\n","epoch 100/1000, step 12/22, loss 1.5197312831878662\n","epoch 100/1000, step 13/22, loss 1.5316861867904663\n","epoch 100/1000, step 14/22, loss 1.5027438402175903\n","epoch 100/1000, step 15/22, loss 1.522246241569519\n","epoch 100/1000, step 16/22, loss 1.5125876665115356\n","epoch 100/1000, step 17/22, loss 1.513917088508606\n","epoch 100/1000, step 18/22, loss 1.5286781787872314\n","epoch 100/1000, step 19/22, loss 1.5108307600021362\n","epoch 100/1000, step 20/22, loss 1.5376540422439575\n","epoch 100/1000, step 21/22, loss 1.5172522068023682\n","epoch 100/1000, step 22/22, loss 1.5072548389434814\n","epoch 101/1000, step 1/22, loss 1.5241713523864746\n","epoch 101/1000, step 2/22, loss 1.5259240865707397\n","epoch 101/1000, step 3/22, loss 1.5022389888763428\n","epoch 101/1000, step 4/22, loss 1.521023154258728\n","epoch 101/1000, step 5/22, loss 1.5256668329238892\n","epoch 101/1000, step 6/22, loss 1.5116331577301025\n","epoch 101/1000, step 7/22, loss 1.5092092752456665\n","epoch 101/1000, step 8/22, loss 1.5310970544815063\n","epoch 101/1000, step 9/22, loss 1.5159878730773926\n","epoch 101/1000, step 10/22, loss 1.5338187217712402\n","epoch 101/1000, step 11/22, loss 1.5238556861877441\n","epoch 101/1000, step 12/22, loss 1.5197391510009766\n","epoch 101/1000, step 13/22, loss 1.5316931009292603\n","epoch 101/1000, step 14/22, loss 1.502751111984253\n","epoch 101/1000, step 15/22, loss 1.5222545862197876\n","epoch 101/1000, step 16/22, loss 1.51259446144104\n","epoch 101/1000, step 17/22, loss 1.5139243602752686\n","epoch 101/1000, step 18/22, loss 1.5286865234375\n","epoch 101/1000, step 19/22, loss 1.5108387470245361\n","epoch 101/1000, step 20/22, loss 1.537663221359253\n","epoch 101/1000, step 21/22, loss 1.517258882522583\n","epoch 101/1000, step 22/22, loss 1.5072541236877441\n","epoch 102/1000, step 1/22, loss 1.5241788625717163\n","epoch 102/1000, step 2/22, loss 1.5259305238723755\n","epoch 102/1000, step 3/22, loss 1.5022459030151367\n","epoch 102/1000, step 4/22, loss 1.521030068397522\n","epoch 102/1000, step 5/22, loss 1.5256723165512085\n","epoch 102/1000, step 6/22, loss 1.5116403102874756\n","epoch 102/1000, step 7/22, loss 1.5092166662216187\n","epoch 102/1000, step 8/22, loss 1.5311024188995361\n","epoch 102/1000, step 9/22, loss 1.515994668006897\n","epoch 102/1000, step 10/22, loss 1.5338263511657715\n","epoch 102/1000, step 11/22, loss 1.5238629579544067\n","epoch 102/1000, step 12/22, loss 1.5197452306747437\n","epoch 102/1000, step 13/22, loss 1.531698226928711\n","epoch 102/1000, step 14/22, loss 1.5027570724487305\n","epoch 102/1000, step 15/22, loss 1.522261619567871\n","epoch 102/1000, step 16/22, loss 1.5126022100448608\n","epoch 102/1000, step 17/22, loss 1.5139296054840088\n","epoch 102/1000, step 18/22, loss 1.5286940336227417\n","epoch 102/1000, step 19/22, loss 1.5108455419540405\n","epoch 102/1000, step 20/22, loss 1.5376702547073364\n","epoch 102/1000, step 21/22, loss 1.5172640085220337\n","epoch 102/1000, step 22/22, loss 1.5072509050369263\n","epoch 103/1000, step 1/22, loss 1.5241847038269043\n","epoch 103/1000, step 2/22, loss 1.5259358882904053\n","epoch 103/1000, step 3/22, loss 1.5022512674331665\n","epoch 103/1000, step 4/22, loss 1.5210351943969727\n","epoch 103/1000, step 5/22, loss 1.5256778001785278\n","epoch 103/1000, step 6/22, loss 1.5116455554962158\n","epoch 103/1000, step 7/22, loss 1.5092226266860962\n","epoch 103/1000, step 8/22, loss 1.531109094619751\n","epoch 103/1000, step 9/22, loss 1.5160006284713745\n","epoch 103/1000, step 10/22, loss 1.5338326692581177\n","epoch 103/1000, step 11/22, loss 1.5238703489303589\n","epoch 103/1000, step 12/22, loss 1.5197519063949585\n","epoch 103/1000, step 13/22, loss 1.531703233718872\n","epoch 103/1000, step 14/22, loss 1.5027616024017334\n","epoch 103/1000, step 15/22, loss 1.5222669839859009\n","epoch 103/1000, step 16/22, loss 1.5126078128814697\n","epoch 103/1000, step 17/22, loss 1.5139343738555908\n","epoch 103/1000, step 18/22, loss 1.5286989212036133\n","epoch 103/1000, step 19/22, loss 1.510850429534912\n","epoch 103/1000, step 20/22, loss 1.537676453590393\n","epoch 103/1000, step 21/22, loss 1.5172688961029053\n","epoch 103/1000, step 22/22, loss 1.5072498321533203\n","epoch 104/1000, step 1/22, loss 1.5241894721984863\n","epoch 104/1000, step 2/22, loss 1.525941014289856\n","epoch 104/1000, step 3/22, loss 1.5022552013397217\n","epoch 104/1000, step 4/22, loss 1.5210416316986084\n","epoch 104/1000, step 5/22, loss 1.5256823301315308\n","epoch 104/1000, step 6/22, loss 1.5116509199142456\n","epoch 104/1000, step 7/22, loss 1.5092272758483887\n","epoch 104/1000, step 8/22, loss 1.531114101409912\n","epoch 104/1000, step 9/22, loss 1.5160058736801147\n","epoch 104/1000, step 10/22, loss 1.5338382720947266\n","epoch 104/1000, step 11/22, loss 1.5238769054412842\n","epoch 104/1000, step 12/22, loss 1.519756555557251\n","epoch 104/1000, step 13/22, loss 1.5317081212997437\n","epoch 104/1000, step 14/22, loss 1.5027657747268677\n","epoch 104/1000, step 15/22, loss 1.5222710371017456\n","epoch 104/1000, step 16/22, loss 1.5126110315322876\n","epoch 104/1000, step 17/22, loss 1.513939380645752\n","epoch 104/1000, step 18/22, loss 1.5287038087844849\n","epoch 104/1000, step 19/22, loss 1.5108543634414673\n","epoch 104/1000, step 20/22, loss 1.5376802682876587\n","epoch 104/1000, step 21/22, loss 1.5172724723815918\n","epoch 104/1000, step 22/22, loss 1.507248878479004\n","epoch 105/1000, step 1/22, loss 1.5241950750350952\n","epoch 105/1000, step 2/22, loss 1.5259456634521484\n","epoch 105/1000, step 3/22, loss 1.5022600889205933\n","epoch 105/1000, step 4/22, loss 1.5210459232330322\n","epoch 105/1000, step 5/22, loss 1.5256853103637695\n","epoch 105/1000, step 6/22, loss 1.5116554498672485\n","epoch 105/1000, step 7/22, loss 1.5092320442199707\n","epoch 105/1000, step 8/22, loss 1.5311177968978882\n","epoch 105/1000, step 9/22, loss 1.5160086154937744\n","epoch 105/1000, step 10/22, loss 1.5338406562805176\n","epoch 105/1000, step 11/22, loss 1.5238795280456543\n","epoch 105/1000, step 12/22, loss 1.5197603702545166\n","epoch 105/1000, step 13/22, loss 1.531711220741272\n","epoch 105/1000, step 14/22, loss 1.50276780128479\n","epoch 105/1000, step 15/22, loss 1.522274136543274\n","epoch 105/1000, step 16/22, loss 1.5126138925552368\n","epoch 105/1000, step 17/22, loss 1.5139421224594116\n","epoch 105/1000, step 18/22, loss 1.5287072658538818\n","epoch 105/1000, step 19/22, loss 1.510857105255127\n","epoch 105/1000, step 20/22, loss 1.5376843214035034\n","epoch 105/1000, step 21/22, loss 1.5172737836837769\n","epoch 105/1000, step 22/22, loss 1.507245659828186\n","epoch 106/1000, step 1/22, loss 1.524198293685913\n","epoch 106/1000, step 2/22, loss 1.525948166847229\n","epoch 106/1000, step 3/22, loss 1.5022642612457275\n","epoch 106/1000, step 4/22, loss 1.521049976348877\n","epoch 106/1000, step 5/22, loss 1.5256870985031128\n","epoch 106/1000, step 6/22, loss 1.5116565227508545\n","epoch 106/1000, step 7/22, loss 1.5092344284057617\n","epoch 106/1000, step 8/22, loss 1.5311208963394165\n","epoch 106/1000, step 9/22, loss 1.516011118888855\n","epoch 106/1000, step 10/22, loss 1.5338438749313354\n","epoch 106/1000, step 11/22, loss 1.5238827466964722\n","epoch 106/1000, step 12/22, loss 1.519763708114624\n","epoch 106/1000, step 13/22, loss 1.5317133665084839\n","epoch 106/1000, step 14/22, loss 1.5027703046798706\n","epoch 106/1000, step 15/22, loss 1.522276759147644\n","epoch 106/1000, step 16/22, loss 1.5126172304153442\n","epoch 106/1000, step 17/22, loss 1.5139449834823608\n","epoch 106/1000, step 18/22, loss 1.528709053993225\n","epoch 106/1000, step 19/22, loss 1.5108590126037598\n","epoch 106/1000, step 20/22, loss 1.5376880168914795\n","epoch 106/1000, step 21/22, loss 1.5172752141952515\n","epoch 106/1000, step 22/22, loss 1.507242202758789\n","epoch 107/1000, step 1/22, loss 1.5241992473602295\n","epoch 107/1000, step 2/22, loss 1.5259501934051514\n","epoch 107/1000, step 3/22, loss 1.5022661685943604\n","epoch 107/1000, step 4/22, loss 1.5210524797439575\n","epoch 107/1000, step 5/22, loss 1.5256881713867188\n","epoch 107/1000, step 6/22, loss 1.5116599798202515\n","epoch 107/1000, step 7/22, loss 1.5092384815216064\n","epoch 107/1000, step 8/22, loss 1.5311235189437866\n","epoch 107/1000, step 9/22, loss 1.5160130262374878\n","epoch 107/1000, step 10/22, loss 1.5338486433029175\n","epoch 107/1000, step 11/22, loss 1.5238854885101318\n","epoch 107/1000, step 12/22, loss 1.5197664499282837\n","epoch 107/1000, step 13/22, loss 1.5317163467407227\n","epoch 107/1000, step 14/22, loss 1.5027730464935303\n","epoch 107/1000, step 15/22, loss 1.5222786664962769\n","epoch 107/1000, step 16/22, loss 1.5126203298568726\n","epoch 107/1000, step 17/22, loss 1.513947606086731\n","epoch 107/1000, step 18/22, loss 1.5287123918533325\n","epoch 107/1000, step 19/22, loss 1.5108616352081299\n","epoch 107/1000, step 20/22, loss 1.5376909971237183\n","epoch 107/1000, step 21/22, loss 1.5172770023345947\n","epoch 107/1000, step 22/22, loss 1.507240891456604\n","epoch 108/1000, step 1/22, loss 1.5242018699645996\n","epoch 108/1000, step 2/22, loss 1.5259521007537842\n","epoch 108/1000, step 3/22, loss 1.5022680759429932\n","epoch 108/1000, step 4/22, loss 1.5210548639297485\n","epoch 108/1000, step 5/22, loss 1.5256901979446411\n","epoch 108/1000, step 6/22, loss 1.5116630792617798\n","epoch 108/1000, step 7/22, loss 1.5092418193817139\n","epoch 108/1000, step 8/22, loss 1.531126856803894\n","epoch 108/1000, step 9/22, loss 1.5160146951675415\n","epoch 108/1000, step 10/22, loss 1.5338516235351562\n","epoch 108/1000, step 11/22, loss 1.5238882303237915\n","epoch 108/1000, step 12/22, loss 1.5197694301605225\n","epoch 108/1000, step 13/22, loss 1.5317189693450928\n","epoch 108/1000, step 14/22, loss 1.502774953842163\n","epoch 108/1000, step 15/22, loss 1.5222809314727783\n","epoch 108/1000, step 16/22, loss 1.5126227140426636\n","epoch 108/1000, step 17/22, loss 1.5139491558074951\n","epoch 108/1000, step 18/22, loss 1.5287144184112549\n","epoch 108/1000, step 19/22, loss 1.5108634233474731\n","epoch 108/1000, step 20/22, loss 1.537692666053772\n","epoch 108/1000, step 21/22, loss 1.5172786712646484\n","epoch 108/1000, step 22/22, loss 1.507238745689392\n","epoch 109/1000, step 1/22, loss 1.524204134941101\n","epoch 109/1000, step 2/22, loss 1.525954246520996\n","epoch 109/1000, step 3/22, loss 1.5022692680358887\n","epoch 109/1000, step 4/22, loss 1.5210567712783813\n","epoch 109/1000, step 5/22, loss 1.5256917476654053\n","epoch 109/1000, step 6/22, loss 1.5116652250289917\n","epoch 109/1000, step 7/22, loss 1.5092440843582153\n","epoch 109/1000, step 8/22, loss 1.5311286449432373\n","epoch 109/1000, step 9/22, loss 1.5160160064697266\n","epoch 109/1000, step 10/22, loss 1.5338542461395264\n","epoch 109/1000, step 11/22, loss 1.5238910913467407\n","epoch 109/1000, step 12/22, loss 1.5197703838348389\n","epoch 109/1000, step 13/22, loss 1.531721591949463\n","epoch 109/1000, step 14/22, loss 1.5027769804000854\n","epoch 109/1000, step 15/22, loss 1.5222822427749634\n","epoch 109/1000, step 16/22, loss 1.5126245021820068\n","epoch 109/1000, step 17/22, loss 1.5139509439468384\n","epoch 109/1000, step 18/22, loss 1.5287163257598877\n","epoch 109/1000, step 19/22, loss 1.5108654499053955\n","epoch 109/1000, step 20/22, loss 1.5376944541931152\n","epoch 109/1000, step 21/22, loss 1.5172793865203857\n","epoch 109/1000, step 22/22, loss 1.5072383880615234\n","epoch 110/1000, step 1/22, loss 1.5242056846618652\n","epoch 110/1000, step 2/22, loss 1.525956392288208\n","epoch 110/1000, step 3/22, loss 1.5022706985473633\n","epoch 110/1000, step 4/22, loss 1.5210585594177246\n","epoch 110/1000, step 5/22, loss 1.525693655014038\n","epoch 110/1000, step 6/22, loss 1.511667013168335\n","epoch 110/1000, step 7/22, loss 1.5092459917068481\n","epoch 110/1000, step 8/22, loss 1.5311310291290283\n","epoch 110/1000, step 9/22, loss 1.5160175561904907\n","epoch 110/1000, step 10/22, loss 1.5338551998138428\n","epoch 110/1000, step 11/22, loss 1.523892879486084\n","epoch 110/1000, step 12/22, loss 1.5197724103927612\n","epoch 110/1000, step 13/22, loss 1.531724214553833\n","epoch 110/1000, step 14/22, loss 1.5027782917022705\n","epoch 110/1000, step 15/22, loss 1.5222830772399902\n","epoch 110/1000, step 16/22, loss 1.51262629032135\n","epoch 110/1000, step 17/22, loss 1.5139514207839966\n","epoch 110/1000, step 18/22, loss 1.5287182331085205\n","epoch 110/1000, step 19/22, loss 1.5108660459518433\n","epoch 110/1000, step 20/22, loss 1.5376960039138794\n","epoch 110/1000, step 21/22, loss 1.517280101776123\n","epoch 110/1000, step 22/22, loss 1.5072364807128906\n","epoch 111/1000, step 1/22, loss 1.5242068767547607\n","epoch 111/1000, step 2/22, loss 1.5259572267532349\n","epoch 111/1000, step 3/22, loss 1.5022718906402588\n","epoch 111/1000, step 4/22, loss 1.521060824394226\n","epoch 111/1000, step 5/22, loss 1.5256942510604858\n","epoch 111/1000, step 6/22, loss 1.51166832447052\n","epoch 111/1000, step 7/22, loss 1.5092475414276123\n","epoch 111/1000, step 8/22, loss 1.5311321020126343\n","epoch 111/1000, step 9/22, loss 1.5160186290740967\n","epoch 111/1000, step 10/22, loss 1.5338565111160278\n","epoch 111/1000, step 11/22, loss 1.5238947868347168\n","epoch 111/1000, step 12/22, loss 1.519774317741394\n","epoch 111/1000, step 13/22, loss 1.5317257642745972\n","epoch 111/1000, step 14/22, loss 1.5027786493301392\n","epoch 111/1000, step 15/22, loss 1.5222834348678589\n","epoch 111/1000, step 16/22, loss 1.5126270055770874\n","epoch 111/1000, step 17/22, loss 1.513953685760498\n","epoch 111/1000, step 18/22, loss 1.528719425201416\n","epoch 111/1000, step 19/22, loss 1.5108673572540283\n","epoch 111/1000, step 20/22, loss 1.5376970767974854\n","epoch 111/1000, step 21/22, loss 1.5172802209854126\n","epoch 111/1000, step 22/22, loss 1.5072356462478638\n","epoch 112/1000, step 1/22, loss 1.5242079496383667\n","epoch 112/1000, step 2/22, loss 1.5259582996368408\n","epoch 112/1000, step 3/22, loss 1.5022730827331543\n","epoch 112/1000, step 4/22, loss 1.5210621356964111\n","epoch 112/1000, step 5/22, loss 1.5256954431533813\n","epoch 112/1000, step 6/22, loss 1.5116692781448364\n","epoch 112/1000, step 7/22, loss 1.509249210357666\n","epoch 112/1000, step 8/22, loss 1.5311336517333984\n","epoch 112/1000, step 9/22, loss 1.5160191059112549\n","epoch 112/1000, step 10/22, loss 1.533857822418213\n","epoch 112/1000, step 11/22, loss 1.523896336555481\n","epoch 112/1000, step 12/22, loss 1.5197758674621582\n","epoch 112/1000, step 13/22, loss 1.5317273139953613\n","epoch 112/1000, step 14/22, loss 1.5027801990509033\n","epoch 112/1000, step 15/22, loss 1.5222834348678589\n","epoch 112/1000, step 16/22, loss 1.5126278400421143\n","epoch 112/1000, step 17/22, loss 1.5139344930648804\n","epoch 112/1000, step 18/22, loss 1.528696060180664\n","epoch 112/1000, step 19/22, loss 1.5108476877212524\n","epoch 112/1000, step 20/22, loss 1.5376794338226318\n","epoch 112/1000, step 21/22, loss 1.5172597169876099\n","epoch 112/1000, step 22/22, loss 1.5071383714675903\n","epoch 113/1000, step 1/22, loss 1.5241872072219849\n","epoch 113/1000, step 2/22, loss 1.5259383916854858\n","epoch 113/1000, step 3/22, loss 1.502255916595459\n","epoch 113/1000, step 4/22, loss 1.521041989326477\n","epoch 113/1000, step 5/22, loss 1.5256738662719727\n","epoch 113/1000, step 6/22, loss 1.5116517543792725\n","epoch 113/1000, step 7/22, loss 1.5092284679412842\n","epoch 113/1000, step 8/22, loss 1.5311135053634644\n","epoch 113/1000, step 9/22, loss 1.5160011053085327\n","epoch 113/1000, step 10/22, loss 1.5338422060012817\n","epoch 113/1000, step 11/22, loss 1.5238765478134155\n","epoch 113/1000, step 12/22, loss 1.5197598934173584\n","epoch 113/1000, step 13/22, loss 1.5317082405090332\n","epoch 113/1000, step 14/22, loss 1.502759337425232\n","epoch 113/1000, step 15/22, loss 1.5222607851028442\n","epoch 113/1000, step 16/22, loss 1.5126067399978638\n","epoch 113/1000, step 17/22, loss 1.5139349699020386\n","epoch 113/1000, step 18/22, loss 1.5286967754364014\n","epoch 113/1000, step 19/22, loss 1.5108487606048584\n","epoch 113/1000, step 20/22, loss 1.5376808643341064\n","epoch 113/1000, step 21/22, loss 1.5172600746154785\n","epoch 113/1000, step 22/22, loss 1.507137656211853\n","epoch 114/1000, step 1/22, loss 1.524187684059143\n","epoch 114/1000, step 2/22, loss 1.5259395837783813\n","epoch 114/1000, step 3/22, loss 1.5022567510604858\n","epoch 114/1000, step 4/22, loss 1.521043300628662\n","epoch 114/1000, step 5/22, loss 1.52567458152771\n","epoch 114/1000, step 6/22, loss 1.5116521120071411\n","epoch 114/1000, step 7/22, loss 1.5092294216156006\n","epoch 114/1000, step 8/22, loss 1.5311144590377808\n","epoch 114/1000, step 9/22, loss 1.5160024166107178\n","epoch 114/1000, step 10/22, loss 1.5338431596755981\n","epoch 114/1000, step 11/22, loss 1.5238771438598633\n","epoch 114/1000, step 12/22, loss 1.5197603702545166\n","epoch 114/1000, step 13/22, loss 1.5317089557647705\n","epoch 114/1000, step 14/22, loss 1.5027598142623901\n","epoch 114/1000, step 15/22, loss 1.522261142730713\n","epoch 114/1000, step 16/22, loss 1.512607455253601\n","epoch 114/1000, step 17/22, loss 1.5139362812042236\n","epoch 114/1000, step 18/22, loss 1.52869713306427\n","epoch 114/1000, step 19/22, loss 1.5108492374420166\n","epoch 114/1000, step 20/22, loss 1.537681221961975\n","epoch 114/1000, step 21/22, loss 1.517260193824768\n","epoch 114/1000, step 22/22, loss 1.5071364641189575\n","epoch 115/1000, step 1/22, loss 1.5241886377334595\n","epoch 115/1000, step 2/22, loss 1.5259402990341187\n","epoch 115/1000, step 3/22, loss 1.5022568702697754\n","epoch 115/1000, step 4/22, loss 1.5210438966751099\n","epoch 115/1000, step 5/22, loss 1.5256747007369995\n","epoch 115/1000, step 6/22, loss 1.5116527080535889\n","epoch 115/1000, step 7/22, loss 1.509230375289917\n","epoch 115/1000, step 8/22, loss 1.5311150550842285\n","epoch 115/1000, step 9/22, loss 1.5160021781921387\n","epoch 115/1000, step 10/22, loss 1.5338412523269653\n","epoch 115/1000, step 11/22, loss 1.5238755941390991\n","epoch 115/1000, step 12/22, loss 1.5197598934173584\n","epoch 115/1000, step 13/22, loss 1.5317069292068481\n","epoch 115/1000, step 14/22, loss 1.502759337425232\n","epoch 115/1000, step 15/22, loss 1.5222598314285278\n","epoch 115/1000, step 16/22, loss 1.5126060247421265\n","epoch 115/1000, step 17/22, loss 1.5139353275299072\n","epoch 115/1000, step 18/22, loss 1.5286955833435059\n","epoch 115/1000, step 19/22, loss 1.5108485221862793\n","epoch 115/1000, step 20/22, loss 1.5376794338226318\n","epoch 115/1000, step 21/22, loss 1.517259120941162\n","epoch 115/1000, step 22/22, loss 1.5071359872817993\n","epoch 116/1000, step 1/22, loss 1.5241869688034058\n","epoch 116/1000, step 2/22, loss 1.5259391069412231\n","epoch 116/1000, step 3/22, loss 1.502256155014038\n","epoch 116/1000, step 4/22, loss 1.521043062210083\n","epoch 116/1000, step 5/22, loss 1.5256729125976562\n","epoch 116/1000, step 6/22, loss 1.511651635169983\n","epoch 116/1000, step 7/22, loss 1.5092295408248901\n","epoch 116/1000, step 8/22, loss 1.5311139822006226\n","epoch 116/1000, step 9/22, loss 1.5160013437271118\n","epoch 116/1000, step 10/22, loss 1.533841848373413\n","epoch 116/1000, step 11/22, loss 1.523876428604126\n","epoch 116/1000, step 12/22, loss 1.5197606086730957\n","epoch 116/1000, step 13/22, loss 1.5317072868347168\n","epoch 116/1000, step 14/22, loss 1.5027596950531006\n","epoch 116/1000, step 15/22, loss 1.522260069847107\n","epoch 116/1000, step 16/22, loss 1.5126063823699951\n","epoch 116/1000, step 17/22, loss 1.513935923576355\n","epoch 116/1000, step 18/22, loss 1.5286959409713745\n","epoch 116/1000, step 19/22, loss 1.510849118232727\n","epoch 116/1000, step 20/22, loss 1.537679672241211\n","epoch 116/1000, step 21/22, loss 1.5172594785690308\n","epoch 116/1000, step 22/22, loss 1.5071353912353516\n","epoch 117/1000, step 1/22, loss 1.524187684059143\n","epoch 117/1000, step 2/22, loss 1.525938868522644\n","epoch 117/1000, step 3/22, loss 1.502255916595459\n","epoch 117/1000, step 4/22, loss 1.5210435390472412\n","epoch 117/1000, step 5/22, loss 1.525673270225525\n","epoch 117/1000, step 6/22, loss 1.5116519927978516\n","epoch 117/1000, step 7/22, loss 1.5092294216156006\n","epoch 117/1000, step 8/22, loss 1.5311135053634644\n","epoch 117/1000, step 9/22, loss 1.5160009860992432\n","epoch 117/1000, step 10/22, loss 1.5338422060012817\n","epoch 117/1000, step 11/22, loss 1.5238765478134155\n","epoch 117/1000, step 12/22, loss 1.5197609663009644\n","epoch 117/1000, step 13/22, loss 1.531707763671875\n","epoch 117/1000, step 14/22, loss 1.5027596950531006\n","epoch 117/1000, step 15/22, loss 1.5222597122192383\n","epoch 117/1000, step 16/22, loss 1.5126063823699951\n","epoch 117/1000, step 17/22, loss 1.513935923576355\n","epoch 117/1000, step 18/22, loss 1.528696060180664\n","epoch 117/1000, step 19/22, loss 1.5108492374420166\n","epoch 117/1000, step 20/22, loss 1.5376797914505005\n","epoch 117/1000, step 21/22, loss 1.5172594785690308\n","epoch 117/1000, step 22/22, loss 1.5071333646774292\n","epoch 118/1000, step 1/22, loss 1.5241875648498535\n","epoch 118/1000, step 2/22, loss 1.5259391069412231\n","epoch 118/1000, step 3/22, loss 1.5022556781768799\n","epoch 118/1000, step 4/22, loss 1.5210447311401367\n","epoch 118/1000, step 5/22, loss 1.5256736278533936\n","epoch 118/1000, step 6/22, loss 1.5116521120071411\n","epoch 118/1000, step 7/22, loss 1.5092297792434692\n","epoch 118/1000, step 8/22, loss 1.531113862991333\n","epoch 118/1000, step 9/22, loss 1.5160013437271118\n","epoch 118/1000, step 10/22, loss 1.5338428020477295\n","epoch 118/1000, step 11/22, loss 1.5238769054412842\n","epoch 118/1000, step 12/22, loss 1.519761323928833\n","epoch 118/1000, step 13/22, loss 1.5317082405090332\n","epoch 118/1000, step 14/22, loss 1.5027598142623901\n","epoch 118/1000, step 15/22, loss 1.5222598314285278\n","epoch 118/1000, step 16/22, loss 1.5126063823699951\n","epoch 118/1000, step 17/22, loss 1.5139364004135132\n","epoch 118/1000, step 18/22, loss 1.5286962985992432\n","epoch 118/1000, step 19/22, loss 1.5108492374420166\n","epoch 118/1000, step 20/22, loss 1.5376802682876587\n","epoch 118/1000, step 21/22, loss 1.5172594785690308\n","epoch 118/1000, step 22/22, loss 1.507132649421692\n","epoch 119/1000, step 1/22, loss 1.5241880416870117\n","epoch 119/1000, step 2/22, loss 1.5259394645690918\n","epoch 119/1000, step 3/22, loss 1.5022563934326172\n","epoch 119/1000, step 4/22, loss 1.5210449695587158\n","epoch 119/1000, step 5/22, loss 1.5256742238998413\n","epoch 119/1000, step 6/22, loss 1.5116523504257202\n","epoch 119/1000, step 7/22, loss 1.5092295408248901\n","epoch 119/1000, step 8/22, loss 1.531114101409912\n","epoch 119/1000, step 9/22, loss 1.5160017013549805\n","epoch 119/1000, step 10/22, loss 1.5338432788848877\n","epoch 119/1000, step 11/22, loss 1.5238776206970215\n","epoch 119/1000, step 12/22, loss 1.5197618007659912\n","epoch 119/1000, step 13/22, loss 1.5317081212997437\n","epoch 119/1000, step 14/22, loss 1.5027601718902588\n","epoch 119/1000, step 15/22, loss 1.5222598314285278\n","epoch 119/1000, step 16/22, loss 1.5126063823699951\n","epoch 119/1000, step 17/22, loss 1.5139366388320923\n","epoch 119/1000, step 18/22, loss 1.5286962985992432\n","epoch 119/1000, step 19/22, loss 1.5108494758605957\n","epoch 119/1000, step 20/22, loss 1.5376806259155273\n","epoch 119/1000, step 21/22, loss 1.5172594785690308\n","epoch 119/1000, step 22/22, loss 1.5071316957473755\n","epoch 120/1000, step 1/22, loss 1.5241883993148804\n","epoch 120/1000, step 2/22, loss 1.5259392261505127\n","epoch 120/1000, step 3/22, loss 1.5022565126419067\n","epoch 120/1000, step 4/22, loss 1.5210450887680054\n","epoch 120/1000, step 5/22, loss 1.5256742238998413\n","epoch 120/1000, step 6/22, loss 1.5116523504257202\n","epoch 120/1000, step 7/22, loss 1.509230136871338\n","epoch 120/1000, step 8/22, loss 1.5311143398284912\n","epoch 120/1000, step 9/22, loss 1.5160014629364014\n","epoch 120/1000, step 10/22, loss 1.5338435173034668\n","epoch 120/1000, step 11/22, loss 1.5238779783248901\n","epoch 120/1000, step 12/22, loss 1.519761562347412\n","epoch 120/1000, step 13/22, loss 1.5317084789276123\n","epoch 120/1000, step 14/22, loss 1.5027605295181274\n","epoch 120/1000, step 15/22, loss 1.5222601890563965\n","epoch 120/1000, step 16/22, loss 1.512606143951416\n","epoch 120/1000, step 17/22, loss 1.5139367580413818\n","epoch 120/1000, step 18/22, loss 1.5286966562271118\n","epoch 120/1000, step 19/22, loss 1.5108494758605957\n","epoch 120/1000, step 20/22, loss 1.5376806259155273\n","epoch 120/1000, step 21/22, loss 1.517259120941162\n","epoch 120/1000, step 22/22, loss 1.5071306228637695\n","epoch 121/1000, step 1/22, loss 1.5241882801055908\n","epoch 121/1000, step 2/22, loss 1.525938868522644\n","epoch 121/1000, step 3/22, loss 1.5022563934326172\n","epoch 121/1000, step 4/22, loss 1.5210447311401367\n","epoch 121/1000, step 5/22, loss 1.5256743431091309\n","epoch 121/1000, step 6/22, loss 1.5116521120071411\n","epoch 121/1000, step 7/22, loss 1.509230136871338\n","epoch 121/1000, step 8/22, loss 1.5311139822006226\n","epoch 121/1000, step 9/22, loss 1.5160014629364014\n","epoch 121/1000, step 10/22, loss 1.5338432788848877\n","epoch 121/1000, step 11/22, loss 1.5238779783248901\n","epoch 121/1000, step 12/22, loss 1.5197618007659912\n","epoch 121/1000, step 13/22, loss 1.5317082405090332\n","epoch 121/1000, step 14/22, loss 1.5027605295181274\n","epoch 121/1000, step 15/22, loss 1.5222597122192383\n","epoch 121/1000, step 16/22, loss 1.5126063823699951\n","epoch 121/1000, step 17/22, loss 1.5139367580413818\n","epoch 121/1000, step 18/22, loss 1.5286966562271118\n","epoch 121/1000, step 19/22, loss 1.5108495950698853\n","epoch 121/1000, step 20/22, loss 1.5376806259155273\n","epoch 121/1000, step 21/22, loss 1.5172590017318726\n","epoch 121/1000, step 22/22, loss 1.5071300268173218\n","epoch 122/1000, step 1/22, loss 1.5241879224777222\n","epoch 122/1000, step 2/22, loss 1.5259383916854858\n","epoch 122/1000, step 3/22, loss 1.5022567510604858\n","epoch 122/1000, step 4/22, loss 1.5210450887680054\n","epoch 122/1000, step 5/22, loss 1.52567458152771\n","epoch 122/1000, step 6/22, loss 1.5116506814956665\n","epoch 122/1000, step 7/22, loss 1.5092288255691528\n","epoch 122/1000, step 8/22, loss 1.5311131477355957\n","epoch 122/1000, step 9/22, loss 1.5160006284713745\n","epoch 122/1000, step 10/22, loss 1.533841848373413\n","epoch 122/1000, step 11/22, loss 1.5238769054412842\n","epoch 122/1000, step 12/22, loss 1.5197606086730957\n","epoch 122/1000, step 13/22, loss 1.5317070484161377\n","epoch 122/1000, step 14/22, loss 1.5027587413787842\n","epoch 122/1000, step 15/22, loss 1.5222588777542114\n","epoch 122/1000, step 16/22, loss 1.5126049518585205\n","epoch 122/1000, step 17/22, loss 1.5139353275299072\n","epoch 122/1000, step 18/22, loss 1.528696060180664\n","epoch 122/1000, step 19/22, loss 1.5108481645584106\n","epoch 122/1000, step 20/22, loss 1.5376797914505005\n","epoch 122/1000, step 21/22, loss 1.517257571220398\n","epoch 122/1000, step 22/22, loss 1.5071269273757935\n","epoch 123/1000, step 1/22, loss 1.5241872072219849\n","epoch 123/1000, step 2/22, loss 1.5259376764297485\n","epoch 123/1000, step 3/22, loss 1.5022560358047485\n","epoch 123/1000, step 4/22, loss 1.5210440158843994\n","epoch 123/1000, step 5/22, loss 1.5256738662719727\n","epoch 123/1000, step 6/22, loss 1.5116509199142456\n","epoch 123/1000, step 7/22, loss 1.5092287063598633\n","epoch 123/1000, step 8/22, loss 1.5311131477355957\n","epoch 123/1000, step 9/22, loss 1.516000509262085\n","epoch 123/1000, step 10/22, loss 1.533841848373413\n","epoch 123/1000, step 11/22, loss 1.5238769054412842\n","epoch 123/1000, step 12/22, loss 1.5197607278823853\n","epoch 123/1000, step 13/22, loss 1.5317070484161377\n","epoch 123/1000, step 14/22, loss 1.5027589797973633\n","epoch 123/1000, step 15/22, loss 1.522258996963501\n","epoch 123/1000, step 16/22, loss 1.5126045942306519\n","epoch 123/1000, step 17/22, loss 1.5139352083206177\n","epoch 123/1000, step 18/22, loss 1.5286959409713745\n","epoch 123/1000, step 19/22, loss 1.5108481645584106\n","epoch 123/1000, step 20/22, loss 1.5376794338226318\n","epoch 123/1000, step 21/22, loss 1.517257571220398\n","epoch 123/1000, step 22/22, loss 1.507125973701477\n","epoch 124/1000, step 1/22, loss 1.5241869688034058\n","epoch 124/1000, step 2/22, loss 1.5259376764297485\n","epoch 124/1000, step 3/22, loss 1.502256155014038\n","epoch 124/1000, step 4/22, loss 1.521044373512268\n","epoch 124/1000, step 5/22, loss 1.5256739854812622\n","epoch 124/1000, step 6/22, loss 1.5116506814956665\n","epoch 124/1000, step 7/22, loss 1.5092287063598633\n","epoch 124/1000, step 8/22, loss 1.5311131477355957\n","epoch 124/1000, step 9/22, loss 1.5160002708435059\n","epoch 124/1000, step 10/22, loss 1.533841848373413\n","epoch 124/1000, step 11/22, loss 1.5238767862319946\n","epoch 124/1000, step 12/22, loss 1.5197607278823853\n","epoch 124/1000, step 13/22, loss 1.5317069292068481\n","epoch 124/1000, step 14/22, loss 1.5027586221694946\n","epoch 124/1000, step 15/22, loss 1.5222591161727905\n","epoch 124/1000, step 16/22, loss 1.5126049518585205\n","epoch 124/1000, step 17/22, loss 1.5139353275299072\n","epoch 124/1000, step 18/22, loss 1.528696060180664\n","epoch 124/1000, step 19/22, loss 1.510848045349121\n","epoch 124/1000, step 20/22, loss 1.5376797914505005\n","epoch 124/1000, step 21/22, loss 1.517257809638977\n","epoch 124/1000, step 22/22, loss 1.5071256160736084\n","epoch 125/1000, step 1/22, loss 1.5241869688034058\n","epoch 125/1000, step 2/22, loss 1.525937557220459\n","epoch 125/1000, step 3/22, loss 1.502256155014038\n","epoch 125/1000, step 4/22, loss 1.5210440158843994\n","epoch 125/1000, step 5/22, loss 1.5256742238998413\n","epoch 125/1000, step 6/22, loss 1.5116504430770874\n","epoch 125/1000, step 7/22, loss 1.5092283487319946\n","epoch 125/1000, step 8/22, loss 1.5311131477355957\n","epoch 125/1000, step 9/22, loss 1.5160002708435059\n","epoch 125/1000, step 10/22, loss 1.533841609954834\n","epoch 125/1000, step 11/22, loss 1.5238769054412842\n","epoch 125/1000, step 12/22, loss 1.5197606086730957\n","epoch 125/1000, step 13/22, loss 1.5317065715789795\n","epoch 125/1000, step 14/22, loss 1.5027586221694946\n","epoch 125/1000, step 15/22, loss 1.5222588777542114\n","epoch 125/1000, step 16/22, loss 1.512604832649231\n","epoch 125/1000, step 17/22, loss 1.5139349699020386\n","epoch 125/1000, step 18/22, loss 1.5286959409713745\n","epoch 125/1000, step 19/22, loss 1.5108476877212524\n","epoch 125/1000, step 20/22, loss 1.537679672241211\n","epoch 125/1000, step 21/22, loss 1.517257809638977\n","epoch 125/1000, step 22/22, loss 1.507124423980713\n","epoch 126/1000, step 1/22, loss 1.5241868495941162\n","epoch 126/1000, step 2/22, loss 1.525937557220459\n","epoch 126/1000, step 3/22, loss 1.5022563934326172\n","epoch 126/1000, step 4/22, loss 1.5210440158843994\n","epoch 126/1000, step 5/22, loss 1.5256742238998413\n","epoch 126/1000, step 6/22, loss 1.5116504430770874\n","epoch 126/1000, step 7/22, loss 1.5092283487319946\n","epoch 126/1000, step 8/22, loss 1.5311132669448853\n","epoch 126/1000, step 9/22, loss 1.5160002708435059\n","epoch 126/1000, step 10/22, loss 1.5338410139083862\n","epoch 126/1000, step 11/22, loss 1.5238771438598633\n","epoch 126/1000, step 12/22, loss 1.5197606086730957\n","epoch 126/1000, step 13/22, loss 1.5317065715789795\n","epoch 126/1000, step 14/22, loss 1.5027583837509155\n","epoch 126/1000, step 15/22, loss 1.5222588777542114\n","epoch 126/1000, step 16/22, loss 1.5126049518585205\n","epoch 126/1000, step 17/22, loss 1.5139349699020386\n","epoch 126/1000, step 18/22, loss 1.5286952257156372\n","epoch 126/1000, step 19/22, loss 1.5108466148376465\n","epoch 126/1000, step 20/22, loss 1.5376787185668945\n","epoch 126/1000, step 21/22, loss 1.517256736755371\n","epoch 126/1000, step 22/22, loss 1.5071215629577637\n","epoch 127/1000, step 1/22, loss 1.524186134338379\n","epoch 127/1000, step 2/22, loss 1.5259368419647217\n","epoch 127/1000, step 3/22, loss 1.5022556781768799\n","epoch 127/1000, step 4/22, loss 1.521043300628662\n","epoch 127/1000, step 5/22, loss 1.525673508644104\n","epoch 127/1000, step 6/22, loss 1.5116491317749023\n","epoch 127/1000, step 7/22, loss 1.5092272758483887\n","epoch 127/1000, step 8/22, loss 1.5311121940612793\n","epoch 127/1000, step 9/22, loss 1.515999436378479\n","epoch 127/1000, step 10/22, loss 1.533840537071228\n","epoch 127/1000, step 11/22, loss 1.5238763093948364\n","epoch 127/1000, step 12/22, loss 1.519760012626648\n","epoch 127/1000, step 13/22, loss 1.5317058563232422\n","epoch 127/1000, step 14/22, loss 1.5027579069137573\n","epoch 127/1000, step 15/22, loss 1.522257924079895\n","epoch 127/1000, step 16/22, loss 1.5126041173934937\n","epoch 127/1000, step 17/22, loss 1.5139343738555908\n","epoch 127/1000, step 18/22, loss 1.5286948680877686\n","epoch 127/1000, step 19/22, loss 1.510846495628357\n","epoch 127/1000, step 20/22, loss 1.5376787185668945\n","epoch 127/1000, step 21/22, loss 1.517256498336792\n","epoch 127/1000, step 22/22, loss 1.5071210861206055\n","epoch 128/1000, step 1/22, loss 1.5241860151290894\n","epoch 128/1000, step 2/22, loss 1.5259366035461426\n","epoch 128/1000, step 3/22, loss 1.5022552013397217\n","epoch 128/1000, step 4/22, loss 1.521043062210083\n","epoch 128/1000, step 5/22, loss 1.5256731510162354\n","epoch 128/1000, step 6/22, loss 1.5116487741470337\n","epoch 128/1000, step 7/22, loss 1.50922691822052\n","epoch 128/1000, step 8/22, loss 1.531111717224121\n","epoch 128/1000, step 9/22, loss 1.5159990787506104\n","epoch 128/1000, step 10/22, loss 1.5338401794433594\n","epoch 128/1000, step 11/22, loss 1.5238760709762573\n","epoch 128/1000, step 12/22, loss 1.5197595357894897\n","epoch 128/1000, step 13/22, loss 1.531705617904663\n","epoch 128/1000, step 14/22, loss 1.5027577877044678\n","epoch 128/1000, step 15/22, loss 1.5222574472427368\n","epoch 128/1000, step 16/22, loss 1.5126038789749146\n","epoch 128/1000, step 17/22, loss 1.5139340162277222\n","epoch 128/1000, step 18/22, loss 1.5286943912506104\n","epoch 128/1000, step 19/22, loss 1.5108462572097778\n","epoch 128/1000, step 20/22, loss 1.537678599357605\n","epoch 128/1000, step 21/22, loss 1.517256736755371\n","epoch 128/1000, step 22/22, loss 1.5071189403533936\n","epoch 129/1000, step 1/22, loss 1.5241856575012207\n","epoch 129/1000, step 2/22, loss 1.5259368419647217\n","epoch 129/1000, step 3/22, loss 1.5022552013397217\n","epoch 129/1000, step 4/22, loss 1.521043062210083\n","epoch 129/1000, step 5/22, loss 1.5256729125976562\n","epoch 129/1000, step 6/22, loss 1.5116486549377441\n","epoch 129/1000, step 7/22, loss 1.5092267990112305\n","epoch 129/1000, step 8/22, loss 1.531111717224121\n","epoch 129/1000, step 9/22, loss 1.5159990787506104\n","epoch 129/1000, step 10/22, loss 1.5338401794433594\n","epoch 129/1000, step 11/22, loss 1.5238759517669678\n","epoch 129/1000, step 12/22, loss 1.5197596549987793\n","epoch 129/1000, step 13/22, loss 1.5317051410675049\n","epoch 129/1000, step 14/22, loss 1.5027574300765991\n","epoch 129/1000, step 15/22, loss 1.5222574472427368\n","epoch 129/1000, step 16/22, loss 1.512603759765625\n","epoch 129/1000, step 17/22, loss 1.5139340162277222\n","epoch 129/1000, step 18/22, loss 1.5286941528320312\n","epoch 129/1000, step 19/22, loss 1.5108461380004883\n","epoch 129/1000, step 20/22, loss 1.5376787185668945\n","epoch 129/1000, step 21/22, loss 1.517256736755371\n","epoch 129/1000, step 22/22, loss 1.5071182250976562\n","epoch 130/1000, step 1/22, loss 1.5241857767105103\n","epoch 130/1000, step 2/22, loss 1.525936484336853\n","epoch 130/1000, step 3/22, loss 1.5022552013397217\n","epoch 130/1000, step 4/22, loss 1.521043062210083\n","epoch 130/1000, step 5/22, loss 1.5256729125976562\n","epoch 130/1000, step 6/22, loss 1.5116486549377441\n","epoch 130/1000, step 7/22, loss 1.5092265605926514\n","epoch 130/1000, step 8/22, loss 1.5311118364334106\n","epoch 130/1000, step 9/22, loss 1.5159987211227417\n","epoch 130/1000, step 10/22, loss 1.5338398218154907\n","epoch 130/1000, step 11/22, loss 1.5238755941390991\n","epoch 130/1000, step 12/22, loss 1.5197592973709106\n","epoch 130/1000, step 13/22, loss 1.531705379486084\n","epoch 130/1000, step 14/22, loss 1.5027570724487305\n","epoch 130/1000, step 15/22, loss 1.5222574472427368\n","epoch 130/1000, step 16/22, loss 1.5126034021377563\n","epoch 130/1000, step 17/22, loss 1.5139336585998535\n","epoch 130/1000, step 18/22, loss 1.5286941528320312\n","epoch 130/1000, step 19/22, loss 1.5108458995819092\n","epoch 130/1000, step 20/22, loss 1.5376787185668945\n","epoch 130/1000, step 21/22, loss 1.517256736755371\n","epoch 130/1000, step 22/22, loss 1.507117509841919\n","epoch 131/1000, step 1/22, loss 1.5241856575012207\n","epoch 131/1000, step 2/22, loss 1.5259361267089844\n","epoch 131/1000, step 3/22, loss 1.5022552013397217\n","epoch 131/1000, step 4/22, loss 1.5210424661636353\n","epoch 131/1000, step 5/22, loss 1.5256723165512085\n","epoch 131/1000, step 6/22, loss 1.5116486549377441\n","epoch 131/1000, step 7/22, loss 1.5092264413833618\n","epoch 131/1000, step 8/22, loss 1.5311118364334106\n","epoch 131/1000, step 9/22, loss 1.5159987211227417\n","epoch 131/1000, step 10/22, loss 1.5338395833969116\n","epoch 131/1000, step 11/22, loss 1.5238755941390991\n","epoch 131/1000, step 12/22, loss 1.5197588205337524\n","epoch 131/1000, step 13/22, loss 1.5317050218582153\n","epoch 131/1000, step 14/22, loss 1.5027568340301514\n","epoch 131/1000, step 15/22, loss 1.5222570896148682\n","epoch 131/1000, step 16/22, loss 1.5126032829284668\n","epoch 131/1000, step 17/22, loss 1.5139333009719849\n","epoch 131/1000, step 18/22, loss 1.5286937952041626\n","epoch 131/1000, step 19/22, loss 1.5108457803726196\n","epoch 131/1000, step 20/22, loss 1.5376783609390259\n","epoch 131/1000, step 21/22, loss 1.5172563791275024\n","epoch 131/1000, step 22/22, loss 1.5071161985397339\n","epoch 132/1000, step 1/22, loss 1.5241854190826416\n","epoch 132/1000, step 2/22, loss 1.5259358882904053\n","epoch 132/1000, step 3/22, loss 1.502254843711853\n","epoch 132/1000, step 4/22, loss 1.5210423469543457\n","epoch 132/1000, step 5/22, loss 1.5256719589233398\n","epoch 132/1000, step 6/22, loss 1.5116482973098755\n","epoch 132/1000, step 7/22, loss 1.5092262029647827\n","epoch 132/1000, step 8/22, loss 1.531111717224121\n","epoch 132/1000, step 9/22, loss 1.5159984827041626\n","epoch 132/1000, step 10/22, loss 1.5338395833969116\n","epoch 132/1000, step 11/22, loss 1.5238752365112305\n","epoch 132/1000, step 12/22, loss 1.5197588205337524\n","epoch 132/1000, step 13/22, loss 1.5317047834396362\n","epoch 132/1000, step 14/22, loss 1.5027567148208618\n","epoch 132/1000, step 15/22, loss 1.5222570896148682\n","epoch 132/1000, step 16/22, loss 1.5126032829284668\n","epoch 132/1000, step 17/22, loss 1.5139330625534058\n","epoch 132/1000, step 18/22, loss 1.528693675994873\n","epoch 132/1000, step 19/22, loss 1.510845422744751\n","epoch 132/1000, step 20/22, loss 1.5376782417297363\n","epoch 132/1000, step 21/22, loss 1.5172563791275024\n","epoch 132/1000, step 22/22, loss 1.5071158409118652\n","epoch 133/1000, step 1/22, loss 1.5241856575012207\n","epoch 133/1000, step 2/22, loss 1.5259358882904053\n","epoch 133/1000, step 3/22, loss 1.502254843711853\n","epoch 133/1000, step 4/22, loss 1.5210421085357666\n","epoch 133/1000, step 5/22, loss 1.5256716012954712\n","epoch 133/1000, step 6/22, loss 1.5116482973098755\n","epoch 133/1000, step 7/22, loss 1.5092262029647827\n","epoch 133/1000, step 8/22, loss 1.531111717224121\n","epoch 133/1000, step 9/22, loss 1.515998363494873\n","epoch 133/1000, step 10/22, loss 1.5338395833969116\n","epoch 133/1000, step 11/22, loss 1.5238748788833618\n","epoch 133/1000, step 12/22, loss 1.5197584629058838\n","epoch 133/1000, step 13/22, loss 1.5317044258117676\n","epoch 133/1000, step 14/22, loss 1.5027564764022827\n","epoch 133/1000, step 15/22, loss 1.5222567319869995\n","epoch 133/1000, step 16/22, loss 1.512602686882019\n","epoch 133/1000, step 17/22, loss 1.513932704925537\n","epoch 133/1000, step 18/22, loss 1.5286931991577148\n","epoch 133/1000, step 19/22, loss 1.5108450651168823\n","epoch 133/1000, step 20/22, loss 1.5376778841018677\n","epoch 133/1000, step 21/22, loss 1.5172559022903442\n","epoch 133/1000, step 22/22, loss 1.5071145296096802\n","epoch 134/1000, step 1/22, loss 1.524185061454773\n","epoch 134/1000, step 2/22, loss 1.5259355306625366\n","epoch 134/1000, step 3/22, loss 1.5022542476654053\n","epoch 134/1000, step 4/22, loss 1.521041750907898\n","epoch 134/1000, step 5/22, loss 1.5256712436676025\n","epoch 134/1000, step 6/22, loss 1.5116478204727173\n","epoch 134/1000, step 7/22, loss 1.509225845336914\n","epoch 134/1000, step 8/22, loss 1.5311113595962524\n","epoch 134/1000, step 9/22, loss 1.5159978866577148\n","epoch 134/1000, step 10/22, loss 1.5338393449783325\n","epoch 134/1000, step 11/22, loss 1.5238745212554932\n","epoch 134/1000, step 12/22, loss 1.5197579860687256\n","epoch 134/1000, step 13/22, loss 1.531704306602478\n","epoch 134/1000, step 14/22, loss 1.5027564764022827\n","epoch 134/1000, step 15/22, loss 1.52225661277771\n","epoch 134/1000, step 16/22, loss 1.512602686882019\n","epoch 134/1000, step 17/22, loss 1.5139325857162476\n","epoch 134/1000, step 18/22, loss 1.5286931991577148\n","epoch 134/1000, step 19/22, loss 1.510841965675354\n","epoch 134/1000, step 20/22, loss 1.5376721620559692\n","epoch 134/1000, step 21/22, loss 1.5172487497329712\n","epoch 134/1000, step 22/22, loss 1.5070953369140625\n","epoch 135/1000, step 1/22, loss 1.524179458618164\n","epoch 135/1000, step 2/22, loss 1.5259301662445068\n","epoch 135/1000, step 3/22, loss 1.502251148223877\n","epoch 135/1000, step 4/22, loss 1.5210360288619995\n","epoch 135/1000, step 5/22, loss 1.5256671905517578\n","epoch 135/1000, step 6/22, loss 1.5116416215896606\n","epoch 135/1000, step 7/22, loss 1.5092189311981201\n","epoch 135/1000, step 8/22, loss 1.5311063528060913\n","epoch 135/1000, step 9/22, loss 1.5159931182861328\n","epoch 135/1000, step 10/22, loss 1.5338331460952759\n","epoch 135/1000, step 11/22, loss 1.5238696336746216\n","epoch 135/1000, step 12/22, loss 1.5197516679763794\n","epoch 135/1000, step 13/22, loss 1.5316991806030273\n","epoch 135/1000, step 14/22, loss 1.5027523040771484\n","epoch 135/1000, step 15/22, loss 1.522253394126892\n","epoch 135/1000, step 16/22, loss 1.5125977993011475\n","epoch 135/1000, step 17/22, loss 1.5139281749725342\n","epoch 135/1000, step 18/22, loss 1.5286870002746582\n","epoch 135/1000, step 19/22, loss 1.510841965675354\n","epoch 135/1000, step 20/22, loss 1.5376719236373901\n","epoch 135/1000, step 21/22, loss 1.5172483921051025\n","epoch 135/1000, step 22/22, loss 1.5070945024490356\n","epoch 136/1000, step 1/22, loss 1.5241789817810059\n","epoch 136/1000, step 2/22, loss 1.5259301662445068\n","epoch 136/1000, step 3/22, loss 1.5022512674331665\n","epoch 136/1000, step 4/22, loss 1.52103590965271\n","epoch 136/1000, step 5/22, loss 1.5256670713424683\n","epoch 136/1000, step 6/22, loss 1.511641502380371\n","epoch 136/1000, step 7/22, loss 1.5092185735702515\n","epoch 136/1000, step 8/22, loss 1.5311061143875122\n","epoch 136/1000, step 9/22, loss 1.5159929990768433\n","epoch 136/1000, step 10/22, loss 1.533833384513855\n","epoch 136/1000, step 11/22, loss 1.5238693952560425\n","epoch 136/1000, step 12/22, loss 1.5197515487670898\n","epoch 136/1000, step 13/22, loss 1.5316990613937378\n","epoch 136/1000, step 14/22, loss 1.5027521848678589\n","epoch 136/1000, step 15/22, loss 1.5222532749176025\n","epoch 136/1000, step 16/22, loss 1.5125977993011475\n","epoch 136/1000, step 17/22, loss 1.5139281749725342\n","epoch 136/1000, step 18/22, loss 1.5286870002746582\n","epoch 136/1000, step 19/22, loss 1.5108416080474854\n","epoch 136/1000, step 20/22, loss 1.5376718044281006\n","epoch 136/1000, step 21/22, loss 1.5172481536865234\n","epoch 136/1000, step 22/22, loss 1.5070935487747192\n","epoch 137/1000, step 1/22, loss 1.524178147315979\n","epoch 137/1000, step 2/22, loss 1.52592933177948\n","epoch 137/1000, step 3/22, loss 1.502250075340271\n","epoch 137/1000, step 4/22, loss 1.5210349559783936\n","epoch 137/1000, step 5/22, loss 1.5256661176681519\n","epoch 137/1000, step 6/22, loss 1.5116406679153442\n","epoch 137/1000, step 7/22, loss 1.5092177391052246\n","epoch 137/1000, step 8/22, loss 1.5311052799224854\n","epoch 137/1000, step 9/22, loss 1.5159916877746582\n","epoch 137/1000, step 10/22, loss 1.5338324308395386\n","epoch 137/1000, step 11/22, loss 1.5238685607910156\n","epoch 137/1000, step 12/22, loss 1.5197508335113525\n","epoch 137/1000, step 13/22, loss 1.5316979885101318\n","epoch 137/1000, step 14/22, loss 1.5027512311935425\n","epoch 137/1000, step 15/22, loss 1.5222525596618652\n","epoch 137/1000, step 16/22, loss 1.5125969648361206\n","epoch 137/1000, step 17/22, loss 1.5139272212982178\n","epoch 137/1000, step 18/22, loss 1.5286861658096313\n","epoch 137/1000, step 19/22, loss 1.5108405351638794\n","epoch 137/1000, step 20/22, loss 1.5376708507537842\n","epoch 137/1000, step 21/22, loss 1.5172473192214966\n","epoch 137/1000, step 22/22, loss 1.5070903301239014\n","epoch 138/1000, step 1/22, loss 1.5241779088974\n","epoch 138/1000, step 2/22, loss 1.5259290933609009\n","epoch 138/1000, step 3/22, loss 1.50225031375885\n","epoch 138/1000, step 4/22, loss 1.521034836769104\n","epoch 138/1000, step 5/22, loss 1.5256656408309937\n","epoch 138/1000, step 6/22, loss 1.5116404294967651\n","epoch 138/1000, step 7/22, loss 1.509217381477356\n","epoch 138/1000, step 8/22, loss 1.5311050415039062\n","epoch 138/1000, step 9/22, loss 1.5159913301467896\n","epoch 138/1000, step 10/22, loss 1.5338319540023804\n","epoch 138/1000, step 11/22, loss 1.5238685607910156\n","epoch 138/1000, step 12/22, loss 1.5197508335113525\n","epoch 138/1000, step 13/22, loss 1.5316978693008423\n","epoch 138/1000, step 14/22, loss 1.5027509927749634\n","epoch 138/1000, step 15/22, loss 1.5222523212432861\n","epoch 138/1000, step 16/22, loss 1.512596607208252\n","epoch 138/1000, step 17/22, loss 1.5139269828796387\n","epoch 138/1000, step 18/22, loss 1.5286856889724731\n","epoch 138/1000, step 19/22, loss 1.5108405351638794\n","epoch 138/1000, step 20/22, loss 1.5376707315444946\n","epoch 138/1000, step 21/22, loss 1.5172470808029175\n","epoch 138/1000, step 22/22, loss 1.507089376449585\n","epoch 139/1000, step 1/22, loss 1.5241775512695312\n","epoch 139/1000, step 2/22, loss 1.5259287357330322\n","epoch 139/1000, step 3/22, loss 1.5022499561309814\n","epoch 139/1000, step 4/22, loss 1.521034598350525\n","epoch 139/1000, step 5/22, loss 1.5256651639938354\n","epoch 139/1000, step 6/22, loss 1.5116400718688965\n","epoch 139/1000, step 7/22, loss 1.5092170238494873\n","epoch 139/1000, step 8/22, loss 1.5311046838760376\n","epoch 139/1000, step 9/22, loss 1.5159912109375\n","epoch 139/1000, step 10/22, loss 1.5338315963745117\n","epoch 139/1000, step 11/22, loss 1.523868441581726\n","epoch 139/1000, step 12/22, loss 1.5197504758834839\n","epoch 139/1000, step 13/22, loss 1.5316972732543945\n","epoch 139/1000, step 14/22, loss 1.5027506351470947\n","epoch 139/1000, step 15/22, loss 1.522251844406128\n","epoch 139/1000, step 16/22, loss 1.5125964879989624\n","epoch 139/1000, step 17/22, loss 1.5139268636703491\n","epoch 139/1000, step 18/22, loss 1.5286856889724731\n","epoch 139/1000, step 19/22, loss 1.5108404159545898\n","epoch 139/1000, step 20/22, loss 1.5376702547073364\n","epoch 139/1000, step 21/22, loss 1.517246961593628\n","epoch 139/1000, step 22/22, loss 1.5070886611938477\n","epoch 140/1000, step 1/22, loss 1.5241774320602417\n","epoch 140/1000, step 2/22, loss 1.5259286165237427\n","epoch 140/1000, step 3/22, loss 1.5022497177124023\n","epoch 140/1000, step 4/22, loss 1.5210342407226562\n","epoch 140/1000, step 5/22, loss 1.5256651639938354\n","epoch 140/1000, step 6/22, loss 1.511639952659607\n","epoch 140/1000, step 7/22, loss 1.5092167854309082\n","epoch 140/1000, step 8/22, loss 1.531104564666748\n","epoch 140/1000, step 9/22, loss 1.5159910917282104\n","epoch 140/1000, step 10/22, loss 1.5338314771652222\n","epoch 140/1000, step 11/22, loss 1.523868203163147\n","epoch 140/1000, step 12/22, loss 1.5197501182556152\n","epoch 140/1000, step 13/22, loss 1.531697154045105\n","epoch 140/1000, step 14/22, loss 1.5027503967285156\n","epoch 140/1000, step 15/22, loss 1.522251844406128\n","epoch 140/1000, step 16/22, loss 1.5125962495803833\n","epoch 140/1000, step 17/22, loss 1.5139265060424805\n","epoch 140/1000, step 18/22, loss 1.528685450553894\n","epoch 140/1000, step 19/22, loss 1.5108401775360107\n","epoch 140/1000, step 20/22, loss 1.5376700162887573\n","epoch 140/1000, step 21/22, loss 1.5172462463378906\n","epoch 140/1000, step 22/22, loss 1.5070877075195312\n","epoch 141/1000, step 1/22, loss 1.5241771936416626\n","epoch 141/1000, step 2/22, loss 1.5259283781051636\n","epoch 141/1000, step 3/22, loss 1.502246618270874\n","epoch 141/1000, step 4/22, loss 1.521031141281128\n","epoch 141/1000, step 5/22, loss 1.5256621837615967\n","epoch 141/1000, step 6/22, loss 1.511636734008789\n","epoch 141/1000, step 7/22, loss 1.509213924407959\n","epoch 141/1000, step 8/22, loss 1.5311018228530884\n","epoch 141/1000, step 9/22, loss 1.5159881114959717\n","epoch 141/1000, step 10/22, loss 1.533828616142273\n","epoch 141/1000, step 11/22, loss 1.5238655805587769\n","epoch 141/1000, step 12/22, loss 1.5197471380233765\n","epoch 141/1000, step 13/22, loss 1.5316941738128662\n","epoch 141/1000, step 14/22, loss 1.5027474164962769\n","epoch 141/1000, step 15/22, loss 1.5222492218017578\n","epoch 141/1000, step 16/22, loss 1.5125932693481445\n","epoch 141/1000, step 17/22, loss 1.5139236450195312\n","epoch 141/1000, step 18/22, loss 1.5286827087402344\n","epoch 141/1000, step 19/22, loss 1.510837197303772\n","epoch 141/1000, step 20/22, loss 1.5376672744750977\n","epoch 141/1000, step 21/22, loss 1.5172432661056519\n","epoch 141/1000, step 22/22, loss 1.507087230682373\n","epoch 142/1000, step 1/22, loss 1.524174451828003\n","epoch 142/1000, step 2/22, loss 1.5259252786636353\n","epoch 142/1000, step 3/22, loss 1.5022462606430054\n","epoch 142/1000, step 4/22, loss 1.5210307836532593\n","epoch 142/1000, step 5/22, loss 1.5256619453430176\n","epoch 142/1000, step 6/22, loss 1.51163649559021\n","epoch 142/1000, step 7/22, loss 1.5092133283615112\n","epoch 142/1000, step 8/22, loss 1.5311015844345093\n","epoch 142/1000, step 9/22, loss 1.5159878730773926\n","epoch 142/1000, step 10/22, loss 1.5338282585144043\n","epoch 142/1000, step 11/22, loss 1.5238652229309082\n","epoch 142/1000, step 12/22, loss 1.519747018814087\n","epoch 142/1000, step 13/22, loss 1.5316938161849976\n","epoch 142/1000, step 14/22, loss 1.5027474164962769\n","epoch 142/1000, step 15/22, loss 1.5222492218017578\n","epoch 142/1000, step 16/22, loss 1.5125929117202759\n","epoch 142/1000, step 17/22, loss 1.513923168182373\n","epoch 142/1000, step 18/22, loss 1.5286824703216553\n","epoch 142/1000, step 19/22, loss 1.5108367204666138\n","epoch 142/1000, step 20/22, loss 1.5376670360565186\n","epoch 142/1000, step 21/22, loss 1.5172431468963623\n","epoch 142/1000, step 22/22, loss 1.5070850849151611\n","epoch 143/1000, step 1/22, loss 1.5241739749908447\n","epoch 143/1000, step 2/22, loss 1.5259252786636353\n","epoch 143/1000, step 3/22, loss 1.5022457838058472\n","epoch 143/1000, step 4/22, loss 1.5210306644439697\n","epoch 143/1000, step 5/22, loss 1.5256614685058594\n","epoch 143/1000, step 6/22, loss 1.5116358995437622\n","epoch 143/1000, step 7/22, loss 1.509212613105774\n","epoch 143/1000, step 8/22, loss 1.531101107597351\n","epoch 143/1000, step 9/22, loss 1.515987515449524\n","epoch 143/1000, step 10/22, loss 1.5338279008865356\n","epoch 143/1000, step 11/22, loss 1.52386474609375\n","epoch 143/1000, step 12/22, loss 1.5197460651397705\n","epoch 143/1000, step 13/22, loss 1.5316938161849976\n","epoch 143/1000, step 14/22, loss 1.5027467012405396\n","epoch 143/1000, step 15/22, loss 1.5222485065460205\n","epoch 143/1000, step 16/22, loss 1.5125925540924072\n","epoch 143/1000, step 17/22, loss 1.5139226913452148\n","epoch 143/1000, step 18/22, loss 1.5286818742752075\n","epoch 143/1000, step 19/22, loss 1.5108363628387451\n","epoch 143/1000, step 20/22, loss 1.53766667842865\n","epoch 143/1000, step 21/22, loss 1.5172431468963623\n","epoch 143/1000, step 22/22, loss 1.5070843696594238\n","epoch 144/1000, step 1/22, loss 1.524173617362976\n","epoch 144/1000, step 2/22, loss 1.5259249210357666\n","epoch 144/1000, step 3/22, loss 1.5022451877593994\n","epoch 144/1000, step 4/22, loss 1.5210304260253906\n","epoch 144/1000, step 5/22, loss 1.5256611108779907\n","epoch 144/1000, step 6/22, loss 1.5116357803344727\n","epoch 144/1000, step 7/22, loss 1.5092124938964844\n","epoch 144/1000, step 8/22, loss 1.531101107597351\n","epoch 144/1000, step 9/22, loss 1.5159871578216553\n","epoch 144/1000, step 10/22, loss 1.533827781677246\n","epoch 144/1000, step 11/22, loss 1.5238643884658813\n","epoch 144/1000, step 12/22, loss 1.5197460651397705\n","epoch 144/1000, step 13/22, loss 1.5316938161849976\n","epoch 144/1000, step 14/22, loss 1.50274658203125\n","epoch 144/1000, step 15/22, loss 1.5222485065460205\n","epoch 144/1000, step 16/22, loss 1.5125921964645386\n","epoch 144/1000, step 17/22, loss 1.5139224529266357\n","epoch 144/1000, step 18/22, loss 1.5286816358566284\n","epoch 144/1000, step 19/22, loss 1.5108363628387451\n","epoch 144/1000, step 20/22, loss 1.5376665592193604\n","epoch 144/1000, step 21/22, loss 1.5172427892684937\n","epoch 144/1000, step 22/22, loss 1.5070838928222656\n","epoch 145/1000, step 1/22, loss 1.524173378944397\n","epoch 145/1000, step 2/22, loss 1.525924563407898\n","epoch 145/1000, step 3/22, loss 1.5022451877593994\n","epoch 145/1000, step 4/22, loss 1.521030306816101\n","epoch 145/1000, step 5/22, loss 1.5256611108779907\n","epoch 145/1000, step 6/22, loss 1.5116355419158936\n","epoch 145/1000, step 7/22, loss 1.5092122554779053\n","epoch 145/1000, step 8/22, loss 1.531100869178772\n","epoch 145/1000, step 9/22, loss 1.5159868001937866\n","epoch 145/1000, step 10/22, loss 1.533827543258667\n","epoch 145/1000, step 11/22, loss 1.5238642692565918\n","epoch 145/1000, step 12/22, loss 1.5197457075119019\n","epoch 145/1000, step 13/22, loss 1.531693696975708\n","epoch 145/1000, step 14/22, loss 1.5027462244033813\n","epoch 145/1000, step 15/22, loss 1.522248387336731\n","epoch 145/1000, step 16/22, loss 1.512592077255249\n","epoch 145/1000, step 17/22, loss 1.5139223337173462\n","epoch 145/1000, step 18/22, loss 1.5286811590194702\n","epoch 145/1000, step 19/22, loss 1.5108360052108765\n","epoch 145/1000, step 20/22, loss 1.5376659631729126\n","epoch 145/1000, step 21/22, loss 1.517242431640625\n","epoch 145/1000, step 22/22, loss 1.5070830583572388\n","epoch 146/1000, step 1/22, loss 1.5241730213165283\n","epoch 146/1000, step 2/22, loss 1.5259244441986084\n","epoch 146/1000, step 3/22, loss 1.5022450685501099\n","epoch 146/1000, step 4/22, loss 1.521030068397522\n","epoch 146/1000, step 5/22, loss 1.5256608724594116\n","epoch 146/1000, step 6/22, loss 1.511635184288025\n","epoch 146/1000, step 7/22, loss 1.5092118978500366\n","epoch 146/1000, step 8/22, loss 1.5311007499694824\n","epoch 146/1000, step 9/22, loss 1.5159865617752075\n","epoch 146/1000, step 10/22, loss 1.533827543258667\n","epoch 146/1000, step 11/22, loss 1.5238639116287231\n","epoch 146/1000, step 12/22, loss 1.5197453498840332\n","epoch 146/1000, step 13/22, loss 1.531693458557129\n","epoch 146/1000, step 14/22, loss 1.5027458667755127\n","epoch 146/1000, step 15/22, loss 1.5222477912902832\n","epoch 146/1000, step 16/22, loss 1.5125919580459595\n","epoch 146/1000, step 17/22, loss 1.513922095298767\n","epoch 146/1000, step 18/22, loss 1.5286809206008911\n","epoch 146/1000, step 19/22, loss 1.510835886001587\n","epoch 146/1000, step 20/22, loss 1.5376659631729126\n","epoch 146/1000, step 21/22, loss 1.5172423124313354\n","epoch 146/1000, step 22/22, loss 1.507082223892212\n","epoch 147/1000, step 1/22, loss 1.5241729021072388\n","epoch 147/1000, step 2/22, loss 1.5259242057800293\n","epoch 147/1000, step 3/22, loss 1.5022447109222412\n","epoch 147/1000, step 4/22, loss 1.5210297107696533\n","epoch 147/1000, step 5/22, loss 1.5256606340408325\n","epoch 147/1000, step 6/22, loss 1.5116350650787354\n","epoch 147/1000, step 7/22, loss 1.509211778640747\n","epoch 147/1000, step 8/22, loss 1.5311005115509033\n","epoch 147/1000, step 9/22, loss 1.5159865617752075\n","epoch 147/1000, step 10/22, loss 1.533827304840088\n","epoch 147/1000, step 11/22, loss 1.5238635540008545\n","epoch 147/1000, step 12/22, loss 1.5197449922561646\n","epoch 147/1000, step 13/22, loss 1.5316933393478394\n","epoch 147/1000, step 14/22, loss 1.502745509147644\n","epoch 147/1000, step 15/22, loss 1.522247314453125\n","epoch 147/1000, step 16/22, loss 1.5125916004180908\n","epoch 147/1000, step 17/22, loss 1.5139217376708984\n","epoch 147/1000, step 18/22, loss 1.5286805629730225\n","epoch 147/1000, step 19/22, loss 1.5108355283737183\n","epoch 147/1000, step 20/22, loss 1.5376657247543335\n","epoch 147/1000, step 21/22, loss 1.5172419548034668\n","epoch 147/1000, step 22/22, loss 1.507081151008606\n","epoch 148/1000, step 1/22, loss 1.5241725444793701\n","epoch 148/1000, step 2/22, loss 1.525923728942871\n","epoch 148/1000, step 3/22, loss 1.5022443532943726\n","epoch 148/1000, step 4/22, loss 1.5210293531417847\n","epoch 148/1000, step 5/22, loss 1.5256602764129639\n","epoch 148/1000, step 6/22, loss 1.5116347074508667\n","epoch 148/1000, step 7/22, loss 1.5092116594314575\n","epoch 148/1000, step 8/22, loss 1.5311001539230347\n","epoch 148/1000, step 9/22, loss 1.5159862041473389\n","epoch 148/1000, step 10/22, loss 1.5338270664215088\n","epoch 148/1000, step 11/22, loss 1.5238633155822754\n","epoch 148/1000, step 12/22, loss 1.5197447538375854\n","epoch 148/1000, step 13/22, loss 1.5316929817199707\n","epoch 148/1000, step 14/22, loss 1.5027451515197754\n","epoch 148/1000, step 15/22, loss 1.5222471952438354\n","epoch 148/1000, step 16/22, loss 1.5125913619995117\n","\n","Program interrupted. (Use 'cont' to resume).\n","Program interrupted. (Use 'cont' to resume).\n","Program interrupted. (Use 'cont' to resume).\n","Program interrupted. (Use 'cont' to resume).\n","--Call--\n","> /usr/lib/python3.6/bdb.py(241)set_trace()\n","-> def set_trace(self, frame=None):\n","\n","Program interrupted. (Use 'cont' to resume).\n","\n","\n","\n","--Call--\n","--Call--\n","--Call--\n","> /usr/lib/python3.6/bdb.py(241)set_trace()\n","-> def set_trace(self, frame=None):--Call--\n","> /usr/lib/python3.6/bdb.py(241)set_trace()\n","-> def set_trace(self, frame=None):> /usr/lib/python3.6/bdb.py(241)set_trace()\n","-> def set_trace(self, frame=None):\n","\n","> /usr/lib/python3.6/bdb.py(241)set_trace()\n","-> def set_trace(self, frame=None):\n","\n","(Pdb) exit\n","--KeyboardInterrupt--\n","--KeyboardInterrupt--\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fe463ebeb70>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 124, in join\n","    res = self._popen.wait(timeout)\n","  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 47, in wait\n","    if not wait([self.sentinel], timeout):\n","  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n","    ready = selector.select(timeout)\n","  File \"/usr/lib/python3.6/selectors.py\", line 376, in select\n","    fd_event_list = self._poll.poll(timeout)\n","KeyboardInterrupt: \n"],"name":"stderr"},{"output_type":"error","ename":"BdbQuit","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-f901e8a027d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m           \u001b[0;31m#print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m           \u001b[0;31m#if (i+1) % 2 ==0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_iterations}, loss {loss.item()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m           \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mss_struct_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_preexist_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/pdb.py\u001b[0m in \u001b[0;36msigint_handler\u001b[0;34m(self, signum, frame)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nProgram interrupted. (Use 'cont' to resume).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mset_trace\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_stopinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \"\"\"Start debugging from `frame`.\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'return'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_call\u001b[0;34m(self, frame, arg)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mBdbQuit\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yckYobEW1VrI","executionInfo":{"status":"ok","timestamp":1611612400802,"user_tz":300,"elapsed":37562,"user":{"displayName":"Sebastian garcia lopez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjC2t48-bE51c9ubEao0oacZ0i3LJmFyS90LgNgfg=s64","userId":"04181361624039461809"}},"outputId":"d1700024-f3f9-4bad-b460-42cabd790791"},"source":["import pdb\r\n","import torch\r\n","import os\r\n","\r\n","import numpy as np\r\n","from convolutional_nn import *\r\n","from one_hot_encoding import *\r\n","from datasetProcessing import *\r\n","from torch.autograd import Variable\r\n","from torch.utils.data import Dataset, DataLoader\r\n","\r\n","'''\r\n","def Q8score_per_prot(t1,treal,hh):\r\n","  tpred=torch.Tensor(t1).type(torch.ByteTensor)\r\n","  #return (t1 == treal).mean()\r\n","  xxxx = (t1 == treal)\r\n","  mask = (treal ==8)\r\n","\r\n","  mask = xxxx\r\n","  xxxx[mask]\r\n","\r\n","'''\r\n","def Q8score_per_prot(t1,treal,hh):\r\n","  tpred=torch.Tensor(t1).type(torch.ByteTensor)\r\n","  cont=0\r\n","  for i in range(0,treal.shape[1]):\r\n","    chartpred= hh.get_inverted_encode(tpred[:,i].tolist())\r\n","    chartreal= hh.get_inverted_encode(treal[:,i].tolist())\r\n","    if chartpred == chartreal:\r\n","      if chartpred == 'NoSeq':\r\n","        continue\r\n","      else:\r\n","        cont+=1\r\n","    else:\r\n","      continue\r\n","  return float(cont)/float(treal.shape[1])\r\n"," \r\n","\r\n","\r\n","if __name__ == \"__main__\":\r\n"," \r\n","  path_preexist_model = '/gdrive/Shareddrives/neurolusion/PhD/Secondary_Structure_Pred/protpred_2d/models/ss_struct3.pth'\r\n","  dataset_path = \"../ICML2014/cullpdb+profile_5926.npy.gz\"\r\n","  n_prot = 5926; n_aa = 700; n_features = 57\r\n","  \r\n","  data_management = datasetProcessing(dataset_path, n_prot, n_aa, n_features)\r\n","  ss_struct_pred = convolutional_nn()\r\n","\r\n","  if os.path.isfile(path_preexist_model):\r\n","    print (\"Loading Model\")  \r\n","    state_dict = torch.load(path_preexist_model)\r\n","    #print(state_dict)\r\n","    ss_struct_pred.load_state_dict(state_dict)\r\n","\r\n","  pdb.set_trace()\r\n","\r\n","  # calculation of Q8 accuracy\r\n","  average_Q8 = []\r\n","  output = ss_struct_pred(data_management.tt)\r\n","  ground_truth=data_management.ttl\r\n","\r\n","  HSC_alphabet = ['L', 'B', 'E', 'G', 'I', 'H', 'S', 'T','NoSeq']\r\n","  HSC_onehot = one_hot_encoding(HSC_alphabet)\r\n","  \r\n","\t# inverted = hh.get_inverted_encode(encode[0])\r\n","  for i in range(0,len(output)):\r\n","    get_output_sec = output[i]\r\n","    get_ground_truth_sec = ground_truth[i]\r\n","    average_Q8.append(Q8score_per_prot(get_output_sec, get_ground_truth_sec, HSC_onehot))\r\n","\r\n","  Q8_acc = np.array(average_Q8).mean()  \r\n","  print(Q8_acc)\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading Model\n","> <ipython-input-4-a489c13679da>(58)<module>()\n","-> average_Q8 = []\n","(Pdb) c\n","0.7530196078431373\n"],"name":"stdout"}]}]}